import osimport sysfunction_directory = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/src/03. Glacier simulations"sys.path.append(function_directory)# from OGGM_data_processing import process_perturbation_data,custom_process_cmip_data,custom_process_gcm_data#%%import multiprocessingimport tracebackimport loggingfrom multiprocessing import Pool, set_start_method, get_contextfrom multiprocessing import Processimport concurrent.futuresfrom matplotlib.lines import Line2Dimport oggmfrom oggm import utils, cfg, workflow, tasks, DEFAULT_BASE_URL, graphics, global_tasksfrom oggm.core import massbalance, flowlinefrom oggm.utils import floatyear_to_date, hydrodate_to_calendardatefrom oggm.sandbox import distribute_2dfrom oggm.sandbox.edu import run_constant_climate_with_biasfrom oggm.tasks import process_cmip_dataimport geopandas as gpdimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport matplotlib.colors as clrsimport xarray as xrimport osimport seaborn as snsimport salemfrom matplotlib.ticker import FuncFormatterimport pandas as pdimport numpy as npfrom matplotlib import animationfrom IPython.display import HTML, displayimport cartopy.crs as ccrsimport cartopy.feature as cfeaturefrom scipy.optimize import curve_fitfrom tqdm import tqdmimport pickleimport textwrapimport matplotlib.patches as mpatchesimport matplotlib.lines as mlinesfrom matplotlib.colors import LinearSegmentedColormap, TwoSlopeNormimport sysfrom xarray.coding.times import CFDatetimeCoder# %% Cell 0: Set base parameterscolors_models = {    "W5E5": ["#000000"],  # "#000000"],  # Black    # Darker to lighter shades of purple    "E3SM": ["#785EF0", "#8F7BF1", "#A6A8F2"],    # Darker to lighter shades of pink    "CESM2": ["#DC267F", "#E58A9E", "#F0A2B6", "#F7BCC4"],    # Darker to lighter shades of orange    "CNRM": ["#FE6100", "#FE7D33", "#FE9A66", "#FEB799", "#FECDB5", "#FEF1E1"],    "IPSL-CM6": ["#FFB000"]  # Dark purple to lighter shades}members = [1, 3, 4, 6, 4, 1]members_averages = [1, 2, 3, 5, 3]models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]timeframe = "monthly"y0_clim = 1985ye_clim = 2014y0_cf = 1901ye_cf = 1985fig_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/02. OGGM simulations/01. Modelled output/3r_a5/'# %% Cell 1: Initialize OGGM with the preferred model parameter set upfolder_path = '/Users/magaliponds/Documents/00. Programming'wd_path = f'{folder_path}/03. Modelled perturbation-glacier interactions - R13-15 A+5km2/'wd_path_fut = f'{folder_path}/03. Modelled perturbation-glacier interactions Future - R13-15 A+5km2/'os.makedirs(wd_path, exist_ok=True)os.makedirs(wd_path_fut, exist_ok=True)cfg.initialize(logging_level='WARNING')cfg.PATHS['working_dir'] = utils.mkdir(wd_path, reset=False)# make a sum dirsum_dir = os.path.join(wd_path, 'summary')os.makedirs(sum_dir, exist_ok=True)# make a logging directorylog_dir = os.path.join(wd_path, "log")os.makedirs(log_dir, exist_ok=True)# Make a pkl directorypkls = os.path.join(wd_path, "pkls")os.makedirs(pkls, exist_ok=True)cfg.PARAMS['baseline_climate'] = "GSWP3-W5E5"cfg.PARAMS['store_model_geometry'] = True# %% Cell 2: Save gdirs_3r_a5 to pkl (fastest way to get started)# pkls_fut = os.path.join(wd_path_fut, "pkls_subset_success")# os.makedirs(pkls_fut, exist_ok=True)# # Save each gdir individually# for gdir in gdirs_3r_a5: #loaded from below#     gdir_path = os.path.join(pkls_fut, f'{gdir.rgi_id}.pkl')#     with open(gdir_path, 'wb') as f:#         pickle.dump(gdir, f)        #%% Cell 2b: Load gdirs_3r_a5 from pklwd_path_pkls_fut = f'{wd_path_fut}/pkls_subset_success/'gdirs_3r_a5 = []for filename in os.listdir(wd_path_pkls_fut):    if filename.endswith('.pkl'):        # f'{gdir.rgi_id}.pkl')        file_path = os.path.join(wd_path_pkls_fut, filename)        with open(file_path, 'rb') as f:            gdir = pickle.load(f)            gdirs_3r_a5.append(gdir)print("loaded", len(gdirs_3r_a5), "gdirs")#%% Cell 3: Process the future climate data# cfg.initialize()# cfg.PATHS['working_dir'] = utils.mkdir(wd_path_fut, reset=False)# members = [4]# models = ["CESM2"]# timeframe = "monthly"# ssps = ["126","370"]#"126",# exp = ["NOI"]#"IRR", "NOI"]# y0=2015# ye=2074# opath_climate = os.path.join(sum_dir, 'climate_historical.nc')# utils.compile_climate_input(#     gdirs_3r_a5, path=opath_climate, filename='climate_historical')# # if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessing# cfg.PARAMS['use_multiprocessing'] = True# cfg.PARAMS['core'] = 9 # üîß set number of cores# def main():#     remake="False"#     for m, model in enumerate(models):#         for member in range(members[m]):#             for s, ssp in enumerate(ssps):#                 for e, ex in enumerate(exp):#                     if member == 1: #skip 0#                         sample_id = f"{model}.00{member}"#                         print(sample_id, ex, "SSP",ssp)#                         if remake=="True":                        #                             # Provide the path to the perturbation dataset#                             # if error with lon.min or ds['time.year'] check if lon>0 in creating input dataframe#                             i_folder_ptb = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/01. Climate data/{model}/{y0}/"#                             ds_path = f"{i_folder_ptb}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total.nc"#                             # ds_path = f"{i_folder_ptb}/{model}.00{member}.{y0_cf}_{ye_cf}.{timeframe}.perturbation.input.%.degC.counterfactual.nc"#                             ds = xr.open_dataset(ds_path)                            #                             #add the original timeline for bias correction (reference period 1961-1990)#                             i_folder_ptb_og = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/01. Climate data/{model}/1985/"#                             ds_path_og = f"{i_folder_ptb_og}/{model}.{ex}.00{member}.1985_2014_selparam_monthly_total.nc"#                             ds_og = xr.open_dataset(ds_path_og)#                             ds = ds.assign_coords(lat=ds_og.lat) #assign ds with the coords of ds_og (3e-6 difference)#                             ds_combined = xr.concat([ds_og,ds], dim="time")#                             ds_combined[['pr','sn','tas']] = ds_combined[['pr','sn','tas']].fillna(-1e9) #fill na values with very small nr#                             # ds_combined = ds_combined.rename({'time_bnds': 'bnds'})#                             # ds_combined = ds_combined.set_coords('bnds')                            #                             time64_combined = pd.to_datetime([t.isoformat() for t in ds_combined['time'].values])#                             ds_combined = ds_combined.assign_coords(time=('time', time64_combined))                            #                             all_prcp = (ds_combined.pr + ds_combined.sn) #convert mm/s to mm/day in function, already summed for days in month#                             all_prcp = all_prcp.to_dataset(name='pr')#                             all_prcp.pr.attrs['units'] = "kg m-2 s-1"#                             all_prcp["time"] = xr.decode_cf(all_prcp).time#                             # all_prcp['time.month'] = ('time', all_prcp['time'].dt.month.data)                            #                             tas = ds_combined.tas.to_dataset(name='tas')#                             tas.tas.attrs['units'] = "degc"#                             tas["time"] = xr.decode_cf(tas).time#                             tas = tas.sortby('time')#                             # tas['time.month'] = ('time', tas['time'].dt.month.data)                            #                             o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/03. Future climate data/"#                             ds_p_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc"#                             ds_t_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc"#                             os.makedirs(o_folder, exist_ok=True)#                             all_prcp.to_netcdf(ds_p_path)#                             tas.to_netcdf(ds_t_path)#                         else:#                             o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/03. Future climate data/"#                             ds_p_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc"#                             ds_t_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc"                            #                             all_prcp = xr.open_dataset(f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc")#                             tas = xr.open_dataset(f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc")                        #                         output_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                         # output_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                         sample_id =f"{model}.00{member}"#                         print(sample_id)#                         # print(tas["time.month"])                        #                         time_vals = tas['lat'].values#                         duplicates = pd.Series(time_vals)[pd.Series(time_vals).duplicated()]                        #                         # Provide the sample ID to provide the processed pertrubations with the correct output suffix#                         workflow.execute_entity_task(custom_process_cmip_data, gdirs_3r_a5,#                                                                          fpath_temp=ds_t_path, fpath_precip=ds_p_path, #                                                                          y0=1986, y1=2073, filesuffix='', #files must start in januari#                                                                          output_filesuffix=output_filesuffix, exp=ex, sample_id=sample_id)#                         # print(all_prcp)                       #                         # opath_perturbations = os.path.join(#                         #     sum_dir, f'climate_historical_perturbation_{sample_id}_noi_bias.nc')#                         # # # opath_perturbations = os.path.join(            #                         # utils.compile_climate_input(gdirs_3r_a5, path=opath_perturbations, filename='climate_historical',#                         #                             # input_filesuffix=f'_perturbation_{sample_id}_counterfactual',#                         #                             input_filesuffix=f'_perturbation_{sample_id}_noi_bias',#                         #                             use_compression=True, use_multiple_files=True)# if __name__ == '__main__':#     multiprocessing.set_start_method('spawn', force=True)#     main()                 #%%   Run climate model    # members = [4]# models = ["CESM2"]# timeframe = "monthly"# ssps = ["126","370"]#"126",# exp = ["IRR","NOI"]#,"IRR"]#, "NOI"]# y0=2014# ye=2074# # opath_climate = os.path.join(sum_dir, 'climate_historical.nc')# # utils.compile_climate_input(# #     gdirs_3r_a5, path=opath_climate, filename='climate_historical')# remake="False"# logging.basicConfig(filename='error_log.txt', level=logging.ERROR)# # # if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessing# cfg.PARAMS['use_multiprocessing'] = True# cfg.PARAMS['core'] = 9 # üîß set number of cores# def main():#     errors = []#     for m, model in enumerate(models):#         for member in range(members[m]):#             for s, ssp in enumerate(ssps):#                 for e, ex in enumerate(exp):#                     if member >=1: #skip 0#                         sample_id = f"{model}.00{member}"#                         print(sample_id, ex, "SSP",ssp)#                         if ex=="NOI":#                             init_model_suffix = f'_perturbed_CESM2.00{member}'#                             input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                         if ex == "IRR":#                             init_model_suffix = f'_baseline_W5E5.000'#                             input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                        #                         out_id = f'_CESM2{input_filesuffix}'                            #                         for gdir in gdirs_3r_a5:#                             try:#                                 workflow.execute_entity_task(#                                     tasks.run_from_climate_data, [gdir],#                                     ys=y0, ye=ye,#                                     max_ys=None, fixed_geometry_spinup_yr=None,#                                     store_monthly_step=False,#                                     store_model_geometry=True,#                                     store_fl_diagnostics=True,#                                     climate_filename='gcm_data',#                                     climate_input_filesuffix=input_filesuffix,#                                     output_filesuffix=out_id,#                                     zero_initial_glacier=False, bias=0,#                                     temperature_bias=None, precipitation_factor=None,#                                     init_model_filesuffix=init_model_suffix,#                                     init_model_yr=2014 #2014#                                 )#                             except Exception as err:#                                 err_msg = (f"‚ùå ERROR for RGI_ID={gdir.rgi_id}, "#                                            f"sample_id={sample_id}, ssp={ssp}, exp={ex}, member={member}\n"#                                            f"{traceback.format_exc()}")#                                 logging.error(err_msg)#                                 errors.append(gdir.rgi_id)#                                 print(f"[ERROR] Logged: {gdir.rgi_id}")#                         # Try compiling the output only if you want to after all gdirs are run#                         # try:#                         #     if ex=="NOI":#                         #         opath = os.path.join(#                         #             sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}_noi_bias.nc')#                         #     else:#                         #         opath = os.path.join(#                         #             sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}.nc')#                         #     ds_ptb = utils.compile_run_output(#                         #         gdirs_3r_a5, input_filesuffix=out_id, path=opath#                         #     )#                         # except Exception as err:#                         #     logging.error(f"‚ö†Ô∏è Failed to compile output for {sample_id}\n{traceback.format_exc()}")#     # Save all errors at the end#     if errors:#         with open(os.path.join(sum_dir, "gdir_errors_noi_bias.txt"), "w") as f:#             for rgi_id in errors:#                 f.write(rgi_id + "\n")#         print(f"\nüö® Finished with {len(errors)} gdir errors. See gdir_errors.txt.")#     else:#         print("\n‚úÖ All glaciers processed successfully.")# if __name__ == '__main__':#     multiprocessing.set_start_method('spawn', force=True)#     main()            #%% Only compile run output# members = [4]# models = ["CESM2"]# timeframe = "monthly"# ssps = ["370"]#"126","126",# exp = ["IRR", "NOI"] #"IRR", # y0=2014 #2015# ye=2074# # gdirs_3r_a5 = [g for g in gdirs_3r_a5 if g.rgi_id != 'RGI60-13.48966']# def main():#     for m, model in enumerate(models):#         for member in range(members[m]):#             for s, ssp in enumerate(ssps):#                 for e, ex in enumerate(exp):#                     if member ==3: #skip 0#                         print(member)#                         sample_id = f"{model}.00{member}"#                         if ex=="NOI": #                             input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                         else:#                             input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                            #                         out_id = f'_CESM2{input_filesuffix}'#                         opath = os.path.join(#                             sum_dir, f'climate_run_output{input_filesuffix}.nc'#                         )#                         # ds = xr.open_dataset(gdirs_3r_a5[10].get_filepath('model_diagnostics', filesuffix=out_id))#                         # print(ds.volume_m3)#                         ds_ptb = utils.compile_run_output(#                             gdirs_3r_a5, path=opath, input_filesuffix=out_id#                         )                    # if __name__ == '__main__':#     multiprocessing.set_start_method('spawn', force=True)#     main()       # #%%# for m, model in enumerate(models):#     for member in range(members[m]):#         for s, ssp in enumerate(ssps):#             for e, ex in enumerate(exp):#                 if member >=1: #skip 0#                     print(member)#                     sample_id = f"{model}.00{member}"#                     if ex=="NOI": #                         input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                     else:#                         input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                        #                     out_id = f'_CESM2{input_filesuffix}'#                     # valid_gdirs = []#                     # for gdir in gdirs_3r_a5:#                     #     fpath = gdir.get_filepath('model_diagnostics', filesuffix=out_id)#                     #     if os.path.exists(fpath):#                     #         valid_gdirs.append(gdir)#                     #     else:#                     #         print(gdir.rgi_id)#                     # print(model, member, ssp, ex, len(valid_gdirs))#                     valid_gdirs = []#                     for gdir in gdirs_3r_a5:#                         try:#                             fpath = gdir.get_filepath('model_diagnostics', filesuffix=out_id)#                             if os.path.exists(fpath):#                                 with xr.open_dataset(fpath) as ds:#                                     if 'time' in ds and ds['time'].size > 0:#                                         valid_gdirs.append(gdir)#                         except Exception as e:#                             print(f"Skipped {gdir.rgi_id} due to error: {e}")#%% Cell 4: Compile climate historical - IRR# members = [4]# models = ["CESM2"]# timeframe = "monthly"# ssps = ["126","370"]#"126",# exp = ["IRR"]#"IRR", "NOI"]# y0=2015# ye=2074# # def main():# for m, model in enumerate(models):#     for member in range(members[m]):#         for s, ssp in enumerate(ssps):#             for e, ex in enumerate(exp):#                 if member >=1: #skip 0#                     sample_id = f"{model}.00{member}"#                     input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"#                     out_id = f'{input_filesuffix}'#                     opath = os.path.join(#                         sum_dir, f'climate_input_data{input_filesuffix}.nc'#                     )#                     ds_ptb = utils.compile_climate_input(#                         gdirs_3r_a5, filename="gcm_data", input_filesuffix=out_id, path=opath#                     )# # if __name__ == '__main__':# #     multiprocessing.set_start_method('spawn', force=True)# #     main()           # ds_base = utils.compile_climate_input(#     gdirs_3r_a5, filename="climate_historical", input_filesuffix="", path=os.path.join(#         sum_dir, f'climate_input_data_historical.nc')# )#%%Cell 4b: Compile climate historical - NOI# members = [4]# models = ["CESM2"]# timeframe = "monthly"# ssps = ["126","370"]#"126",# exp = ["NOI"]#"IRR", "NOI"]# y0=2015# ye=2074# # def main():# for m, model in enumerate(models):#     for member in range(members[m]):#         # for s, ssp in enumerate(ssps):#         #     for e, ex in enumerate(exp):#                 # if member >=2: #skip 0#                 #     sample_id = f"{model}.00{member}"#                 #     input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"#                 #     out_id = f'{input_filesuffix}'#                 #     opath = os.path.join(#                 #         sum_dir, f'climate_input_data{input_filesuffix}.nc'#                 #     )#                 #     ds_ptb = utils.compile_climate_input(#                 #         gdirs_3r_a5, filename="gcm_data", input_filesuffix=out_id, path=opath#                 #     )# # if __name__ == '__main__':# #     multiprocessing.set_start_method('spawn', force=True)# #     main()   #         if member >=1: #skip 0#             print(member)#             ds_base = utils.compile_climate_input(gdirs_3r_a5, filename="gcm_data", input_filesuffix=f"_perturbed_CESM2.00{member}", #                                                   path=os.path.join(sum_dir, f'climate_input_data_historical_noi_CESM2.00{member}')            # )                        #%%# for gdir in gdirs_3r_a5[:10]:#     for fname in ['model_diagnostics', 'fl_diagnostics', 'model_geometry']:#         fpath = gdir.get_filepath(fname, filesuffix='_CESM2_SSP370_NOI.002_noi_bias')#         try:#            ds = xr.open_dataset(fpath)#            print(ds)#         except:#             print(f"Skipping {gdir.rgi_id} due to missing/corrupt output")#             if not os.path.exists(fpath):                # print(f"[MISSING] {fname} for {gdir.rgi_id}")#%% RUn with hydro future    members = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["IRR","NOI"]#,"IRR"]#, "NOI"]y0=2014ye=2074# opath_climate = os.path.join(sum_dir, 'climate_historical.nc')# utils.compile_climate_input(#     gdirs_3r_a5, path=opath_climate, filename='climate_historical')remake="False"logging.basicConfig(filename='error_log.txt', level=logging.ERROR)# # if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessingcfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # üîß set number of coresdef main():    errors = []    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >=1: #skip 0                        sample_id = f"{model}.00{member}"                        print(sample_id, ex, "SSP",ssp)                        if ex=="NOI":                            init_model_suffix = f'_perturbed_CESM2.00{member}'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        if ex == "IRR":                            init_model_suffix = f'_baseline_W5E5.000'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                                                out_id = f'CESM2{input_filesuffix}_hydro'                                                    for gdir in gdirs_3r_a5:                            try:                                workflow.execute_entity_task(                                    tasks.run_with_hydro, [gdir],                                    run_task=tasks.run_from_climate_data,                                    ys=y0, ye=ye,                                    max_ys=None, fixed_geometry_spinup_yr=None,                                    store_monthly_step=False,                                    store_model_geometry=True,                                    store_fl_diagnostics=True,                                    climate_filename='gcm_data',                                    climate_input_filesuffix=input_filesuffix,                                    output_filesuffix=out_id,                                    zero_initial_glacier=False, bias=0,                                    temperature_bias=None, precipitation_factor=None,                                    init_model_filesuffix=init_model_suffix,                                    init_model_yr=2014                                )                            except Exception as err:                                err_msg = (f"‚ùå ERROR for RGI_ID={gdir.rgi_id}, "                                           f"sample_id={sample_id}, ssp={ssp}, exp={ex}, member={member}\n"                                           f"{traceback.format_exc()}")                                logging.error(err_msg)                                errors.append(gdir.rgi_id)                                print(f"[ERROR] Logged: {gdir.rgi_id}")                        # Try compiling the output only if you want to after all gdirs are run                        try:                            print("Compiling member:", member)                            #                         )                            if ex=="NOI":                                opath = os.path.join(                                    sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}_noi_bias_hydro.nc')                            else:                                opath = os.path.join(                                    sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}_hydro.nc')                            ds_ptb = utils.compile_run_output(                                gdirs_3r_a5, input_filesuffix=out_id, path=opath                            )                        except Exception as err:                            logging.error(f"‚ö†Ô∏è Failed to compile output for {sample_id}\n{traceback.format_exc()}")    # Save all errors at the end    if errors:        with open(os.path.join(sum_dir, "gdir_errors_hdyro.txt"), "w") as f:            for rgi_id in errors:                f.write(rgi_id + "\n")        print(f"\nüö® Finished with {len(errors)} gdir errors. See gdir_errors.txt.")    else:        print("\n‚úÖ All glaciers processed successfully.")if __name__ == '__main__':    multiprocessing.set_start_method('spawn', force=True)    main()        