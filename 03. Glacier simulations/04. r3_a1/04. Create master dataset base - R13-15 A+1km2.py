# %% Cell 0: Load data packages#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Wed Jul  3 15:36:52 2024@author: magalipondsThis script creates a master dataset from OGGM operations and pre-existing data (Hugonnet, RGI)      """# -*- coding: utf-8 -*-import oggm# from OGGM_data_processing import process_perturbation_dataimport concurrent.futuresfrom matplotlib.lines import Line2Dimport oggmfrom oggm import utils, cfg, workflow, tasks, DEFAULT_BASE_URL, graphics, global_tasksfrom oggm.core import massbalance, flowlinefrom oggm.utils import floatyear_to_date, hydrodate_to_calendardate,compile_glacier_statisticsfrom oggm.sandbox import distribute_2dfrom oggm.sandbox.edu import run_constant_climate_with_biasimport geopandas as gpdimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport matplotlib.colors as clrsimport xarray as xrimport osimport seaborn as snsimport salemfrom matplotlib.ticker import FuncFormatterimport pandas as pdimport numpy as npfrom matplotlib import animationfrom IPython.display import HTML, displayimport matplotlib.pyplot as pltimport cartopy.crs as ccrsimport cartopy.feature as cfeatureimport geopandas as gpdimport pandas as pdfrom scipy.optimize import curve_fitfrom tqdm import tqdmimport pickleimport sysimport textwrapimport matplotlib.patches as mpatchesimport matplotlib.lines as mlinesfrom matplotlib.colors import LinearSegmentedColormap, TwoSlopeNormfunction_directory = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/src/03. Glacier simulations"sys.path.append(function_directory)# %% Cell 0: Set base parameterscolors = {    "W5E5": ["#000000"],  # "#000000"],  # Black    # Darker to lighter shades of purple    "E3SM": ["#785EF0", "#8F7BF1", "#A6A8F2"],    # Darker to lighter shades of pink    "CESM2": ["#DC267F", "#E58A9E", "#F0A2B6", "#F7BCC4"],    # Darker to lighter shades of orange    "CNRM": ["#FE6100", "#FE7D33", "#FE9A66", "#FEB799", "#FECDB5", "#FEF1E1"],    "IPSL-CM6": ["#FFB000"]  # Dark purple to lighter shades}members = [1, 3, 4, 6, 4, 1]members_averages = [1, 2, 3, 5, 3]models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]timeframe = "monthly"y0_clim = 1985ye_clim = 2014fig_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/02. OGGM simulations/01. Modelled output/3r_a1/'folder_path = '/Users/magaliponds/Documents/00. Programming'wd_path = f'{folder_path}/04. Modelled perturbation-glacier interactions - R13-15 A+1km2/'sum_dir = os.path.join(wd_path, 'summary')# %% Cell 1a: Load gdirs_3r from pkl (fastest way to get started)wd_path_pkls = f'{wd_path}/pkls/'gdirs_3r = []for filename in os.listdir(wd_path_pkls):    if filename.endswith('.pkl'):        # f'{gdir.rgi_id}.pkl')        file_path = os.path.join(wd_path_pkls, filename)        with open(file_path, 'rb') as f:            gdir = pickle.load(f)            gdirs_3r.append(gdir)# %% Cell 1b: Load gdirs_3r_a1 from pkl (fastest way to get started)wd_path_pkls = f'{wd_path}/pkls_subset_success/'gdirs_3r_a1 = []for filename in os.listdir(wd_path_pkls):    if filename.endswith('.pkl'):        # f'{gdir.rgi_id}.pkl')        file_path = os.path.join(wd_path_pkls, filename)        with open(file_path, 'rb') as f:            gdir = pickle.load(f)            gdirs_3r_a1.append(gdir)# # print(gdirs)# %% Cell 2: Create a dataset that contains Gdir Area, volume, rgi date and rgi ID# only for glaciers with an area larger than 5# Initialize lists to store outputrgi_data = []count = 0# Iterate through each glacier directoryfor gdir in gdirs_3r_a1:    count += 1    print((count*100)/len(gdirs_3r_a1))  # show progress in percentage    try:        # Create a temporary dictionary to hold data for this glacier        temp_data = {            'rgi_id': gdir.rgi_id,  # RGI ID            'rgi_region': gdir.rgi_region,  # RGI region            'rgi_subregion': gdir.rgi_subregion,  # RGI subregion            'cenlon': gdir.cenlon,            'cenlat': gdir.cenlat,            'rgi_date': gdir.rgi_date,  # Validity date            'rgi_area_km2': gdir.rgi_area_km2}  # Area corresponding to RGI date        # Load the model diagnostics        with xr.open_dataset(gdir.get_filepath('model_diagnostics', filesuffix="_historical")) as ds:            vol = ds.volume_m3[1]  # Use the most relevant volume data            temp_data['rgi_volume_km3'] = vol.values * 10**-9  # Convert to kmÂ³        # If no error, append the temp_data to the final list        rgi_data.append(temp_data)    except Exception as e:        # Print error message for debugging        print(f"Error processing {gdir.rgi_id}: {e}")        # Do not append the incomplete data# Create DataFrame if data was collectedif rgi_data:    df_3r_a1 = pd.DataFrame(rgi_data)else:    # Empty DataFrame if no data    df_3r_a1 = pd.DataFrame(columns=['rgi_id', 'rgi_region', 'rgi_subregion', 'cenlon', 'cenlat',                                  'rgi_date', 'rgi_area_km2', 'rgi_volume_km3'])# Output the DataFramedf_3r_a1.to_csv(f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V.csv")print(df_3r_a1.head)# %% Cell 3: Update the master dataset with RGI iddf_3r_a1 = pd.read_csv(f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V.csv", index_col=0)subregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected.shp"subregions = gpd.read_file(subregions_path)subregions = subregions[['o2region', 'full_name']]subregions = subregions.rename(columns={'o2region': 'rgi_subregion'})df_complete = pd.merge(df_3r_a1, subregions, on='rgi_subregion')new_order = ['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',             'rgi_area_km2', 'rgi_volume_km3']df_complete = df_complete[new_order]df_complete.to_csv(    f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")# %% Cell 4: Update the master dataset with B, A, RGI ID and location for all the different sample ids (541x15=8115) and the region namemaster_df = pd.read_csv(    f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv", index_col=0)data = []rgi_ids = []labels = []rgi_ids_sel=[]members = [1, 3, 4, 6, 4]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]# Iterate over models and members, collecting data for boxplots# only take the model shortlist as members are handled separatelyfor m, model in enumerate(models_shortlist):    for member in range(members[m]):        if model=="IPSL-CM6" or member>=1:            sample_id = f"{model}.0{member:02d}"  # Ensure leading zeros            print(sample_id)            i_path = os.path.join(                sum_dir, f'specific_massbalance_mean_{sample_id}.csv')                # Load the CSV file into a DataFrame and convert to xarray            mb = pd.read_csv(i_path, index_col=0).to_xarray()                        # Collect B values for each model and member            print(sample_id, len(mb.B.values))            data.append(mb.B.values)                            # Store RGI IDs only for the first model/member            if m == 0 and member == 0:                rgi_ids.append(mb.rgi_id.values)                labels.append(sample_id)i_path_base = os.path.join(sum_dir, 'specific_massbalance_mean_W5E5.000.csv')mb_base = pd.read_csv(i_path_base)base_array = np.array(mb_base.B)# Convert the list of data into a NumPy array and transpose itdata_array = np.array(data)# Shape: (number of B values, number of models * members)reshaped_data = data_array.T# Create a DataFrame for the reshaped datadf = pd.DataFrame(reshaped_data, index=rgi_ids, columns=np.repeat(labels, 1))df['B_irr'] = base_arraydf.rename_axis("rgi_id", inplace=True)df.reset_index(drop=False, inplace=True)# Step 1: Melt the DataFrame to get B_noirr valuesdf_melted = pd.melt(df, id_vars='rgi_id', value_vars=[col for col in df.columns if col != 'B_irr'],                    var_name='sample_id', value_name='B_noirr')# Step 2: Create a DataFrame with repeated B_irr values# Keep only the rgi_id and B_irr columnsb_irr_repeated = df[['rgi_id', 'B_irr']].copy()b_irr_repeated = b_irr_repeated.merge(    df_melted[['rgi_id', 'sample_id']], on='rgi_id')  # Ensure all combinations# Now merge B_irr with melted DataFramedf_complete = pd.merge(df_melted, b_irr_repeated, on=['rgi_id', 'sample_id'])df_complete['B_delta_irr'] = df_complete.B_irr-df_complete.B_noirr# Merge with rgis_complete# master_df = master_ds.to_dataframe()df_complete = pd.merge(df_complete, master_df, on='rgi_id', how='inner')new_order = ['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',             'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta_irr']df_complete = df_complete[new_order]# print(df_complete[1:5])df_complete.to_csv(    f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B.csv")# %% TEST Cell 4: Update the master dataset with B, A, RGI ID and location for all the different sample ids (541x15=8115) and the region namemaster_df = pd.read_csv(    f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")data = []rgi_ids = []labels = []members = [1, 3, 4, 6, 1]# Iterate over models and members, collecting data for boxplots# only take the model shortlist as members are handled separatelyfor m, model in enumerate(models_shortlist):    for member in range(members[m]):        sample_id = f"{model}.0{member:02d}"  # Ensure leading zeros        i_path = os.path.join(            sum_dir, f'specific_massbalance_mean_{sample_id}_2000_2014.csv')        # Load the CSV file into a DataFrame and convert to xarray        mb = pd.read_csv(i_path, index_col=0).to_xarray()        # Collect B values for each model and member        data.append(mb.B.values)        # Store RGI IDs only for the first model/member        if m == 0 and member == 0:            rgi_ids.append(mb.rgi_id.values)        labels.append(sample_id)i_path_base = os.path.join(    sum_dir, 'specific_massbalance_mean_W5E5.000_2000_2014.csv')mb_base = pd.read_csv(i_path_base, index_col=0)base_array = np.array(mb_base)# Convert the list of data into a NumPy array and transpose itdata_array = np.array(data)# Shape: (number of B values, number of models * members)reshaped_data = data_array.T# Create a DataFrame for the reshaped datadf = pd.DataFrame(reshaped_data,  index=rgi_ids, columns=np.repeat(labels, 1))df['B_irr'] = base_arraydf.rename_axis("rgi_id", inplace=True)df.reset_index(drop=False, inplace=True)# Step 1: Melt the DataFrame to get B_noirr valuesdf_melted = pd.melt(df, id_vars='rgi_id', value_vars=[col for col in df.columns if col != 'B_irr'],                    var_name='sample_id', value_name='B_noirr')# Step 2: Create a DataFrame with repeated B_irr values# Keep only the rgi_id and B_irr columnsb_irr_repeated = df[['rgi_id', 'B_irr']].copy()b_irr_repeated = b_irr_repeated.merge(    df_melted[['rgi_id', 'sample_id']], on='rgi_id')  # Ensure all combinations# Now merge B_irr with melted DataFramedf_complete = pd.merge(df_melted, b_irr_repeated, on=['rgi_id', 'sample_id'])df_complete['B_delta'] = df_complete.B_irr-df_complete.B_noirr# Merge with rgis_complete# master_df = master_ds.to_dataframe()df_complete = pd.merge(df_complete, master_df, on='rgi_id', how='inner')new_order = ['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',             'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta']df_complete = df_complete[new_order]df_complete.to_csv(    f"{wd_path}/masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_2000_2014.csv")# %% Cell 5: Open all hugonnet data and align with rgi_ids from r3_a1def str_to_float(value):    return float(value)all_data = pd.DataFrame(columns=['B', 'rgi_id'])regions = [13, 14, 15]for region in regions:    Hugonnet_path = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/04. Reference/01. Climate data/Hugonnet/aggregated_2000_2020/{region}_mb_glspec.dat"    df_raw = pd.read_csv(Hugonnet_path)    # Convert the pandas DataFrame to an Xarray Dataset    ds = xr.Dataset.from_dataframe(df_raw)    # Load the data variables, as they are listed in the first row    var_index = ds.coords['index'].values    header_str = var_index[1]  # Set the var_index as the header string    data_rows_str = var_index[2:]  # Set the datarows    # # print(header_str, data_rows_str)    # # split data input in such a way that it is loaded as dataframe with columns headers and data in table below    header = header_str.split()    # # Transform the data type from string values to integers for the relevant columns    data_rows = [row.split() for row in data_rows_str]    hugo_ds = pd.DataFrame(data_rows, columns=header)    for col in hugo_ds.columns:        if col != 'RGI-ID':  # Exclude column 'B'            hugo_ds[col] = hugo_ds[col].apply(str_to_float)    # # # create a dataset from the transformed data in order to select the required glaciers    hugo_ds = hugo_ds.rename(columns={'RGI-ID': 'rgi_id'})    new_data = pd.DataFrame({        # Repeat the rgi_id for each B value        'rgi_id': hugo_ds['rgi_id'].values,        'Area': hugo_ds['Area'].values,        # Assuming this is a 1D array or single value        'B': hugo_ds['B'].values,        'errB': hugo_ds['errB'].values    })    # Concatenate the new data into the main DataFrame    all_data = pd.concat([all_data, new_data], ignore_index=True)all_data.to_csv(    f"{wd_path}masters/master_hugo_only_all_glaciers_13_14_15.csv")master = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B.csv", index_col=0)hugo_ds_tot = pd.merge(master, all_data, on='rgi_id', how='left')hugo_ds_tot = hugo_ds_tot.rename(columns={'B': 'B_hugo','errB': 'errB_hugo'})hugo_ds_tot.to_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")hugo_df = hugo_ds_tot[['rgi_id', 'B_hugo']]#%% Add Comitted mass loss for boxplots new master dsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")# df = pd.read_csv(#     f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]members_averages = [2, 3,  3 ,5, 1 ]models_shortlist = ["E3SM", "CESM2",  "NorESM",  "CNRM", "IPSL-CM6"]filepath=f"climate_run_output_baseline_W5E5.000_comitted_random.nc"# filepath_start=f"climate_run_output_baseline_W5E5.000.nc"#open and add comitted mass loss baseline_path = os.path.join(    wd_path, "summary", filepath)baseline = xr.open_dataset(baseline_path)baseline_end=baseline.sel(time=2264) #use final yearprint(len(baseline_end.volume))com = baseline_end[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]com = com.rename(columns={'volume': 'V_2264_irr'})merged = pd.merge(master_ds, com, on=['rgi_id'], how='left')start_baseline = xr.open_dataset(os.path.join( wd_path, "summary", f"climate_run_output_baseline_W5E5.000.nc"))for time_sel in [1985,2014]:    #open and add initial simulation volume     baseline_start=start_baseline.sel(time=time_sel) #use initial year    start = baseline_start[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]    start = start.rename(columns={'volume': f'V_{time_sel}_irr'})    merged = pd.merge(merged, start, on=['rgi_id'], how='left')all_com_noi = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        filepath = f'climate_run_output_perturbed_{sample_id}_comitted_random.nc'                noirr_path = os.path.join(            wd_path, "summary", filepath)        noirr_all = xr.open_dataset(noirr_path)        noirr_all=noirr_all.sel(time=2264) #use final year        print(len(noirr_all.volume))        com_noi = noirr_all[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]        com_noi = com_noi.rename(columns={'volume': 'V_2264_noirr'})        com_noi['sample_id']=sample_id        all_com_noi.append(com_noi)     all_com_noi_df = pd.concat(all_com_noi, ignore_index=True)merged = pd.merge(merged, all_com_noi_df, on=['rgi_id', 'sample_id'], how='left')merged.to_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")for time_sel in [1985,2014]:    all_com_noi = []    for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each         for j in range(members_averages[m]):            sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"            filepath = f'climate_run_output_perturbed_{sample_id}.nc'            noirr_path = os.path.join(                wd_path, "summary", filepath)            noirr_all = xr.open_dataset(noirr_path)            noirr_all=noirr_all.sel(time=time_sel) #use final year            print(len(noirr_all.volume))            com_noi = noirr_all[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]            com_noi = com_noi.rename(columns={'volume': f'V_{time_sel}_noirr'})            com_noi['sample_id']=sample_id            all_com_noi.append(com_noi)             all_com_noi_df = pd.concat(all_com_noi, ignore_index=True)        merged = pd.merge(merged, all_com_noi_df, on=['rgi_id', 'sample_id'], how='left')merged.to_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv"")#%% Add Comitted mass loss for boxplots new master dsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")# df = pd.read_csv(#     f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]members_averages = [2, 3,  3 ,5]models_shortlist = ["E3SM", "CESM2",  "NorESM",  "CNRM"]filepath=f"climate_run_output_baseline_W5E5.000_comitted_random.nc"# filepath_start=f"climate_run_output_baseline_W5E5.000.nc"#open and add comitted mass loss baseline_path = os.path.join(    wd_path, "summary", filepath)baseline = xr.open_dataset(baseline_path)baseline_end=baseline.sel(time=2264) #use final yearprint(len(baseline_end.volume))com = baseline_end[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]com = com.rename(columns={'volume': 'V_2264_irr'})merged = pd.merge(master_ds, com, on=['rgi_id'], how='left')start_baseline = xr.open_dataset(os.path.join( wd_path, "summary", f"climate_run_output_baseline_W5E5.000.nc"))for time_sel in [1985,2014]:    #open and add initial simulation volume     baseline_start=start_baseline.sel(time=time_sel) #use initial year    start = baseline_start[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]    start = start.rename(columns={'volume': f'V_{time_sel}_irr'})    merged = pd.merge(merged, start, on=['rgi_id'], how='left')all_com_noi = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        filepath = f'climate_run_output_perturbed_{sample_id}_comitted_random.nc'                noirr_path = os.path.join(            wd_path, "summary", filepath)        noirr_all = xr.open_dataset(noirr_path)        noirr_all=noirr_all.sel(time=2264) #use final year        print(len(noirr_all.volume))        com_noi = noirr_all[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]        com_noi = com_noi.rename(columns={'volume': 'V_2264_noirr'})        com_noi['sample_id']=sample_id        all_com_noi.append(com_noi)     all_com_noi_df = pd.concat(all_com_noi, ignore_index=True)merged = pd.merge(merged, all_com_noi_df, on=['rgi_id', 'sample_id'], how='left')merged.to_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")for time_sel in [1985,2014]:    all_com_noi = []    for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each         for j in range(members_averages[m]):            sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"            filepath = f'climate_run_output_perturbed_{sample_id}.nc'            noirr_path = os.path.join(                wd_path, "summary", filepath)            noirr_all = xr.open_dataset(noirr_path)            noirr_all=noirr_all.sel(time=time_sel) #use final year            print(len(noirr_all.volume))            com_noi = noirr_all[['volume']].to_dataframe().reset_index()[['rgi_id', 'volume']]            com_noi = com_noi.rename(columns={'volume': f'V_{time_sel}_noirr'})            com_noi['sample_id']=sample_id            all_com_noi.append(com_noi)             all_com_noi_df = pd.concat(all_com_noi, ignore_index=True)        merged = pd.merge(merged, all_com_noi_df, on=['rgi_id', 'sample_id'], how='left')merged.to_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom_noIPSL.csv") #%% Calculate % of glacier area and volume included in subsetcfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # ðŸ”§ set number of coresdef main():    cfg.PARAMS['use_multiprocessing']=False        # Compile the stats into a single DataFrame    ofilesuffix='gdir_3r_a1_share_VA_analysis.csv'    opath=f'{wd_path}/masters/'    df = compile_glacier_statistics(gdirs_3r, path=opath, filesuffix=ofilessufix)        # Define thresholds    thresholds = [5, 2, 1]    summary_data = []        # Total area and volume (for % calculation)    total_area = df['area_km2'].sum()    total_volume = df['volume_km3'].sum()        for t in thresholds:        sub = df[df['area_km2'] > t]        n_glaciers = len(sub)        area = sub['area_km2'].sum()        volume = sub['volume_km3'].sum()        summary_data.append({            'Threshold (A > kmÂ²)': t,            '# Glaciers': n_glaciers,            'Total Area (kmÂ²)': area,            'Area % of Total': area / total_area * 100,            'Total Volume (kmÂ³)': volume,            'Volume % of Total': volume / total_volume * 100        })        # Create summary table    summary_df = pd.DataFrame(summary_data)        # Display the result    import ace_tools as tools; tools.display_dataframe_to_user(name="Glacier Area Threshold Summary", dataframe=summary_df)if __name__ == '__main__':    multiprocessing.set_start_method('spawn', force=True)    main()            