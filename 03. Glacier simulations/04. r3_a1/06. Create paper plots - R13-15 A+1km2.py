# -*- coding: utf-8 -*-# %% Cell 0: Load data packages#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Wed Jul  3 15:36:52 2024@author: magalipondsThis script runs makes plots of the selected 3 regions (13-14-15) with Areas larger than 1km2 (3r_a1)"""# -*- coding: utf-8 -*-import oggm# from OGGM_data_processing import process_perturbation_data# import mpl_axes_alignerimport concurrent.futuresfrom shapely.geometry import LineString, MultiLineStringimport astimport stringfrom matplotlib.lines import Line2Dimport oggmfrom oggm import utils, cfg, workflow, tasks, DEFAULT_BASE_URL, graphics, global_tasksfrom oggm.core import massbalance, flowlinefrom oggm.utils import floatyear_to_date, hydrodate_to_calendardatefrom oggm.sandbox import distribute_2dfrom oggm.sandbox.edu import run_constant_climate_with_biasimport geopandas as gpdimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport matplotlib.colors as clrsfrom matplotlib.colors import ListedColormap, BoundaryNormfrom matplotlib.patches import ConnectionPatchimport xarray as xrimport osimport seaborn as snsimport salemfrom matplotlib.ticker import FuncFormatterimport pandas as pdimport numpy as npfrom matplotlib import animationfrom IPython.display import HTML, displayimport matplotlib.pyplot as pltimport cartopy.crs as ccrsimport cartopy.feature as cfeatureimport geopandas as gpdimport pandas as pdfrom scipy.optimize import curve_fitfrom scipy import statsfrom matplotlib.legend_handler import HandlerTuplefrom matplotlib.colors import LightSourcefrom tqdm import tqdmimport pickleimport sysimport textwrapimport matplotlib.patches as mpatchesimport matplotlib.lines as mlinesfrom matplotlib.colors import LinearSegmentedColormap, TwoSlopeNorm, Normalize, ListedColormapfrom mpl_toolkits.axes_grid1.inset_locator import inset_axesfrom shapely.geometry import Pointimport matplotlib.gridspec as gridspecfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTERfrom matplotlib.patches import Rectangle, ConnectionPatchfrom mpl_toolkits.axes_grid1.inset_locator import inset_axesfunction_directory = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/src/03. Glacier simulations"sys.path.append(function_directory)# %% Cell 1: Set base parameters# colors = {#     "irr": ["#000000", "#777777"],#     "noirr": ["#6363FF", "#B1B1FF"],#     # much lighter versions of noirr colors#     "noirr_com": ["#A6A6FF", "#E0E0FF"],#     # much lighter grey fade of irr colors#     "irr_com": ["#C0C0C0", "#E0E0E0"],#     "cf": ["#FF5722", "#FFA780"],#     "Yellow": ["#FFC107", "#FFE08A"]# }xkcd_colors = clrs.XKCD_COLORS# colors = {#     "irr": ["#000000", "#555555"],  # Black and dark gray#     # Darker brown and golden yellow for contrast#     # "noirr": ["#f5bf03","#fbeaac"],#["#8B5A00", "#D4A017"], #fdde6c#     "noirr": ["dimgrey","darkgrey"],#["#8B5A00", "#D4A017"], #fdde6c#     # "noirr_com": ["#E3C565", "#F6E3B0"],  # Lighter, distinguishable tan shades ##     # "noirr_com": ["#380282", "#ceaefa"],  # Lighter, distinguishable tan shades ##     "noirr_com": ["#FFC107", "#FFF3CD"],  # Lighter, distinguishable tan shades ##     "irr_com": ["#fe4b03", "#D0D0D0"],  # Light gray, no change#     # "irr_com": ["#B5B5B5", "#D0D0D0"],  # Light gray, no change#     "cf": ["#004C4C", "#40E0D0"],#     "cf_com": ["#008B8B", "#40E0D0"],#     "cline": ["dimgrey", '#FFC107']# }colors = {    "irr": ["#000000", "#555555"],  # Black and dark gray    "noirr": ["dimgrey","darkgrey"],#["#8B5A00", "#D4A017"], #fdde6c    # "noirr_com": ["#FFC107", "#FFF3CD"],  # Lighter, distinguishable tan shades #    # "irr_com": ["#fe4b03", "#D0D0D0"],  # Light gray, no change    # "noirr_com": ["#FFA500", "#FFD580"],    # "irr_com": ["#00796B", "#388E3C"],    # Teal and deep green (distinct from SSPs)    # "noirr_com": ["#80CBC4", "#A5D6A7"],  # Light turquoise and mint    "irr_com": ["#A0522D", "#BF6C38"],     # Dry, earthy rust tones    "noirr_com": ["#E6A87D", "#F3D4B3"],   # Muted peach and tan    "cf": ["#004C4C", "#40E0D0"],    "cf_com": ["#008B8B", "#40E0D0"],    "cline": ["dimgrey", '#E6A87D']}region_colors = {13: 'blue', 14: 'crimson', 15: 'orange'}colors_models = {    "W5E5": ["#000000"],  # "#000000"],  # Black    # Darker to lighter shades of purple    "E3SM": ["#785EF0", "#8F7BF1", "#A6A8F2"],    # Darker to lighter shades of pink    "CESM2": ["#DC267F", "#E58A9E", "#F0A2B6", "#F7BCC4"],    # Darker to lighter shades of orange    "CNRM": ["#FE6100", "#FE7D33", "#FE9A66", "#FEB799", "#FECDB5", "#FEF1E1"],    "IPSL-CM6": ["#FFB000"],    "NorESM": ["#FFC107", "#FFE08A"]  # Dark purple to lighter shades}members = [1, 3, 4, 6, 4, 1]members_averages = [1, 2, 3, 5, 3]models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]timeframe = "monthly"y0_clim = 1985ye_clim = 2014fig_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/98. Final Figures A+1km2/'folder_path = '/Users/magaliponds/Documents/00. Programming'wd_path = f'{folder_path}/04. Modelled perturbation-glacier interactions - R13-15 A+1km2/'sum_dir = os.path.join(wd_path, 'summary')# %% Cell 1b: Load gdirswd_path_pkls = f'{wd_path}/pkls_subset_success/'gdirs_3r_a1 = []for filename in os.listdir(wd_path_pkls):    if filename.endswith('.pkl'):        # f'{gdir.rgi_id}.pkl')        file_path = os.path.join(wd_path_pkls, filename)        with open(file_path, 'rb') as f:            gdir = pickle.load(f)            gdirs_3r_a1.append(gdir)#%% Cell 2a: map plot - prepare data"""Process master for map plot """# Load datasetsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]# master_ds = df#df[(~df['sample_id'].str.endswith('0')) ]# master_ds = master_ds[(master_ds['sample_id'].str.startswith('E3SM'))] #Temporarymaster_ds = master_ds.reset_index(drop=True)# Normalize values# divide all the B values with 1000 to transform to m w.e. average over 30 yrs# master_ds[['B_noirr', 'B_irr', 'B_delta_irr', 'B_cf',  "B_delta_cf"]] /= 1000master_ds.loc[:, ['B_noirr', 'B_irr', 'B_delta']] /= 1000master_ds = master_ds[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta']]# Define custom aggregation functions for grouping over the 11 member dataaggregation_functions = {    'rgi_region': 'first',    'rgi_subregion': 'first',    'full_name': 'first',    'cenlon': 'first',    'cenlat': 'first',    'rgi_date': 'first',    'rgi_area_km2': 'first',    'rgi_volume_km3': 'first',    'B_noirr': 'mean',    'B_irr': 'mean',    'B_delta': 'mean',    'sample_id': 'first'}master_ds_avg = master_ds.groupby(['rgi_id'], as_index=False).agg({    'B_delta': 'mean',    'B_noirr': 'mean',    'B_irr': 'mean',    # lamda is anonmous functions, returns 11 member average    'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id']}})master_ds_avg.to_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")# Aggregate data for scatter plotmaster_ds_avg['grid_lon'] = np.floor(master_ds_avg['cenlon'])master_ds_avg['grid_lat'] = np.floor(master_ds_avg['cenlat'])# Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member averageaggregated_ds = master_ds_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({    'B_delta': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'B_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'B_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'rgi_area_km2': 'sum',  # Sum for area    'rgi_volume_km3': 'sum',  # Sum for volume    # 'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id', 'rgi_area_km2', 'rgi_volume_km3']}})aggregated_ds.rename(    columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)aggregated_ds.to_csv(    f"{wd_path}masters/complete_master_processed_for_map_plot.csv")# f"{wd_path}masters/complete_master_processed_for_map_plot_E3SM.csv")#%% Cell 2b: map plot - Process ∆ volume datamembers_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]climate_run_output_irr = xr.open_dataset(os.path.join(    sum_dir, f'climate_run_output_baseline_W5E5.000.nc'))initial_volume=climate_run_output_irr.volume.sel(time=1985) #only select first year for initial valuedelta_volume_irr = climate_run_output_irr.volume - initial_volumeall_datasets = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                climate_run_output_noirr = xr.open_dataset(os.path.join(            sum_dir, f'climate_run_output_perturbed_{sample_id}.nc'))        delta_volume_noirr = climate_run_output_noirr.volume - initial_volume        delta_volume_noirr_rel = xr.where(delta_volume_irr != 0, delta_volume_noirr / delta_volume_irr, 0)                        yearly_mean_volume = climate_run_output_noirr.mean(dim='time')                ds_new = xr.Dataset({            "delta_volume_irr": delta_volume_irr,            "delta_volume_noirr": delta_volume_noirr,            "sample_id": [sample_id],            "delta_volume_noirr_rel": delta_volume_noirr_rel        })        # Add a new coordinate for sample_id        # ds_new = ds_new.expand_dims(dim={"sample_id": [sample_id]})        # Append to list        all_datasets.append(ds_new)        volume_change_ds = xr.concat(all_datasets, dim="sample_id")volume_change_ds["delta_volume_noirr_rel"].attrs["description"] = "Relative volume change ∆ NoIrr (resp. Irr-1985)/ ∆ Irr (resp. Irr-1985)"# Group by rgi_id and time, then compute the mean for numeric variablesvolume_ds_avg = volume_change_ds.mean(dim="sample_id").mean(dim="time") #30-yr average values for all 14 members in the model ensembleopath_bymember = os.path.join(wd_path,"masters", 'delta_volume_evolution_bymember.nc')volume_change_ds.to_netcdf(opath_bymember)opath_averaged = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average.nc')opath_averaged_csv = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average.csv')# opath_averaged = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average_CESM2.nc')# opath_averaged_csv = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average_CESM2.csv')volume_ds_avg.to_netcdf(opath_averaged)volume_df_avg = volume_ds_avg.to_dataframe()master_df = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")[['rgi_id', 'cenlon','cenlat', 'rgi_area_km2']]volume_df_avg = volume_df_avg.merge(    master_df, on="rgi_id", how="left")# Aggregate data for scatter plotvolume_df_avg['grid_lon'] = np.floor(volume_df_avg['cenlon'])volume_df_avg['grid_lat'] = np.floor(volume_df_avg['cenlat'])# Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member averagevolume_df_avg = volume_df_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({    'delta_volume_irr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'delta_volume_noirr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'delta_volume_noirr_rel': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'rgi_area_km2': 'sum',  # Sum for area    # 'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in volume_df_avg.columns if col not in ['delta_volume_irr', 'delta_volume_noirr', 'delta_volume_noirr_rel', 'rgi_area_km2']}})volume_df_avg.rename(    columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)volume_df_avg.to_csv(opath_averaged_csv)#%% Cell 2c: map plot - prepare volume evolution#tracker regions = [13, 14, 15]subregions = [9, 3, 3]region_data = []members_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2", "CNRM", "IPSL-CM6","NorESM"]        # Storage for time seriessubregion_series = {}  # subregion → DataArray[time, member]global_series = []     # total average over all subregions per membermembers_all = []       # (model, member_id) pairs to track 14-member averagefor m, model in enumerate(models_shortlist):    for j in range(members_averages[m]):        member_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        sample_id = member_id  # used in filename        print(sample_id)        # Open NetCDF file        path = os.path.join(sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')        ds = xr.open_dataset(path, engine="h5netcdf")        members_all.append(member_id)        # Per subregion        for reg, region in enumerate(regions):            for sub in range(subregions[reg]):                region_id = f"{region}-0{sub+1}"                subregion_ds = master_ds_avg[                    master_ds_avg['rgi_subregion'].str.contains(region_id)                ]                if subregion_ds.empty:                    continue                # Filter data                ds_filtered = ds.where(                    ds['rgi_id'].isin(subregion_ds.rgi_id.values), drop=True)                # Sum volume over glaciers in this subregion                vol_timeseries = ds_filtered["volume"].sum(dim="rgi_id")  # dims: time                key = region_id                if key not in subregion_series:                    subregion_series[key] = []                subregion_series[key].append(vol_timeseries.assign_coords(member=member_id))        # Add global average (sum over all glaciers, no filter)        global_total = ds["volume"].sum(dim="rgi_id")  # dims: time        global_series.append(global_total.assign_coords(member=member_id))# ---------------------------------------#  Convert per-subregion data to Datasetsubregion_datasets = {}for region_id, series_list in subregion_series.items():    da = xr.concat(series_list, dim="member")    da = da.transpose("time", "member")    da_avg = da.mean(dim="member").assign_coords(member="14-member-avg")    da_full = xr.concat([da, da_avg.expand_dims(dim="member")], dim="member")    subregion_datasets[region_id] = da_full# Merge all subregions into one dataset with region_id as a new dimensionds_subregions = xr.concat(    [ds.expand_dims(subregion=[key]) for key, ds in subregion_datasets.items()],    dim="subregion")# ---------------------------------------# Create global average datasetglobal_all = xr.concat(global_series, dim="member").transpose("time", "member")global_avg = global_all.mean(dim="member").assign_coords(member="14-member-avg")global_full = xr.concat([global_all, global_avg.expand_dims(dim="member")], dim="member")# ---------------------------------------# Now you have:# - ds_subregions: [time, member, subregion]# - global_full:   [time, member]# You can save them if needed:ds_subregions.to_netcdf(f"{wd_path}masters/master_volume_ts_subregions_members.nc")global_full.to_netcdf(f"{wd_path}masters/master_volume_ts_global_members.nc")# ds_subregions.to_netcdf(f"{wd_path}masters/master_volume_ts_subregions_members_CESM2.nc")# global_full.to_netcdf(f"{wd_path}masters/master_volume_ts_global_members_CESM2.nc")#%% Cell 3: Create map plot - ∆B or ∆Vmembers_averages = [2, 3,    5 ,1,3]models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]""" Load data"""aggregated_ds = pd.read_csv(    f"{wd_path}masters/complete_master_processed_for_map_plot.csv")aggregated_ds_vol = pd.read_csv(    f"{wd_path}masters/delta_volume_evolution_ensemble_average.csv")gdf = gpd.GeoDataFrame(aggregated_ds, geometry=gpd.points_from_xy(    aggregated_ds['lon'] +0.5 , aggregated_ds['lat'] + 0.5),    crs="EPSG:4326")gdf_volume = gpd.GeoDataFrame(aggregated_ds_vol, geometry=gpd.points_from_xy(    aggregated_ds_vol['lon'] +0.5 , aggregated_ds_vol['lat'] + 0.5),    crs="EPSG:4326")""" Plot shaded relief """fig, ax = plt.subplots(figsize=(15,12), subplot_kw={                       'projection': ccrs.PlateCarree()})dem_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/02. DEM/ETOPO2v2c_f4.nc"with xr.open_dataset(dem_path) as ds:    dem=ds['z']# Print dataset info to find variable namesdem = dem.where((dem.x >= 54.5) & (dem.x <= 115) & (dem.y >= 16) & (dem.y <= 55), drop=True)dx =(dem['x'][1]-dem['x'][0]).valuesdy =(dem['y'][1]-dem['y'][0]).valuesz = dem.valuesx, y = np.meshgrid(dem.x, dem.y)# Create a LightSource object for hillshadingls = LightSource(azdeg=315, altdeg=45)# Overlay the hillshade (for relief effect)im = ax.imshow(ls.hillshade(z, vert_exag=10, dx=0.01, dy=0.01),extent=[dem.x.min(), dem.x.max(), dem.y.min(), dem.y.max()],          transform=ccrs.PlateCarree(), alpha=0.1, cmap="grey_r")  # Adjust alpha for effect# Add geographical features# ax.add_feature(cfeature.COASTLINE, linewidth=0.7)# ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle="--")# ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='none')ax.add_feature(cfeature.RIVERS, edgecolor='blue', facecolor='none', linewidth=0.6, alpha=0.6)"""Set up plot incl shapefile"""subregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"subregions = gpd.read_file(subregions_path).to_crs('EPSG:4326')ax.spines['geo'].set_visible(False)# Optionally, remove gridlines# Remove gridlines properlygl = ax.gridlines(draw_labels=False)  # Create gridlines without labelsgl.xlines = False  # Remove longitude linesgl.ylines = False  # Remove latitude lines""" Plot subregions"""# define movements for the annotation of subregionsmovements = {    '13-01': [0, -0.2], #A    '13-02': [4.8, 2], #    '13-03': [-0.2, -0.6], #B    '13-04': [-0.6, 0], #C    '13-05': [-4.6,2.5],#-0.2, 0.8], #E    '13-06': [-10, 0], #F    '13-07': [-1, 1], #G    '13-08': [-2, -0.5], #J    '13-09': [1.5, 0], #O    '14-01': [0.5, -0.8], #H    '14-02': [2,-1.8],#[1, -0.4], #I    '14-03': [3.4, -3.5], #K    '15-01': [0, 0], #L    '15-02': [-2, -1], #M    '15-03': [3.5, 4.5], #N}# annotate subregions# i=0   # Create a ListedColormap with some colors (example)""" Include the Mass Balance plot including color bar"""h_leg = 0.65w_leg = 0.646cax = fig.add_axes([0.76, h_leg-0.015, 0.03, 0.06])  # [left, bottom, width, height]scatter_data = "B"# Define vmin and vmax for the color scale (asymmetric)vmin, vmax = -0.2, 0.7  # Asymmetric rangestart =(abs(vmin)/(vmax-vmin))#Trim cmap so that white is at 0n_colors=256half=n_colors//2original_cmap = plt.cm.bwr.reversed()  # Blue-White-Red colormap# colors_trimmed = [#     (0.0, "#fdc1c5"),   # lighter red at -0.2#     (0.222, "white"),   # white at 0.0 → (0 - (-0.2)) / (0.7 - (-0.2)) = ~0.222#     (1.0, "#0203e2")    # dark blue at 0.7# ]colors_trimmed = [    (0.0,  "#fdc1c5"),   # Light red at far negative    (0.222, "white"),    # White at 0    (0.5, "#6a79f7"),   # Deep blue at moderate positive #0203e2 0.65 limit before    (1.0,  "#110954")    # Purple at strong positive    ]# Create custom colormapcustom_cmap = LinearSegmentedColormap.from_list("custom_trimmed_rwb", colors_trimmed, N=256)# Normalize to match data rangenorm = Normalize(vmin=-0.2, vmax=0.7)# Create the colorbarcbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm,cmap=custom_cmap ),                    cax=cax, orientation="vertical")# Define **evenly spaced** tick positionsnum_ticks = 2  # Adjust as neededtick_values = np.linspace(vmin, vmax, num_ticks)  # Ensures equal spacing# Apply ticks and labelscbar.set_ticks(tick_values)cbar.set_ticklabels([f'{b:.1f}' for b in tick_values])# Define the boundaries for each color blockcbar.ax.tick_params(labelsize=18) # Set the colorbar labelcbar.set_label('∆B$_{Irr}$ \nm yr$^{-1}$', fontsize=18, ha='left', rotation=0, labelpad=0, linespacing=0.8)cbar.ax.yaxis.label.set_position((1.1, 1.4))  # (X, Y) - Adjust Y to move upcbar.ax.yaxis.set_label_position('left')gdf["marker_size"]=(np.sqrt(gdf['rgi_area_km2'])*5)**1.3scatter = ax.scatter(gdf.geometry.x, gdf.geometry.y,                     s=gdf['marker_size'], c=gdf['B_delta'], cmap=custom_cmap, norm=norm, edgecolor='k', alpha=0.8)#plot subregion linesfor attribute, subregion in subregions.groupby('o2region'):   # Uncomment for coloring of specific subregions    linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"    # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,    subregion.plot(ax=ax, edgecolor='black', linewidth=1.2,                  facecolor='none') # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion    # i+=1    """ Add volume legend"""custom_sizes = [200, 2000]#500, 1000, 2000, 3000]  # Example sizes for the legend (area)size_labels = [f"{size:.0f}" for size in custom_sizes]  # Create labels for sizes# Create legend handles (scatter points with different sizes)legend_handles = [    plt.scatter([], [], s=(np.sqrt(size)*5)**1.3, edgecolor='k', facecolor='none')    for size in custom_sizes]  # Adjust size factor if neededtext_handles = [Line2D([0], [0], linestyle="none", label=label) for label in size_labels]# Create a separate axis for the legendcax_legend = fig.add_axes([w_leg -0.015, h_leg, 0.15, 0.1])  # [left, bottom, width, height]# Remove axis visualscax_legend.set_frame_on(False)  # Hide framecax_legend.set_xticks([])  # Remove x-tickscax_legend.set_yticks([])  # Remove y-ticks# Add legend to the separate axislegend = cax_legend.legend(    legend_handles, size_labels, loc="center",    fontsize=18, ncol=1,frameon=False, title="Total Area \n km$^2$", title_fontsize=18, scatterpoints=1, labelspacing=1)    # handler_map={tuple: HandlerTuple(ndivide=None)}, labelspacing=0.3, columnspacing=1, handletextpad=0.3)# Adjust legend marker positions (stacked effect)"""Comment from here to remove all panels"""""" Add call out plots for the different regions"""# Define and iterate over grid layoutlayout = [["13-01", "13-03", "13-04", "14-02", "13-05", "13-06"], ["13-02", "", "", "","", "13-07"], [    "14-01", "", "", "","", "13-08"], ["","","", "","", "13-09"], ["","","14-03", "15-01", "15-02", "15-03"]]w = 0.14w_space=0.014#0.035h = w/0.17*0.17#w/0.17*0.15 h_space=0.04 #0.06start_x=0.07#-0.12start_y = 0.82 #0.9y_buffer=0.03 #0.05nr_cols=6nr_rows=5# grid_positions = [[-0.15+ col * (w + 0.06), 0.82 - (h + 0.09) * row - 0.05, w, h]#                   if layout[row][col] else None for row in range(4) for col in range(5)]grid_positions = [[start_x + col * (w + w_space), start_y - (h + h_space) * row - y_buffer, w, h]                  if layout[row][col] else None for row in range(nr_rows) for col in range(nr_cols)]x_min, x_max = start_x, start_x + nr_cols * (w + w_space)y_min, y_max = start_y, start_y - (h + h_space) * nr_rows - y_buffersubregion_ids = list(string.ascii_uppercase)  i=0# print(subregion_ids)  ds_subregions = xr.open_dataset(f"{wd_path}masters/master_volume_ts_subregions_members.nc")global_full = xr.open_dataset(f"{wd_path}masters/master_volume_ts_global_members.nc")for idx, pos in enumerate(grid_positions):    if pos:        subregion_id=subregion_ids[i]        i+=1        ax_callout = fig.add_axes(pos, ylim=(-32,20),facecolor='white')#'#E9E9E9'whitesmoke            region_id = layout[idx // nr_cols][idx % nr_cols] #index columns and rows, // is rounded by full nrs                        # Find the corresponding subregion to add axes        subregion = subregions[subregions['o2region'] == region_id] #convert to meters        #load data in the plots        subregion_ds = master_ds_avg[master_ds_avg['rgi_subregion'].str.contains(            f"{region_id}")]        mask = (subregion_ds["rgi_subregion"] == region_id)        subregion_name = subregion_ds.full_name.iloc[0] #if error reload data above        # Baseline and model plotting        baseline_path = os.path.join(            wd_path, "summary", f"climate_run_output_baseline_W5E5.000.nc")        baseline = xr.open_dataset(baseline_path, engine='netcdf4')        rgi_ids_in_baseline = baseline['rgi_id'].values        matching_rgi_ids = np.intersect1d(            rgi_ids_in_baseline, subregion_ds.rgi_id.values)        baseline_filtered = baseline.sel(rgi_id=matching_rgi_ids)        # Check if there are any matching rgi_id values        # Ensure that rgi_id values exist in both datasets                initial_volume = baseline_filtered['volume'].sum(dim="rgi_id")[0].values        # Plot model member data        ax_callout.plot(baseline_filtered["time"].values, baseline_filtered['volume'].sum(dim="rgi_id")/initial_volume*100-100,                        label="W5E5.000", color=colors['irr'][0], linewidth=3, zorder=15)        filtered_member_data = []                        # Temporarily commented to speed up the plotting        for m, model in enumerate(models_shortlist):            for j in range(members_averages[m]):                sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                # with xr.open_dataset(os.path.join(                #     sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')) as climate_run_output:                #     climate_run_output = climate_run_output.where(                #         climate_run_output['rgi_id'].isin(subregion_ds.rgi_id.values), drop=True)                climate_run_output = ds_subregions.sel(member=sample_id).sel(subregion=region_id)                 # ax_callout.plot(climate_run_output["time"].values, climate_run_output["volume"]/initial_volume*100-100, label=sample_id, color="grey", linewidth=1, linestyle="dotted")                # ax_callout.plot(climate_run_output["time"].values, climate_run_output["volume"].sum(                #     dim="rgi_id")/initial_volume*100-100, label=sample_id, color="grey", linewidth=1, linestyle="dotted")                # filtered_member_data.append(                #     climate_run_output["volume"].sum(dim="rgi_id").values/initial_volume*100-100)                        # Mean and range plotting        mean_values = ds_subregions.sel(subregion=region_id).sel(member='14-member-avg').volume.values/initial_volume*100-100 #np.mean(filtered_member_data, axis=0).flatten()        min_values = ds_subregions.sel(subregion=region_id).min(dim="member").volume.values/initial_volume*100-100 #np.min(filtered_member_data, axis=0).flatten()        max_values = ds_subregions.sel(subregion=region_id).max(dim="member").volume.values/initial_volume*100-100# np.max(filtered_member_data, axis=0).flatten()        std_values = ((ds_subregions.sel(subregion=region_id).volume / initial_volume) * 100 - 100).std(dim="member").values        ax_callout.plot(climate_run_output["time"].values, mean_values,                        color=colors['noirr'][0], linestyle='dashed', lw=3, label=f"{sum(members_averages)}-member average")        # ax_callout.fill_between(        #     climate_run_output["time"].values, min_values, max_values, color=colors['noirr'][1], alpha=0.3)#color="lightblue", alpha=0.3)        ax_callout.fill_between(climate_run_output["time"].values, (mean_values-std_values), (mean_values+std_values), color=colors['noirr'][1], alpha=0.3, label=r"Historical NoIrr 1$\sigma$" )        ax_callout.plot(climate_run_output["time"].values, np.ones(len(climate_run_output["time"]))*100-100, color="black", linewidth=1, linestyle="dashed")        # Subplot formatting        if region_id=="13-02":            subregion_title ="Pamir"        elif region_id=="13-04":            subregion_title ="East Tien Shan"        elif region_id=="13-06":            subregion_title ="East Kun Lun"        else:            subregion_title=subregion_name                    ax_callout.set_title(f"{subregion_title}", bbox=dict(            facecolor='none', edgecolor='none', pad=1), fontsize=18) # fontweight="bold"        # Count the number of glaciers (assuming each 'rgi_id' represents a glacier)        glacier_count = subregion_ds['rgi_id'].nunique()        subregion_volume = round(subregion_ds['rgi_volume_km3'].sum())        # Add number of glaciers as a text annotation in the lower left corner        ax_callout.text(0.05, 0.05, f"#:{glacier_count}, V:{subregion_volume}",                        transform=ax_callout.transAxes, fontsize=18, verticalalignment='bottom', fontstyle='italic')        ax_callout.text(0.85, 0.8, f"{subregion_id}",                        transform=ax_callout.transAxes, fontsize=18, verticalalignment='bottom'  ,fontweight='bold')                boundary = subregion.geometry.boundary.iloc[0]        if isinstance(boundary, MultiLineString):            boundary = list(boundary.geoms)[0]        boundary_coords = list(boundary.coords)        boundary_x, boundary_y = boundary_coords[0]  # First point on the boundary        boundary_x -= movements[region_id][0]        boundary_y -= movements[region_id][1]        # Annotate or place text near the boundary        ax.text(boundary_x, boundary_y, f"{subregion_id}",                horizontalalignment='center', fontsize=18, color='black', fontweight='bold')                       ax_callout.tick_params(axis='x', labelbottom=False)        if idx % nr_cols != 0:            ax_callout.tick_params(axis='y', labelleft=False)        ax_callout.xaxis.set_tick_params(labelsize=18)        ax_callout.yaxis.set_tick_params(labelsize=18)                    callout_x, callout_y, callout_w, callout_h = pos# Create a new figure for the\small legend plotfig_legend = fig.add_axes([start_x, 0.069, w*2.1, h*2.2], ylim=(-8,4), facecolor="white")##e9e9e9")  # make twice as large as the callout plots #w*2.4, h*2.15total_initial_volume = baseline['volume'].sum(dim="rgi_id")[0].valuesvol_display="relative"fig_legend.plot(baseline["time"].values, baseline['volume'].sum(dim="rgi_id")/total_initial_volume*100-100,                label="Historical, W5E5", color= colors['irr'][0], linewidth=3, zorder=15)member_data = []# Temporarily commented to speed up the plottingfor m, model in enumerate(models_shortlist):    for j in range(members_averages[m]):        if j ==1 and m==1:            legendlabel = 'Historical NoIrr member'        else:            legendlabel = '_nolegend_'        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        # with xr.open_dataset(os.path.join(        #     sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')) as climate_run_output:        climate_run_output = global_full.sel(member=sample_id)        fig_legend.plot(climate_run_output["time"].values, climate_run_output["volume"]/total_initial_volume*100-100, label=legendlabel, color=colors['noirr'][0], linewidth=1, linestyle="dotted")            # fig_legend.plot(climate_run_output["time"].values, climate_run_output["volume"].sum(            #     dim="rgi_id")/total_initial_volume*100-100, label=legendlabel, color=colors['noirr'][0], linewidth=2, linestyle="dotted")        # member_data.append(        #     climate_run_output["volume"].sum(dim="rgi_id").values/total_initial_volume*100-100)# Mean and range plottingmean_values = global_full.sel(member='14-member-avg').volume.values/total_initial_volume*100-100 #np.mean(member_data, axis=0).flatten()min_values = global_full.volume.min(dim='member').values/total_initial_volume*100-100 #np.min(member_data, axis=0).flatten()max_values = global_full.volume.max(dim='member').values/total_initial_volume*100-100 #np.max(member_data, axis=0).flatten()std_values = ((global_full.volume / global_full.volume.isel(time=0)) * 100 - 100).std(dim="member").values# std_values = global_full.volume.std(dim='member').values #np.std(member_data, axis=0).flatten()  # Replace min with stdfig_legend.plot(climate_run_output["time"].values, mean_values,                color=colors['noirr'][0], linestyle='dashed', lw=3, label=f"Historical NoIrr avg.")fig_legend.fill_between(climate_run_output["time"].values, (mean_values-std_values), (mean_values+std_values), color=colors['noirr'][1], alpha=0.3, label=r"Historical NoIrr 1$\sigma$" )fig_legend.plot(climate_run_output["time"].values, np.zeros(len(climate_run_output["time"])), color="k", linewidth=1, linestyle="dashed")total_nr = len(master_ds_avg)total_volume = round(master_ds_avg['rgi_volume_km3'].sum())# Annotate number of glaciers (just for example)fig_legend.text(0.05, 0.05, f'#:{total_nr}, V:{total_volume} km$^3$', transform=fig_legend.transAxes,                fontsize=18, verticalalignment='bottom', horizontalalignment='left',                bbox=dict(facecolor='none', edgecolor='none', pad=2), fontstyle='italic')handles, labels = fig_legend.get_legend_handles_labels()# Define the custom order (indices correspond to handles/labels order)custom_order = [0,3,2,1]#[0,1,2]#,1]#[0,3,2,1]  # Example: Rearrange items by index# Apply the custom order# fig_legend.legend([handles[i] for i in custom_order], #           [labels[i] for i in custom_order], #           loc='lower left', bbox_to_anchor=(0.0, 0.1), ncol=1, fontsize=18,#                             frameon=False, labelspacing=0.4)# Set labels for axes# fig_legend.set_xlabel('Time', fontsize=20, labelpad=10 )#fontweight="bold"fig_legend.set_ylabel('∆ Volume [%, vs. 1985]', fontsize=18, labelpad=10) #, fontweight="bold"fig_legend.set_title('High Mountain Asia', fontsize=18, fontweight='bold')fig_legend.xaxis.set_tick_params(labelsize=18)fig_legend.yaxis.set_tick_params(labelsize=18)# Remove tick marks but keep the tick labelsfig.tight_layout()fig_folder = os.path.join(fig_path)#, "04. Map"os.makedirs(fig_folder, exist_ok=True)plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR.png", dpi=300, bbox_inches="tight", pad_inches=0.1)# plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_2000_2014.png", dpi=300, bbox_inches="tight", pad_inches=0.1)plt.show()#%% Cell 3a: map plot - prepare data per model"""Process master for map plot """# Load datasetsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]for m,model in enumerate(models_shortlist):    print(model)    master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL                   (df['sample_id'].str.startswith('IPSL'))]    # master_ds = df#df[(~df['sample_id'].str.endswith('0')) ]    master_ds = master_ds[(master_ds['sample_id'].str.startswith(f'{model}'))] #Temporary    master_ds = master_ds.reset_index(drop=True)        # Normalize values    # divide all the B values with 1000 to transform to m w.e. average over 30 yrs    # master_ds[['B_noirr', 'B_irr', 'B_delta_irr', 'B_cf',  "B_delta_cf"]] /= 1000    master_ds.loc[:, ['B_noirr', 'B_irr', 'B_delta']] /= 1000        master_ds = master_ds[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                           'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta']]        # Define custom aggregation functions for grouping over the 11 member data    aggregation_functions = {        'rgi_region': 'first',        'rgi_subregion': 'first',        'full_name': 'first',        'cenlon': 'first',        'cenlat': 'first',        'rgi_date': 'first',        'rgi_area_km2': 'first',        'rgi_volume_km3': 'first',        'B_noirr': 'mean',        'B_irr': 'mean',        'B_delta': 'mean',        'sample_id': 'first'    }            master_ds_avg = master_ds.groupby(['rgi_id'], as_index=False).agg({        'B_delta': 'mean',        'B_noirr': 'mean',        'B_irr': 'mean',        # lamda is anonmous functions, returns 11 member average        'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id']}    })        master_ds_avg.to_csv(        f"{wd_path}masters/master_lon_lat_rgi_id_{model}.csv")    # Aggregate data for scatter plot    master_ds_avg['grid_lon'] = np.floor(master_ds_avg['cenlon'])    master_ds_avg['grid_lat'] = np.floor(master_ds_avg['cenlat'])            # Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member average    aggregated_ds = master_ds_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({        'B_delta': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'B_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'B_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'rgi_area_km2': 'sum',  # Sum for area        'rgi_volume_km3': 'sum',  # Sum for volume        # 'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id', 'rgi_area_km2', 'rgi_volume_km3']}    })        aggregated_ds.rename(        columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)    print(aggregated_ds.head())        aggregated_ds.to_csv(        f"{wd_path}masters/complete_master_processed_for_map_plot_{model}.csv")    # f"{wd_path}masters/complete_master_processed_for_map_plot_E3SM.csv")#%% Cell 3b: map plot - Process ∆ volume data, per modelmembers_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]climate_run_output_irr = xr.open_dataset(os.path.join(    sum_dir, f'climate_run_output_baseline_W5E5.000.nc'))initial_volume=climate_run_output_irr.volume.sel(time=1985) #only select first year for initial valuedelta_volume_irr = climate_run_output_irr.volume - initial_volumemaster_df = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")[['rgi_id', 'cenlon','cenlat', 'rgi_area_km2']]all_datasets = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                climate_run_output_noirr = xr.open_dataset(os.path.join(            sum_dir, f'climate_run_output_perturbed_{sample_id}.nc'))        delta_volume_noirr = climate_run_output_noirr.volume - initial_volume        delta_volume_noirr_rel = xr.where(delta_volume_irr != 0, delta_volume_noirr / delta_volume_irr, 0)                        yearly_mean_volume = climate_run_output_noirr.mean(dim='time')                ds_new = xr.Dataset({            "delta_volume_irr": delta_volume_irr,            "delta_volume_noirr": delta_volume_noirr,            "sample_id": [sample_id],            "delta_volume_noirr_rel": delta_volume_noirr_rel        })        # Append to list        all_datasets.append(ds_new)            volume_change_ds = xr.concat(all_datasets, dim="sample_id")    volume_change_ds["delta_volume_noirr_rel"].attrs["description"] = "Relative volume change ∆ NoIrr (resp. Irr-1985)/ ∆ Irr (resp. Irr-1985)"    # Group by rgi_id and time, then compute the mean for numeric variables    volume_ds_avg = volume_change_ds.mean(dim="sample_id").mean(dim="time") #30-yr average values for all 14 members in the model ensemble            opath_bymember = os.path.join(wd_path,"masters", 'delta_volume_evolution_bymember.nc')    volume_change_ds.to_netcdf(opath_bymember)        opath_averaged = os.path.join(wd_path,"masters", f'delta_volume_evolution_ensemble_average_{model}.nc')    opath_averaged_csv = os.path.join(wd_path,"masters", f'delta_volume_evolution_ensemble_average_{model}.csv')    print(opath_averaged_csv)    print(model)        volume_ds_avg.to_netcdf(opath_averaged)        volume_df_avg = volume_ds_avg.to_dataframe()            volume_df_avg = volume_df_avg.merge(        master_df, on="rgi_id", how="left")        # Aggregate data for scatter plot    volume_df_avg['grid_lon'] = np.floor(volume_df_avg['cenlon'])    volume_df_avg['grid_lat'] = np.floor(volume_df_avg['cenlat'])        # Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member average    volume_df_avg = volume_df_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({        'delta_volume_irr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'delta_volume_noirr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'delta_volume_noirr_rel': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'rgi_area_km2': 'sum',  # Sum for area        # 'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in volume_df_avg.columns if col not in ['delta_volume_irr', 'delta_volume_noirr', 'delta_volume_noirr_rel', 'rgi_area_km2']}    })        volume_df_avg.rename(        columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)        volume_df_avg.to_csv(opath_averaged_csv)#%% Cell 3c: Create map plot per model subfigures - appendixmembers_averages = [2, 3,    5 ,1,3]models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]fig,axes = plt.subplots(2,3,figsize=(12,7),subplot_kw={                       'projection': ccrs.PlateCarree()})axes=axes.flatten()for m,model in enumerate(models_shortlist):    ax = axes[m]        """ Load data"""    aggregated_ds = pd.read_csv(        f"{wd_path}masters/complete_master_processed_for_map_plot_{model}.csv")        aggregated_ds_vol = pd.read_csv(        f"{wd_path}masters/delta_volume_evolution_ensemble_average_{model}.csv")        gdf = gpd.GeoDataFrame(aggregated_ds, geometry=gpd.points_from_xy(        aggregated_ds['lon'] +0.5 , aggregated_ds['lat'] + 0.5),        crs="EPSG:4326")        gdf_volume = gpd.GeoDataFrame(aggregated_ds_vol, geometry=gpd.points_from_xy(        aggregated_ds_vol['lon'] +0.5 , aggregated_ds_vol['lat'] + 0.5),        crs="EPSG:4326")        """ Plot shaded relief """    # fig, ax = plt.subplots(figsize=(15,12), subplot_kw={    #                        'projection': ccrs.PlateCarree()})    dem_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/02. DEM/ETOPO2v2c_f4.nc"    with xr.open_dataset(dem_path) as ds:        dem=ds['z']        # Print dataset info to find variable names        dem = dem.where((dem.x >= 65.5) & (dem.x <= 108) & (dem.y >= 23) & (dem.y <= 48), drop=True)    # dem = dem.where((dem.x >= 54.5) & (dem.x <= 115) & (dem.y >= 16) & (dem.y <= 55), drop=True)    dx =(dem['x'][1]-dem['x'][0]).values    dy =(dem['y'][1]-dem['y'][0]).values    z = dem.values    x, y = np.meshgrid(dem.x, dem.y)        # Create a LightSource object for hillshading    ls = LightSource(azdeg=315, altdeg=45)            # Overlay the hillshade (for relief effect)    im = ax.imshow(ls.hillshade(z, vert_exag=10, dx=0.01, dy=0.01),extent=[dem.x.min(), dem.x.max(), dem.y.min(), dem.y.max()],              transform=ccrs.PlateCarree(), alpha=0.1, cmap="grey_r")  # Adjust alpha for effect        # Add geographical features    # ax.add_feature(cfeature.COASTLINE, linewidth=0.7)    # ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle="--")    # ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='none')    ax.add_feature(cfeature.RIVERS, edgecolor='blue', facecolor='none', linewidth=0.6, alpha=0.6)        """Set up plot incl shapefile"""            subregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"    subregions = gpd.read_file(subregions_path).to_crs('EPSG:4326')    ax.spines['geo'].set_visible(False)        # Optionally, remove gridlines    # Remove gridlines properly    gl = ax.gridlines(draw_labels=False)  # Create gridlines without labels    gl.xlines = False  # Remove longitude lines    gl.ylines = False  # Remove latitude lines            """ Plot subregions"""    # define movements for the annotation of subregions    movements = {        '13-01': [0, -0.2], #A        '13-02': [4.8, 2], #        '13-03': [-0.2, -0.6], #B        '13-04': [-0.6, 0], #C        '13-05': [-4.6,2.5],#-0.2, 0.8], #E        '13-06': [-10, 0], #F        '13-07': [-1, 1], #G        '13-08': [-2, -0.5], #J        '13-09': [1.5, 0], #O        '14-01': [0.5, -0.8], #H        '14-02': [2,-1.8],#[1, -0.4], #I        '14-03': [3.4, -3.5], #K        '15-01': [0, 0], #L        '15-02': [-2, -1], #M        '15-03': [3.5, 4.5], #N    }    # annotate subregions    # i=0           # Create a ListedColormap with some colors (example)    """ Include the Mass Balance plot including color bar"""        h_leg = 0.65    w_leg = 0.646            scatter_data = "B"        # Define vmin and vmax for the color scale (asymmetric)    vmin, vmax = -0.2, 0.7  # Asymmetric range    start =(abs(vmin)/(vmax-vmin))    #Trim cmap so that white is at 0    n_colors=256    half=n_colors//2    original_cmap = plt.cm.bwr.reversed()  # Blue-White-Red colormap        # colors_trimmed = [    #     (0.0, "#fdc1c5"),   # lighter red at -0.2    #     (0.222, "white"),   # white at 0.0 → (0 - (-0.2)) / (0.7 - (-0.2)) = ~0.222    #     (1.0, "#0203e2")    # dark blue at 0.7    # ]    colors_trimmed = [        (0.0,  "#fdc1c5"),   # Light red at far negative        (0.222, "white"),    # White at 0        (0.5, "#6a79f7"),   # Deep blue at moderate positive #0203e2 0.65 limit before        (1.0,  "#110954")    # Purple at strong positive        ]        # Create custom colormap    custom_cmap = LinearSegmentedColormap.from_list("custom_trimmed_rwb", colors_trimmed, N=256)        # Normalize to match data range    norm = Normalize(vmin=-0.2, vmax=0.7)                gdf["marker_size"]=(np.sqrt(gdf['rgi_area_km2'])*5)**1.3/3    scatter = ax.scatter(gdf.geometry.x, gdf.geometry.y,                         s=gdf['marker_size'], c=gdf['B_delta'], cmap=custom_cmap, norm=norm, edgecolor='k', alpha=0.8)    #plot subregion lines                for attribute, subregion in subregions.groupby('o2region'):           # Uncomment for coloring of specific subregions            linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"        # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,        subregion.plot(ax=ax, edgecolor='black', linewidth=1.2,                      facecolor='none') # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion        # i+=1    ax.set_title(model, fontsize=18) """Create the colorbar"""cax = fig.add_axes([0.7, 0.13, 0.05, 0.17])  # [left, bottom, width, height]# cax = fig.add_axes([0.76, h_leg-0.015, 0.03, 0.06])  # [left, bottom, width, height]cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm,cmap=custom_cmap ),                    cax=cax, orientation="vertical")# Define **evenly spaced** tick positionsnum_ticks = 2  # Adjust as neededtick_values = np.linspace(vmin, vmax, num_ticks)  # Ensures equal spacing# Apply ticks and labelscbar.set_ticks(tick_values)cbar.set_ticklabels([f'{b:.1f}' for b in tick_values])# Define the boundaries for each color blockcbar.ax.tick_params(labelsize=18) # Set the colorbar labelcbar.set_label('∆B$_{Irr}$ \nm [yr$^{-1}]$', fontsize=18, ha='left', rotation=0, labelpad=0, linespacing=1)cbar.ax.yaxis.label.set_position((1.1, 1.2))  # (X, Y) - Adjust Y to move up yprev = 1.4cbar.ax.yaxis.set_label_position('left')       """ Add volume legend"""custom_sizes = [200, 2000]#500, 1000, 2000, 3000]  # Example sizes for the legend (area)size_labels = [f"{size:.0f}" for size in custom_sizes]  # Create labels for sizes# Create legend handles (scatter points with different sizes)legend_handles = [    axes[5].scatter([], [], s=(np.sqrt(size)*5)**1.3/3, edgecolor='k', facecolor='none')    for size in custom_sizes]  # Adjust size factor if neededtext_handles = [Line2D([0], [0], linestyle="none", label=label) for label in size_labels]# Create a separate axis for the legendcax_legend = axes[5] #fig.add_axes([w_leg -0.015, h_leg, 0.15, 0.1])  # [left, bottom, width, height]# Remove axis visualscax_legend.set_frame_on(False)  # Hide framecax_legend.set_xticks([])  # Remove x-tickscax_legend.set_yticks([])  # Remove y-ticks# Add legend to the separate axislegend = cax_legend.legend(    legend_handles, size_labels, loc="center",    fontsize=18, ncol=1,frameon=False, title="Total Area \n [km$^2]$", title_fontsize=18, scatterpoints=1, labelspacing=1.5, bbox_to_anchor=(0.7,0.59))# Remove tick marks but keep the tick labelsfig.tight_layout()fig_folder = os.path.join(fig_path)#, "04. Map"os.makedirs(fig_folder, exist_ok=True)plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_permodel.png", dpi=300, bbox_inches="tight", pad_inches=0.1)# plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_2000_2014.png", dpi=300, bbox_inches="tight", pad_inches=0.1)plt.show()# %% Cell 4a: Create nan mask for ids without succesful comitted runmembers = [3, 4, 6, 4, 1, 1]models = ["E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]#, "IPSL-CM6"]overview_df = pd.DataFrame()for m, model in enumerate(models):    for member in range(members[m]):        df_tot = pd.DataFrame()        sample_id = f"{model}.00{member}"        for f, filepath in enumerate([f"climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_comitted_random.nc"]):            calendar_year = 2014            if model != "W5E5":                filepath = [f'climate_run_output_perturbed_{sample_id}.nc',                            f'climate_run_output_perturbed_{sample_id}_comitted_random.nc'][f]                # f'climate_run_output_perturbed_{sample_id}_comitted_cst.nc'][f]            ds = xr.open_dataset(os.path.join(                # log_dir, f"stats_perturbed_{sample_id}_climate_run_test.csv"))                sum_dir, filepath))            ds_zeros = ds.volume.sel(time=2014).to_dataframe()            df_individual = ds_zeros[["calendar_year", "volume"]].reset_index()            # df_tot = pd.concat([df_tot, df_individual], ignore_index=True)            if df_tot.empty:                df_tot = df_individual            if f == 1:                # Merge based on rgi_id                df_tot = pd.merge(                    df_tot,                    df_individual[["rgi_id", "volume", "calendar_year"]],                    on="rgi_id",                    how="outer",                    suffixes=("", "_noirr")                )            # if f == 2:            #     # Merge based on rgi_id            #     df_tot = pd.merge(            #         df_tot,            #         df_individual[["rgi_id", "volume", "calendar_year"]],            #         on="rgi_id",            #         how="outer",            #         suffixes=("", "_noforcing")            #     )        df_zero_volume = df_tot[            pd.isna(df_tot["volume_noirr"])|  pd.isna(df_tot["volume"])] #| pd.isna(df_tot["volume_noforcing"]) |    overview_df = pd.concat([overview_df, df_zero_volume], ignore_index=True)unique_rgi_ids = overview_df['rgi_id'].unique()# print(len(unique_rgi_ids))unique_rgi_ids = pd.DataFrame(unique_rgi_ids, columns=['rgi_ids'])unique_rgi_ids.to_csv(os.path.join(    wd_path, 'masters', 'nan_mask_comitted_random.csv'))    # wd_path, 'masters', 'nan_mask_comitted_random_noIPSL.csv'))# Save the result to a CSVoutput_path = os.path.join(    wd_path, "masters", "nan_mask_all_models_volume_comitted_random.csv")# wd_path, "masters", "nan_mask_all_models_volume_comitted_random_noIPSL.csv")unique_rgi_ids.to_csv(output_path, index=False)#%% Cell 4b: Create comitted mass loss plot transient with panelsregions = [13, 14, 15]subregions = [9, 3, 3]fig, ax = plt.subplots(figsize=(15,10))  # create a new figuremembers_averages = [2, 3,  3,  6, 1]models_shortlist = ["E3SM", "CESM2",  "NorESM",  "CNRM", "IPSL-CM6"]# define the variables for p;lottingfactors = [10**-9]variable_names = ["Volume"]variable_axes = ["Volume compared to 1985 All Forcings scenario [%]"]use_multiprocessing = Falsergi_ids_test=[]subset_gdirs = gdirs_3r_a1for gdir in subset_gdirs:     rgi_ids_test.append(gdir.rgi_id)      #Exclude error idsnan_mask = pd.read_csv(os.path.join(    wd_path, "masters", "nan_mask_all_models_volume_comitted_random.csv")).rgi_ids# Remove duplicates if needednan_mask = set(pd.DataFrame({'rgi_id': nan_mask.unique()}).rgi_id.to_numpy())rgi_ids_test = [rgi_id for rgi_id in rgi_ids_test if rgi_id not in nan_mask]   aggregated_ds_total = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv") #load dataset - needed to filter rgi_ids per region    aggregated_ds = aggregated_ds_total[aggregated_ds_total['rgi_id'].isin(rgi_ids_test)]plt.rcParams.update({'font.size': 14})#create a figure consisting of 1 large plot and several smaller ones for every subregionfig = plt.figure(figsize=(15,8))gs = gridspec.GridSpec(4, 6, figure=fig)  # Adjust the grid size as neededax_big = fig.add_subplot(gs[:3, :3])  # Spanning first 2 rows and 3 columnssmall_axes = []positions = [    (0, 3), (0, 4), (0, 5), (1, 3), (1, 4), (1, 5),  # First row right side    (2, 3),(2, 4),(2, 5),  # Third row    (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5)  # Fourth row]axes_dict = {}  # Dictionary to store axes for later useresp_values_dict={}linestyles = ['solid', 'dashed']for f, filepath in enumerate(["climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_comitted_random.nc"]):    if f != 0:        run_type = "noirr"        legend_id = "committed"        bar_values = 50        run_label = "NoIrr"    else:        run_type = "irr"        legend_id = ""        bar_values = 20    # load and plot the baseline data    baseline_path = os.path.join(        wd_path, "summary", filepath)    baseline_all = xr.open_dataset(baseline_path)    print(len(baseline_all.rgi_id))    baseline_all = baseline_all.where(        baseline_all.rgi_id.isin(rgi_ids_test), drop=True)        #define the 1985 value for relative volume calculation    if f == 0:        resp_values_dict['Total'] = (baseline_all['volume'].sum(dim="rgi_id")[0].values * factors)[0]                legendtext=f"W5E5.000"    else:        legendtext="W5E5.000, comitted run"    ax_big.plot(baseline_all["time"], (baseline_all["volume"].sum(dim="rgi_id") * factors)/resp_values_dict['Total']*100,            label=legendtext, color=colors[f"irr"][0], linewidth=2, zorder=15, linestyle=linestyles[f])    nan_runs_noirr = []        run_type = "noirr"    legend_id = "" #comitted    bar_values = 50    run_label = "NoIrr"    member_data_noirr_all = []       p=0 #set to zero for plot indices    for reg, region in enumerate(regions):        for sub in range(subregions[reg]):            member_data_noirr_region = []            region_id = f"{region}.0{sub+1}"            print(region_id)            pos = positions[p]            if f==0:                 ax = fig.add_subplot(gs[pos])                ax.set_ylim(0,150)                ax.set_title(region_id)                axes_dict[region_id] = ax                # if p not in {0,3,6,9}:                ax.set_yticks([])                # if p <9:                ax.set_xticks([])            p+=1 #loop trhough all the different subplots                        subregion_ds = aggregated_ds[aggregated_ds['rgi_subregion'].str.contains(                f"{region}.0{sub+1}")]            print(f"{region}.0{sub+1}")            subregion_mask = set(pd.DataFrame({'rgi_id': subregion_ds.rgi_id.unique()}).rgi_id.to_numpy())            # rgi_ids_region = [] #create filter for data not in that region            # for gdir in subset_gdirs:            #      rgi_ids_region.append(gdir.rgi_id)            rgi_ids_region = [rgi_id for rgi_id in rgi_ids_test if rgi_id in subregion_mask]                        #plot baseline data per region            baseline = baseline_all.where(                baseline_all.rgi_id.isin(rgi_ids_region), drop=True) #filter baseline to region            if f == 0:                resp_values_dict[region_id] = baseline['volume'].sum(dim="rgi_id")[0].values * factors                        axes_dict[region_id].plot(baseline["time"], (baseline['volume'].sum(dim="rgi_id") * factors)/resp_values_dict[region_id]*100,                    label=legendtext, color=colors[f"irr"][0], linewidth=2, zorder=15, linestyle=linestyles[f])            #define files for climate runs for comitted and normal run                         for m, model in enumerate(models_shortlist):                for i in range(members_averages[m]):                    sample_id = f"{model}.00{i}"                    filepath = [f'climate_run_output_perturbed_{sample_id}.nc',                                f'climate_run_output_perturbed_{sample_id}_comitted_random.nc',                                ][f]                    climate_run_opath_noirr = os.path.join(                        sum_dir, filepath)  # f'climate_run_output_perturbed_{sample_id}_comitted.nc')                    climate_run_output_noirr_all = xr.open_dataset(                        climate_run_opath_noirr)                    climate_run_output_noirr_selected = climate_run_output_noirr_all.where(                        climate_run_output_noirr_all.rgi_id.isin(rgi_ids_test), drop=True)                    if reg==0: #include the 3-region total climate run data for every model                        member_data_noirr_all.append((climate_run_output_noirr_selected["volume"].sum(                            dim="rgi_id").values)*factors/resp_values_dict['Total']*100)                      climate_run_output_noirr = climate_run_output_noirr_selected.where(                         climate_run_output_noirr_selected.rgi_id.isin(rgi_ids_region), drop=True)                       # add all the summed volumes/areas to the member list, so a multi-member average can be calculated                    member_data_noirr_region.append((climate_run_output_noirr["volume"].sum(                        dim="rgi_id").values)*factors/resp_values_dict[region_id]*100)                                          if members_averages[m] > 1:                        i += 1                        label = None                    else:                        label = "GCM member"                                # stack the member data            stacked_member_data_region = np.stack(member_data_noirr_region)            mean_values_noirr_region = np.median(                stacked_member_data_region, axis=0).flatten()                        # mean_values_noirr = np.mean(stacked_member_data, axis=0).flatten()            if f==0:                labeltext = None#f"{run_label} (14-member avg) {legend_id}"                # labeltext = f"{run_label} ({sum(members_averages)}-member avg) {legend_id}"            else:                labeltext = None                            axes_dict[region_id].plot(climate_run_output_noirr["time"].values, mean_values_noirr_region,                    color=colors[f"{run_type}"][0], linestyle=linestyles[f], lw=2, label=labeltext, zorder=5)            # calculate and plot volume/area 10-member min and max for ribbon            min_values_noirr = np.min(stacked_member_data_region, axis=0).flatten()            max_values_noirr = np.max(stacked_member_data_region, axis=0).flatten()            if f==0:                labeltext = "NoIrr 14-member avg" #f"{run_label} (14-member range) {legend_id}"                labeltext_range="NoIrr 14-member range"                # labeltext = f"{run_label} ({sum(members_averages)}-member range) {legend_id}"            else:                labeltext=label="NoIrr 14-member avg, comitted run"                labeltext_range=None            axes_dict[region_id].fill_between(climate_run_output_noirr["time"].values, min_values_noirr, max_values_noirr,                            color=colors[f"{run_type}"][f], alpha=0.3, label=None, zorder=16)            axes_dict[region_id].axhline(100, color='black', linestyle='--',                        linewidth=1, zorder=1)  # Dashed line at 0                       try:                subregion_name = subregion_ds.full_name.iloc[0]            except:                subregion_name="no ids in subset"            if region_id=="13.02":                subregion_title ="Pamir"            elif region_id=="13.04":                subregion_title ="East Tien Shan"            elif region_id=="13.06":                subregion_title ="East Kun Lun"            else:                subregion_title=subregion_name            print(region_id, subregion_title)                        axes_dict[region_id].set_title(subregion_title, fontweight="bold", bbox=dict(                facecolor='white', edgecolor='none', pad=1), fontsize=14)    stacked_member_data_all = np.stack(member_data_noirr_all)    mean_values_noirr_all = np.median(        stacked_member_data_all, axis=0).flatten()    ax_big.plot(climate_run_output_noirr_all["time"].values, mean_values_noirr_all,            color=colors[f"{run_type}"][0], linestyle=linestyles[f], lw=2, label=labeltext, zorder=5)    # calculate and plot volume/area 10-member min and max for ribbon    min_values_noirr_all = np.min(stacked_member_data_all, axis=0).flatten()    max_values_noirr_all = np.max(stacked_member_data_all, axis=0).flatten()    ax_big.fill_between(climate_run_output_noirr["time"].values, min_values_noirr_all, max_values_noirr_all,                    color=colors[f"{run_type}"][f], alpha=0.3, label=labeltext_range, zorder=16)    ax_big.axhline(100, color='black', linestyle='--',                linewidth=1, zorder=1)  # Dashed line at 0    # Set labels and title for the combined plot    ax_big.set_ylabel("∆Volume compared to 1985 All Forcings [%]", fontweight='bold')    ax_big.set_xlabel("Time [year]")    ax_big.set_title(f"All Regions ", fontweight="bold", bbox=dict(        facecolor='white', edgecolor='none', pad=1), fontsize=14)    ax_big.set_ylim(0,150)    # Adjust the legend    handles, labels = ax_big.get_legend_handles_labels()    ax_big.legend(handles, labels,               ncol=2)        plt.tight_layout()plt.subplots_adjust(hspace=0.4, wspace=0.2)plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0)# specify and create a folder for saving the data (if it doesn't exists already) and save the ploto_folder_data = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/02. OGGM simulations/01. Modelled output/3r_a1/1. Volume/00. Combined"os.makedirs(o_folder_data, exist_ok=True)o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.comitted_random_subplots.png"# o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.{variable_names[v]}.comitted_cst_test.png"plt.savefig(o_file_name, bbox_inches='tight')            plt.show()         #%% Cell 4c: Comitted mass loss boxplots# Function to plot the boxplotsdef plot_boxplot(data, position, label, color, alpha, is_noirr, ax):    bp = ax.boxplot(data, patch_artist=True, labels=[""],#label if is_noirr else [""],  # only set labels for Noirr                    vert=True, widths=0.3,                    boxprops=dict(facecolor=color, alpha=alpha,                                  edgecolor='none'),                    medianprops=dict(color='black', linewidth=2),                    positions=[position], showfliers=False, zorder=2)        # Get the median value from the boxplot    ax.axhline(y=0, color='black', linestyle='--', linewidth=1, zorder=0)    ax.tick_params(labelsize=10)    if p>0:        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)        fontweight="medium"    else:         ax.set_ylabel('Volume change (%, vs. 1985 historic)', labelpad=15, fontsize=12)        ax.tick_params(left=True, bottom=False, labelleft=True, labelbottom=False, labelsize=12)        for spine in ax.spines.values():            spine.set_linewidth(2)        fontweight="bold"    if is_noirr:        ax.set_xlabel(label[0], rotation=30, fontweight=fontweight, fontsize=12)    median_value = bp['medians'][0].get_xdata()[0]    ax.set_ylim(-100, 140)    # ax.text(median_value + 10, position, f'{median_value:.1f}',  # Place above for Irr    #         va='center', ha='center', fontsize=10, color='black', #fontweight='bold',    #         # bbox=dict(boxstyle='round,pad=0.001',    #         #           facecolor='white', edgecolor='none'),    #         zorder=3)    return bpregions = [13, 14, 15]subregions = [9, 3, 3]df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]master_ds['V_2264_noirr'] = (master_ds['V_2264_noirr']-master_ds['V_1985_irr'])/master_ds['V_1985_irr']*100master_ds['V_2264_irr'] = (master_ds['V_2264_irr']-master_ds['V_1985_irr'])/master_ds['V_1985_irr']*100master_ds['V_2014_noirr'] = (master_ds['V_2014_noirr']-master_ds['V_1985_irr'])/master_ds['V_1985_irr']*100master_ds['V_2014_irr'] = (master_ds['V_2014_irr']-master_ds['V_1985_irr'])/master_ds['V_1985_irr']*100master_ds = master_ds[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr']]master_ds_avg = master_ds.groupby(['rgi_id'], as_index=False).agg({  # calculate the 11 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'mean',    'V_2264_irr': 'mean',    'V_2014_noirr': 'mean',    'V_2014_irr': 'mean',    # lamda is anonmous functions, returns 11 member average    'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr', 'sample_id']}})# Aggregate dataset, area-weighted BDelta Birr and Bnoirr,master_ds_area_weighted = master_ds_avg.groupby(['rgi_subregion'], as_index=False).agg({    'V_2264_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'V_2264_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'V_2014_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'V_2014_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'rgi_area_km2': 'sum',  # Sum for area    'rgi_volume_km3': 'sum',  # Sum for volume    'V_1985_irr': 'sum',  # Sum for volume    # 'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_irr', 'V_2264_noirr','V_1985_irr','rgi_area_km2', 'rgi_volume_km3']}})master_ds_avg=master_ds_avg[master_ds_avg['V_2264_irr'].notna()]master_ds_area_weighted=master_ds_area_weighted[master_ds_area_weighted['V_2264_irr'].notna()]use_weights = Truev_space_noi2 = 1.9  # Vertical space between irr and noirr boxplotsv_space_irr2 = 1.5  # Vertical space between irr and noirr boxplotsv_space_noi1 = 0.5  # Vertical space between irr and noirr boxplotsv_space_irr1 = 0.1  # Vertical space between irr and noirr boxplots# Storage for combined dataall_noirr, all_irr = [], []position_counter = 1cumulative_index = 14# Initialize plotfig = plt.figure(figsize=(16, 6))gs = gridspec.GridSpec(1, 16, width_ratios=[1.5] + [1]*15)  # First axis twice as wideaxes = [fig.add_subplot(gs[i]) for i in range(16)]# fig, axes = plt.subplots(1,16, figsize=(15, 6), sharey=True)print(axes)# axes = axes.flatten()p=1# Example usage: Main loop through regions and subregionsfor r, region in enumerate((regions)):    for sub in range(list((subregions))[r]):        # Filter subregion-specific data        ax = axes[p]        print(f"{region}.0{sub+1}")               subregion_ds = master_ds_avg[master_ds_avg['rgi_subregion'].str.contains(            f"{region}.0{sub+1}")]        # Calculate mean values for Noirr and Irr        noirr_mean = master_ds_area_weighted['V_2264_noirr'][cumulative_index]        irr_mean = master_ds_area_weighted['V_2264_irr'][cumulative_index]        # Set color and label based on the region        try:            label = [subregion_ds.full_name.iloc[0]]        except:            label = [f"{region}-0{sub+1}"]                if region==13:            if sub ==1:                label = ["Pamir"]            elif sub ==3:                label = ["East Tien Shan"]            elif sub ==5:                label = ["East Kun Lun"]        # Plot Noirr and Irr boxplots                box_irr = plot_boxplot(            subregion_ds['V_2014_irr'], position_counter + v_space_irr1, label="", color=colors['irr'][0], alpha=1, is_noirr=False, ax)        box_irr = plot_boxplot(            subregion_ds['V_2264_irr'], position_counter + v_space_irr2, label="", color=colors['irr'][1], alpha=1, is_noirr=False, ax)        box_noirr = plot_boxplot(            subregion_ds['V_2014_noirr'], position_counter + v_space_noi1, label, color=colors['noirr'][0], alpha=1, is_noirr=True, ax)        box_noirr = plot_boxplot(            subregion_ds['V_2264_noirr'], position_counter + v_space_noi2, label, color=colors['noirr'][1], alpha=1, is_noirr=True, ax)                # Annotate the number of glaciers and delta between the two columns        num_glaciers = len(subregion_ds)        # delta = noirr_mean - irr_mean        # Display number of glaciers and delta        initial_volume = round(sum(subregion_ds['V_1985_irr'].values)*1e-9)        ax.text(1.1, 120,                  f'{initial_volume}\n({num_glaciers})',  # \nΔ = {delta:.2f}',                 va='center', ha='left', fontsize=12, color='black',                 backgroundcolor="white",  zorder=10)        p+=1            overall_area_weighted_mean = {    'V_2264_irr': (master_ds_avg['V_2264_irr'] * master_ds_avg['rgi_area_km2']).sum() / master_ds_avg['rgi_area_km2'].sum(),    'V_2264_noirr': (master_ds_avg['V_2264_noirr'] * master_ds_avg['rgi_area_km2']).sum() / master_ds_avg['rgi_area_km2'].sum(),    'V_2014_irr': (master_ds_avg['V_2014_irr'] * master_ds_avg['rgi_area_km2']).sum() / master_ds_avg['rgi_area_km2'].sum(),    'V_2014_noirr': (master_ds_avg['V_2014_noirr'] * master_ds_avg['rgi_area_km2']).sum() / master_ds_avg['rgi_area_km2'].sum(),    'total_area_km2': master_ds_avg['rgi_area_km2'].sum(),  # Total area sum    'V_1985_irr': master_ds_avg['V_1985_irr'].sum(),  # Total area sum    # Total volume sum    'total_volume_km3': master_ds_avg['rgi_volume_km3'].sum()}# # Plot overall average boxplots for Irr and Noirrp=0ax = axes[p]avg_irr = plot_boxplot(master_ds_avg['V_2014_irr'],  position_counter +                       v_space_irr1, "", color=colors['irr'][0], alpha=1, is_noirr=False, ax)avg_irr = plot_boxplot(master_ds_avg['V_2264_irr'],  position_counter +                       v_space_irr2, "", color=colors['irr'][1], alpha=1, is_noirr=False, ax)avg_noi = plot_boxplot(master_ds_avg['V_2014_noirr'], position_counter + v_space_noi1, [    "High Mountain Asia"], color=colors['noirr'][0], alpha=1, is_noirr=True, ax)avg_noi = plot_boxplot(master_ds_avg['V_2264_noirr'], position_counter + v_space_noi2, [    "High Mountain Asia"], color=colors['noirr'][1], alpha=1, is_noirr=True, ax)initial_total_volume = round(overall_area_weighted_mean['V_1985_irr']*1e-9)# Annotate the number of glaciers for the overall averageax.text(1.1, 120,          f"V:{initial_total_volume}\n(#:{len(master_ds_avg)})",# Display total number of glaciers         va='center', ha='left', fontsize=12, color='black',          backgroundcolor="white", zorder=10)#fontstyle='italic',# position_counter += 4# Create custom legend elements for mean (dot) and median (stripe)mean_dot = Line2D([0], [0], marker='o', color='w',                  label='Area-weighted Mean (dot)', markerfacecolor='black', markersize=10)# median_stripe = Line2D([0], [0], color='black', lw=2, label='Median (stripe)')# Add a legend for regions, mean (dot), and median (stripe)region_legend_patches = [mpatches.Patch(color=colors['irr'][0], label='Historical (W5E5)'),                         mpatches.Patch(color=colors['noirr'][0], label='Historical NoIrr'),                         mpatches.Patch(color=colors['irr'][1], label='Comitted (historical)'),                         mpatches.Patch(color=colors['noirr'][1], label='Comitted (Historical NoIrr)'),                         ]# Append the custom legend items for the mean dot and median stripe# region_legend_patches += [median_stripe]# Create the legend with the updated patches# fig.legend(handles=region_legend_patches, loc='center right',#            bbox_to_anchor=(1.15, 0.6), ncols=1)fig.legend(handles=region_legend_patches, loc='upper right',          bbox_to_anchor=(1, 1.05), ncols=5, fontsize=12)# fig.subplots_adjust(right=0.97)# Extend the ylims for more padding# y_min, y_max = fig.get_ylim()  # Get current y-axis limits# padding = 1  # Adjust this value as needed# ax.set_ylim(y_min - padding, y_max + padding)# Adjust layout and display the plotplt.tight_layout()# fig_folder = os.path.join(fig_path, "03. Mass Balance", "Boxplot")# os.makedirs(fig_folder, exist_ok=True)# plt.savefig(#     f"{fig_folder}/Gaussian_distribution_total_region_by_region_median.png")plt.show()                   #%% Cell 4d: Comitted mass loss boxplots totals# Function to plot the boxplotsylim=50y = ylim-10x=1.3def plot_boxplot(data, position, label, color, alpha, is_noirr):    bp = ax.boxplot(data, patch_artist=True, labels=[""],#label if is_noirr else [""],  # only set labels for Noirr                    vert=True, widths=0.7,                    boxprops=dict(facecolor=color, alpha=alpha,                                  edgecolor='none'),                    medianprops=dict(color='black', linewidth=2),                    positions=[position], showfliers=False, zorder=2)        # Get the median value from the boxplot    ax.axhline(y=0, color='black', linestyle='--', linewidth=1, zorder=0)    ax.tick_params(labelsize=10)    if p>0:        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)        fontweight="medium"    else:         ax.set_ylabel('Volume change (%, vs. 1985 historic)', labelpad=15, fontsize=12)        ax.tick_params(left=True, bottom=False, labelleft=True, labelbottom=False, labelsize=12)        for spine in ax.spines.values():            spine.set_linewidth(2)        fontweight="bold"    if is_noirr and label == ["High Mountain Asia"]:        ax.set_xlabel("High \n Mountain \n Asia", rotation=0, fontweight=fontweight, fontsize=12, labelpad = 10 )    elif is_noirr:        ax.set_xlabel(label[0], rotation=30, fontweight=fontweight, fontsize=12)    median_value = bp['medians'][0].get_xdata()[0]    ax.set_ylim(-80, ylim)    # ax.text(median_value + 10, position, f'{median_value:.1f}',  # Place above for Irr    #         va='center', ha='center', fontsize=10, color='black', #fontweight='bold',    #         # bbox=dict(boxstyle='round,pad=0.001',    #         #           facecolor='white', edgecolor='none'),    #         zorder=3)    return bpregions = [13, 14, 15]subregions = [9, 3, 3]df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]master_ds_tot_vol = master_ds.groupby(['rgi_subregion', 'sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    'rgi_id': lambda _: "regional total",    # lamda is anonmous functions, returns 11 member average    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr', 'sample_id', 'rgi_id']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_tot_vol['V_2264_noirr_delta'] = ((master_ds_tot_vol['V_2264_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr'])*100master_ds_tot_vol['V_2264_irr_delta'] = (master_ds_tot_vol['V_2264_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_noirr_delta'] = (master_ds_tot_vol['V_2014_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_irr_delta'] = (master_ds_tot_vol['V_2014_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol = master_ds_tot_vol[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]master_ds = master_ds[master_ds['V_2264_irr'].notna()]master_ds_tot_vol=master_ds_tot_vol[master_ds_tot_vol['V_2264_irr'].notna()]# master_ds_area_weighted=master_ds_area_weighted[master_ds_area_weighted['V_2264_irr'].notna()]use_weights = True# v_space_noi2 = 1.9  # Vertical space between irr and noirr boxplotsv_space_com = 1.4  # Vertical space between irr and noirr boxplotsv_space_hist = 0.6  # Vertical space between irr and noirr boxplots# v_space_irr1 = 0.1  # Vertical space between irr and noirr boxplots# Storage for combined dataall_noirr, all_irr = [], []position_counter = 1cumulative_index = 14# Initialize plotfig = plt.figure(figsize=(16, 6))gs = gridspec.GridSpec(1, 16, width_ratios=[1.5] + [1]*15)  # First axis twice as wideaxes = [fig.add_subplot(gs[i]) for i in range(16)]# fig, axes = plt.subplots(1,16, figsize=(15, 6), sharey=True)print(axes)# axes = axes.flatten()p=1# Example usage: Main loop through regions and subregionsfor r, region in enumerate((regions)):    for sub in range(list((subregions))[r]):        # Filter subregion-specific data        ax = axes[p]        print(f"{region}.0{sub+1}")               subregion_ds = master_ds_tot_vol[master_ds_tot_vol['rgi_subregion'].str.contains(            f"{region}-0{sub+1}")]        # Calculate mean values for Noirr and Irr        # noirr_mean = master_ds_area_weighted['V_2264_noirr'][cumulative_index]        # irr_mean = master_ds_area_weighted['V_2264_irr'][cumulative_index]        # Set color and label based on the region        try:            label = [subregion_ds.full_name.iloc[0]]        except:            label = [f"{region}-0{sub+1}"]                if region==13:            if sub ==1:                label = ["Pamir"]            elif sub ==3:                label = ["East Tien Shan"]            elif sub ==5:                label = ["East Kun Lun"]        # Plot Noirr and Irr boxplots        print(position_counter)        print(subregion_ds['V_2014_irr_delta'].iloc[0])        print(subregion_ds['V_2264_irr_delta'].iloc[0])        # box_irr = plot_boxplot(        #     subregion_ds['V_2014_irr_delta'], position_counter + v_space_hist, label="", color=colors['irr'][0], alpha=1, is_noirr=False)        # box_irr = plot_boxplot(        #     subregion_ds['V_2264_irr_delta'], position_counter + v_space_com, label="", color=colors['irr'][1], alpha=1, is_noirr=False)                        mean_hist = subregion_ds['V_2014_noirr_delta'].mean()        mean_com = subregion_ds['V_2264_noirr_delta'].mean()         box_noirr = plot_boxplot(            subregion_ds['V_2014_noirr_delta'], position_counter + v_space_hist, label, color=colors['noirr'][0], alpha=1, is_noirr=True)        box_noirr = plot_boxplot(             subregion_ds['V_2264_noirr_delta'], position_counter + v_space_com, label, color=colors['noirr'][1], alpha=1, is_noirr=True)                box_irr = ax.scatter([position_counter +                               v_space_hist], subregion_ds['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="x", s=100, linewidths=3, zorder=10)        box_irr = ax.scatter([position_counter +                               v_space_com], subregion_ds['V_2264_irr_delta'].iloc[1], color=colors['irr'][1], marker="x", s=100, linewidths=3, zorder=10)        ax.hlines(mean_hist,           xmin=position_counter + v_space_hist - 0.3,  # adjust width          xmax=position_counter + v_space_hist + 0.3,          linestyles='dashed',           color='black',          linewidth=2,          zorder=9)                ax.hlines(mean_com,           xmin=position_counter + v_space_com - 0.3,  # adjust width          xmax=position_counter + v_space_com + 0.3,          linestyles='dashed',           color='black',          linewidth=2,          zorder=9)                # Annotate the number of glaciers and delta between the two columns        num_glaciers = len(master_ds[master_ds.rgi_subregion==f"{region}-0{sub+1}"][master_ds.sample_id=="IPSL-CM6.000"])                # delta = noirr_mean - irr_mean        # Display number of glaciers and delta        initial_volume = round(subregion_ds['V_1985_irr'].iloc[0]*1e-9)        ax.text(x, y,                  f'{initial_volume}\n({num_glaciers})',  # \nΔ = {delta:.2f}',                 va='center', ha='left', fontsize=12, color='black',                 backgroundcolor="white",  zorder=10)        p+=1            # # Plot overall average boxplots for Irr and Noirrp=0ax = axes[p]master_ds_hma = master_ds.groupby(['sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    # 'rgi_id': lambda _: "regional total",    # 'rgi_subregion': lambda _: "High Mountain Asia",    # 'rgi_region': lambda _: "High Mountain Asia",    # lamda is anonmous functions, returns 11 member average    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr']}#,  'rgi_id', 'rgi_subregion','rgi_region']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_hma['V_2264_noirr_delta'] = ((master_ds_hma['V_2264_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr'])*100master_ds_hma['V_2264_irr_delta'] = (master_ds_hma['V_2264_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_noirr_delta'] = (master_ds_hma['V_2014_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_irr_delta'] = (master_ds_hma['V_2014_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma = master_ds_hma[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]# avg_irr = plot_boxplot(master_ds_hma['V_2014_irr_delta'],  position_counter +#                        v_space_hist, "", color=colors['irr'][0], alpha=1, is_noirr=False)avg_irr = ax.scatter([position_counter +                       v_space_hist], master_ds_hma['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="x", s=100, linewidths=3)avg_irr = ax.scatter([position_counter +                       v_space_com], master_ds_hma['V_2264_irr_delta'].iloc[0], color=colors['irr'][1], marker="x", s=100, linewidths=3)avg_noi = plot_boxplot( master_ds_hma['V_2014_noirr_delta'], position_counter + v_space_hist, [    "High Mountain Asia"], color=colors['noirr'][0], alpha=1, is_noirr=True)avg_noi = plot_boxplot(master_ds_hma['V_2264_noirr_delta'], position_counter + v_space_com, [    "High Mountain Asia"], color=colors['noirr'][1], alpha=1, is_noirr=True)initial_total_volume = round(master_ds_hma['V_1985_irr'][0]*1e-9)# Annotate the number of glaciers for the overall averagelength=len(master_ds[master_ds.sample_id == "CNRM.001"])ax.text(x, y,          f"V:{initial_total_volume}\n(#:{length})",# Display total number of glaciers         va='center', ha='left', fontsize=12, color='black',          backgroundcolor="white", zorder=10)#fontstyle='italic',mean_hist = master_ds_hma['V_2014_noirr_delta'].mean()mean_com = master_ds_hma['V_2264_noirr_delta'].mean()ax.hlines(mean_hist,   xmin=position_counter + v_space_hist - 0.3,  # adjust width  xmax=position_counter + v_space_hist + 0.3,  linestyles='dashed',   color='black',  linewidth=2,  zorder=11)ax.hlines(mean_com,   xmin=position_counter + v_space_com - 0.3,  # adjust width  xmax=position_counter + v_space_com + 0.3,  linestyles='dashed',   color='black',  linewidth=2,  zorder=11)# position_counter += 4# Create custom legend elements for mean (dot) and median (stripe)# mean_dot = Line2D([0], [0], marker='o', color='w',#                   label='Area-weighted Mean (dot)', markerfacecolor='black', markersize=10)# median_stripe = Line2D([0], [0], color='black', lw=2, label='Median (stripe)')# Add a legend for regions, mean (dot), and median (stripe)region_legend_patches = [Line2D([0], [0], marker='x', color=colors['irr'][0], linestyle='None', markersize=10, label='Historical (W5E5)'),                        mpatches.Patch(color=colors['noirr'][0], label='Historical NoIrr'),                        Line2D([0], [0], marker='x', color=colors['irr'][1], linestyle='None', markersize=10, label='Committed (historical)'),                        mpatches.Patch(color=colors['noirr'][1], label='Committed (Historical NoIrr)'),                        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2, label=f'NoIrr, {len(master_ds_hma)}-member mean')                        ]# Append the custom legend items for the mean dot and median stripe# region_legend_patches += [median_stripe]# Create the legend with the updated patches# fig.legend(handles=region_legend_patches, loc='center right',#            bbox_to_anchor=(1.15, 0.6), ncols=1)fig.legend(handles=region_legend_patches, loc='upper center',          bbox_to_anchor=(0.512, 0.96), ncols=5, fontsize=12,columnspacing=1.5)# fig.subplots_adjust(right=0.97)fig.subplots_adjust(wspace=0.1)#, hspace=0.1)# Extend the ylims for more padding# y_min, y_max = fig.get_ylim()  # Get current y-axis limits# padding = 1  # Adjust this value as needed# ax.set_ylim(y_min - padding, y_max + padding)# Adjust layout and display the plot# plt.tight_layout()# fig_folder = os.path.join(fig_path, "03. Mass Balance", "Boxplot")# os.makedirs(fig_folder, exist_ok=True)# plt.savefig(#     f"{fig_folder}/Gaussian_distribution_total_region_by_region_median.png")plt.show()           #%% Cell 4e: Comitted mass loss boxplots totals - 1 plot# Function to plot the boxplotsylim=30y = ylim-10x=1.3def plot_boxplot(data, position, label, color, colorline, alpha, is_noirr):    if position<5:        width=2    else:        width=1    bp = ax.boxplot(data, patch_artist=True, labels=[""],#label if is_noirr else [""],  # only set labels for Noirr                    vert=True, widths=width,                    boxprops=dict(facecolor=color, alpha=alpha,                                  edgecolor='none'),                    medianprops=dict(color=colorline, linewidth=3),                    positions=[position], showfliers=False, zorder=2)        # Get the median value from the boxplot        ax.tick_params(labelsize=10)    if p>0:        # ax.tick_params(left=False, bottom=True, labelleft=False, labelbottom=False)        fontweight="medium"    else:                 for spine in ax.spines.values():            spine.set_linewidth(2)        fontweight="bold"    # if is_noirr and label == ["High Mountain Asia"]:    #     ax.set_xlabel("High \n Mountain \n Asia", rotation=0, fontweight=fontweight, fontsize=12, labelpad = 10 )    # elif is_noirr:    #     ax.set_xlabel(label[0], rotation=30, fontweight=fontweight, fontsize=12)    median_value = bp['medians'][0].get_xdata()[0]    ax.set_ylim(-75, ylim)    # ax.text(median_value + 10, position, f'{median_value:.1f}',  # Place above for Irr    #         va='center', ha='center', fontsize=10, color='black', #fontweight='bold',    #         # bbox=dict(boxstyle='round,pad=0.001',    #         #           facecolor='white', edgecolor='none'),    #         zorder=3)    return bpregions = [13, 14, 15]subregions = [9, 3, 3]df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]master_ds_tot_vol = master_ds.groupby(['rgi_subregion', 'sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    'rgi_id': lambda _: "regional total",    # lamda is anonmous functions, returns 11 member average    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr', 'sample_id', 'rgi_id']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_tot_vol['V_2264_noirr_delta'] = ((master_ds_tot_vol['V_2264_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr'])*100master_ds_tot_vol['V_2264_irr_delta'] = (master_ds_tot_vol['V_2264_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_noirr_delta'] = (master_ds_tot_vol['V_2014_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_irr_delta'] = (master_ds_tot_vol['V_2014_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol = master_ds_tot_vol[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]master_ds = master_ds[master_ds['V_2264_irr'].notna()]master_ds_tot_vol=master_ds_tot_vol[master_ds_tot_vol['V_2264_irr'].notna()]# master_ds_area_weighted=master_ds_area_weighted[master_ds_area_weighted['V_2264_irr'].notna()]use_weights = True# v_space_noi2 = 1.9  # Vertical space between irr and noirr boxplotsv_space_com = 1.6  # Vertical space between irr and noirr boxplotsv_space_hist = 0.4  # Vertical space between irr and noirr boxplotsv_space_com_all = 2.3  # Vertical space between irr and noirr boxplotsv_space_hist_all = 0.1  # Vertical space between irr and noirr boxplots# v_space_irr1 = 0.1  # Vertical space between irr and noirr boxplots# Storage for combined dataall_noirr, all_irr = [], []position_counter = 1cumulative_index = 14# Initialize plotfig = plt.figure(figsize=(16, 6))gs = gridspec.GridSpec(1, 16, width_ratios=[1.5] + [1]*15)  # First axis twice as wide# axes = [fig.add_subplot(gs[i]) for i in range(16)]fig, axes = plt.subplots(1,1, figsize=(16, 6), sharey=True)print(axes)# axes = axes.flatten()p=5.4tick_positions=[]tick_labels=[]mean_list=[]# Example usage: Main loop through regions and subregionsfor r, region in enumerate((regions)):    for sub in range(list((subregions))[r]):        # Filter subregion-specific data        ax = axes        region_id = f"{region}.0{sub+1}"        print(region_id)               subregion_ds = master_ds_tot_vol[master_ds_tot_vol['rgi_subregion'].str.contains(            f"{region}-0{sub+1}")]        # Calculate mean values for Noirr and Irr        # noirr_mean = master_ds_area_weighted['V_2264_noirr'][cumulative_index]        # irr_mean = master_ds_area_weighted['V_2264_irr'][cumulative_index]        # Set color and label based on the region        try:            label = subregion_ds.full_name.iloc[0]        except:            label = f"{region}-0{sub+1}"                if region==13:            if sub ==1:                label = "Pamir"            elif sub ==3:                label = "East Tien Shan"            elif sub ==5:                label = "East Kun Lun"        # Plot Noirr and Irr boxplots        print(position_counter)        print(subregion_ds['V_2014_irr_delta'].iloc[0])        print(subregion_ds['V_2264_irr_delta'].iloc[0])                tick_positions.append(p +1 )#+ v_space_com -0.2)  # or average of hist + com if you want centered        tick_labels.append(label)                mean_hist = subregion_ds['V_2014_noirr_delta'].mean()        mean_com_irr = subregion_ds['V_2264_irr_delta'].mean()         mean_com_noirr = subregion_ds['V_2264_noirr_delta'].mean()           mean_list.append((region_id, mean_com_irr, mean_com_noirr))                box_noirr = plot_boxplot(            subregion_ds['V_2014_noirr_delta'], p + v_space_hist, label, color=colors['noirr'][1],colorline=colors['cline'][0], alpha=1, is_noirr=True)        box_noirr = plot_boxplot(             subregion_ds['V_2264_noirr_delta'], p + v_space_com, label, color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)                box_irr = ax.scatter([p +                               v_space_hist], subregion_ds['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="x", s=60, linewidths=3, zorder=10)        box_irr = ax.scatter([p +                               v_space_com], subregion_ds['V_2264_irr_delta'].iloc[1], color=colors['irr_com'][0], marker="x", s=60, linewidths=3, zorder=10)        # ax.hlines(mean_hist,         #   xmin=p + v_space_hist - 0.5,  # adjust width        #   xmax=p + v_space_hist + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][0],        #   linewidth=3,        #   zorder=9)                # ax.hlines(mean_com_noirr,         #   xmin=p + v_space_com - 0.5,  # adjust width        #   xmax=p + v_space_com + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][1],        #   linewidth=3,        #   zorder=9)                # Annotate the number of glaciers and delta between the two columns        num_glaciers = len(master_ds[master_ds.rgi_subregion==f"{region}-0{sub+1}"][master_ds.sample_id=="IPSL-CM6.000"])                # delta = noirr_mean - irr_mean        # Display number of glaciers and delta        initial_volume = round(subregion_ds['V_1985_irr'].iloc[0]*1e-9)        # ax.text(p, y,         #          f'{initial_volume}\n({num_glaciers})',  # \nΔ = {delta:.2f}',        #          va='center', ha='left', fontsize=12, color='black',        #          backgroundcolor="white",  zorder=10)        ax.axvline(p-0.5, color='lightgrey', linestyle='--',                    linewidth=1, zorder=1)         p+=3            # # Plot overall average boxplots for Irr and Noirrp=1.2ax = axesmaster_ds_hma = master_ds.groupby(['sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr']}#,  'rgi_id', 'rgi_subregion','rgi_region']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_hma['V_2264_noirr_delta'] = ((master_ds_hma['V_2264_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr'])*100master_ds_hma['V_2264_irr_delta'] = (master_ds_hma['V_2264_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_noirr_delta'] = (master_ds_hma['V_2014_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_irr_delta'] = (master_ds_hma['V_2014_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma = master_ds_hma[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]avg_irr = ax.scatter([p +                       v_space_hist_all], master_ds_hma['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="x", s=100, linewidths=4)avg_irr = ax.scatter([p +                       v_space_com_all], master_ds_hma['V_2264_irr_delta'].iloc[0], color=colors['irr_com'][0], marker="x", s=100, linewidths=4)avg_noi_hist = plot_boxplot( master_ds_hma['V_2014_noirr_delta'], p + v_space_hist_all, [    "High Mountain Asia"], color=colors['noirr'][1], colorline=colors['cline'][0], alpha=1, is_noirr=True)avg_noi_com = plot_boxplot(master_ds_hma['V_2264_noirr_delta'], p + v_space_com_all, [    "High Mountain Asia"], color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)initial_total_volume = round(master_ds_hma['V_1985_irr'][0]*1e-9)# Annotate the number of glaciers for the overall averagelength=len(master_ds[master_ds.sample_id == "CNRM.001"])# ax.text(v_space_hist, y, #          f"V:{initial_total_volume}\n(#:{length})",# Display total number of glaciers#          va='center', ha='left', fontsize=12, color='black',#           backgroundcolor="white", zorder=10)#fontstyle='italic',mean_hist = master_ds_hma['V_2014_noirr_delta'].mean()mean_com_irr = master_ds_hma['V_2264_irr_delta'].mean()mean_com_noirr = master_ds_hma['V_2264_noirr_delta'].mean()mean_list.append(("High Mountain Asia", mean_com_irr, mean_com_noirr))df_means = pd.DataFrame(mean_list, columns=["subregion", "V_2264_irr_delta","V_2264_noirr_delta"])df_means.to_csv(f"{wd_path}masters/mean_deltaV_Comitted.csv")# ax.hlines(mean_hist, #   xmin=p + v_space_hist - 1.3,  # adjust width#   xmax=p + v_space_hist + 0.7,#   linestyles='dotted', #   color=colors['cline'][0],#   linewidth=3,#   zorder=11)# ax.hlines(mean_com_noirr, #   xmin=p + v_space_com -0.3,  # adjust width#   xmax=p + v_space_com + 1.7,#   linestyles='dotted', #   color=colors['cline'][1],#   linewidth=3,#   zorder=11)# Add a legend for regions, mean (dot), and median (stripe)region_legend_patches = [Line2D([0], [0], marker='x', color=colors['irr'][0], linestyle='None', markersize=8, lw=20, label='Historical (W5E5)'),                        mpatches.Patch(color=colors['noirr'][1], label='Historical NoIrr'),                        Line2D([0], [0], marker='x', color=colors['irr_com'][0], linestyle='None', markersize=8, lw=20, label='Committed (historical)'),                        mpatches.Patch(color=colors['noirr_com'][0], label='Committed (Historical NoIrr)'),                        # Line2D([0], [0], color='black', linestyle='dashed', linewidth=2, label=f'NoIrr, {len(master_ds_hma)}-member mean')                        ]tick_positions.append(1 + v_space_com)tick_labels.append("High\nMountain\nAsia")# Set tick positions and rotated labels (except HMA)ax.set_xticks(tick_positions)ax.set_xticklabels(tick_labels, fontsize=14, rotation=30)# Override HMA rotation to be flat (if needed)for label in ax.get_xticklabels():    if label.get_text() == "High\nMountain\nAsia":        label.set_rotation(0)        label.set_fontweight("bold")        fig.legend(handles=region_legend_patches, loc='upper center',          bbox_to_anchor=(0.512, 0.96), ncols=5, fontsize=14,columnspacing=1.5)ax.set_ylabel('Volume change (%, vs. 1985 historic)', labelpad=15, fontsize=14)fig.subplots_adjust(wspace=0.1)#, hspace=0.1)ax.set_xlim(0, 3*15+5)ax.tick_params(labelsize=12)ax.axhline(y=0, color='grey', linestyle='--', linewidth=1, zorder=0)output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member.nc")# output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member_noIPSL.nc")tlines = xr.open_dataset(output_nc_path)ax_inset = inset_axes(ax, width="24%", height="28%", loc='lower left',bbox_to_anchor=(286,130,1800, 700))  # width/height can be % or floatax_inset.tick_params(axis='both', labelsize=12)ax_inset.yaxis.tick_right()             # Move tick labels to the rightax_inset.yaxis.set_label_position("right")for m,model in enumerate(models_shortlist):    for x in range(members_averages[m]):        if x>0:            ax_inset.plot(                tlines.sel(scenario='committed', model=model, member=x, experiment="NoIrr").time.values,               tlines.sel(scenario='committed', model=model, member=x,experiment="NoIrr").volume_percent-100, ls=':', lw=1, color=colors['noirr_com'][0]            )ax_inset.plot(tlines.sel(scenario='committed', model="W5E5", member=0, experiment="Irr").time.values,tlines.sel(scenario='committed', model="W5E5", member=0,experiment="Irr").volume_percent-100, ls='-', lw=2, color=colors['irr_com'][0])ax_inset.plot(tlines.sel(scenario='committed', model="avg", member=0, experiment="NoIrr").time.values,tlines.sel(scenario='committed', model="avg", member=0,experiment="NoIrr").volume_percent-100, ls='--', lw=2, color=colors['noirr_com'][0])ax_inset.set_title("Transient evolution", fontsize=12)ax_inset.set_xticks([2014, 2139, 2264])ax_inset.set_xticklabels(['0', '125', '250'])rect_x = p + v_space_hist_all -1.1rect_y = -25rect_w=4.5rect_height=30square = mpatches.Rectangle((rect_x, rect_y), rect_w, rect_height,                               linewidth=0.7, edgecolor='black', facecolor='none')ax.add_patch(square)rect_x_zoom = 0.2rect_y_zoom = -67rect_w_zoom=12rect_height_zoom=30square_zoom = mpatches.Rectangle((rect_x_zoom, rect_y_zoom), rect_w_zoom, rect_height_zoom,                               linewidth=1, edgecolor='none', facecolor='none')ax.add_patch(square_zoom)# --- Convert rectangle data coordinates to figure coordinates ---# Lower left and lower right corners of the rectanglell_data = (rect_x, rect_y)lr_data = (rect_x + rect_w, rect_y)ll_disp = ax.transData.transform(ll_data)lr_disp = ax.transData.transform(lr_data)ll_fig = fig.transFigure.inverted().transform(ll_disp)lr_fig = fig.transFigure.inverted().transform(lr_disp)# --- Get upper left and right of the inset in figure coordinates ---# These are in the inset's axes coordinates, so (0, 1) is upper-left, (1, 1) is upper-rightul_data_zoom = (rect_x_zoom, rect_y_zoom+rect_height_zoom)ur_data_zoom = (rect_x_zoom + rect_w_zoom, rect_y_zoom+rect_height_zoom)ul_disp_zoom = ax.transData.transform(ul_data_zoom)ur_disp_zoom = ax.transData.transform(ur_data_zoom)ul_fig_zoom = fig.transFigure.inverted().transform(ul_disp_zoom)ur_fig_zoom = fig.transFigure.inverted().transform(ur_disp_zoom)# --- Draw lines from rectangle to inset corners ---line_left = mlines.Line2D([ll_fig[0], ul_fig_zoom[0]], [ll_fig[1], ul_fig_zoom[1]],                          transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)line_right = mlines.Line2D([lr_fig[0], ur_fig_zoom[0]], [lr_fig[1], ur_fig_zoom[1]],                           transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)fig.lines.extend([line_left, line_right])# Extend the ylims for more padding# y_min, y_max = fig.get_ylim()  # Get current y-axis limits# padding = 1  # Adjust this value as needed# ax.set_ylim(y_min - padding, y_max + padding)# Adjust layout and display the plot# plt.tight_layout()# =fig_folder = os.path.join(fig_path)#, "03. Mass Balance", "Boxplot")# os.makedirs(fig_folder, exist_ok=True)plt.savefig(    f"{fig_path}/Boxplot_Comitted_Mass_Loss.png")plt.show()      #%% Cell 4d: Create comitted mass loss plot transient for totals onlymembers_averages = [2, 3,  3,  5, 1]models_shortlist = ["E3SM", "CESM2",  "NorESM",  "CNRM", "IPSL-CM6"]# define the variables for p;lottingvariables = ["volume", "area"]factors = [10**-9, 10**-6]variable_names = ["Volume", "Area"]variable_axes = ["Volume compared to 1985 Irr-scenario [%]",                 "Area compared to 1985 Irr-scenario [%]"]use_multiprocessing = False# output_csv_path = os.path.join(wd_path, "masters", f"error_rgi_ids.csv")error_ids =[""] #pd.read_csv(output_csv_path)['rgi_id'].tolist()subset_gdirs = gdirs_3r_a1[:100]# nan_mask = pd.read_csv(os.path.join(#     wd_path, "masters", "nan_mask_comitted_random.csv")).rgi_ids# # Remove duplicates if needed# nan_mask = set(pd.DataFrame({'rgi_id': nan_mask.unique()}).rgi_id.to_numpy())# rgi_ids_test = []# for gdir in subset_gdirs:#     rgi_ids_test.append(gdir.rgi_id)# rgi_ids_test = [rgi_id for rgi_id in rgi_ids_test if rgi_id not in nan_mask]# print(len(rgi_ids_test))all_var_data = {}# repeat the loop for area and volumefor v, var in enumerate(variables):    print(var)    # create a new plot for both area and volume    fig, ax = plt.subplots(figsize=(7, 4))  # create a new figure    # create a timeseries for all the model members to add the data, needed to calculate averages later    linestyles = ['solid', 'solid']    for f, filepath in enumerate([f"climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_committed_random.nc"]):        scenario = "committed" if "committed" in filepath or "comitted" in filepath else "historical"        # for f, filepath in enumerate([f"climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_comitted_cst_test.nc"]):        if f == 1:            color_id = "_com"            legend_id = "committed"            bar_values = 50        else:            color_id = ""            legend_id = ""            bar_values = 20        print(f)        # load and plot the baseline data        baseline_path = os.path.join(            wd_path, "summary", filepath)        baseline = xr.open_dataset(baseline_path)        # if f == 0:        # rgi_ids_test = baseline.rgi_id.values[:10]                # baseline = baseline.where(        #     baseline.rgi_id.isin(rgi_ids_test), drop=True)                # print(baseline[var].sum(        # dim="rgi_id").values * factors[v])        # print(len(baseline.rgi_id))        if f == 0:            resp_value = baseline[var].sum(dim="rgi_id")[0].values * factors[v]        ax.plot(baseline["time"], (baseline[var].sum(dim="rgi_id") * factors[v])/resp_value*100,                label=f"AllForcings {legend_id}", color=colors[f"irr{color_id}"][0], linewidth=2, zorder=15, linestyle=linestyles[f])        series_values= (baseline[var].sum(dim="rgi_id") * factors[v])/resp_value*100        da = xr.DataArray(            series_values,            dims=["time"],            coords={                "time": baseline["time"].values,                "model": "W5E5",                "member": 0,                "scenario": scenario,                "experiment":"Irr"            },            name=variable_names[v].lower() + "_percent"        )        # Initialize if needed        if variable_names[v].lower() not in all_var_data:            all_var_data[variable_names[v].lower()] = []                all_var_data[variable_names[v].lower()].append(da)                print("baseline time from:", baseline["time"][0].values)        print("baseline time to:", baseline["time"][-1].values)        mean_values_irr = (baseline[var].sum(            dim="rgi_id") * factors[v]).values        member_data_noirr = []        nan_runs_noirr = []                        for m, model in enumerate(models_shortlist):            for i in range(members_averages[m]):                # make sure the counter for sample ids starts with 001, 000 are averages of all members by model                # IPSL-CM6 only has 1 member, so the sample_id must end with 000                if members_averages[m] > 1:                    i += 1                    label = None                else:                    label = "GCM member"                sample_id = f"{model}.00{i}"                filepath = [f'climate_run_output_perturbed_{sample_id}.nc',                            f'climate_run_output_perturbed_{sample_id}_committed_random.nc'][f]                # f'climate_run_output_perturbed_{sample_id}_comitted_cst_test.nc'][f]                print(sample_id)                # load and plot the data from the climate output run and counterfactual                climate_run_opath_noirr = os.path.join(                    sum_dir, filepath)  # f'climate_run_output_perturbed_{sample_id}_comitted.nc')                climate_run_output_noirr = xr.open_dataset(                    climate_run_opath_noirr)                # if f == 0:                # rgi_ids_test=baseline.rgi_id[:10]                                # climate_run_output_noirr = climate_run_output_noirr.where(                #     climate_run_output_noirr.rgi_id.isin(rgi_ids_test), drop=True)                                ax.plot(climate_run_output_noirr["time"], (climate_run_output_noirr[var].sum(dim="rgi_id") * factors[v])/resp_value*100,                        label=None, color=colors[f"noirr{color_id}"][1], linewidth=1, linestyle=linestyles[f])                # add all the summed volumes/areas to the member list, so a multi-member average can be calculated                member_data_noirr.append((climate_run_output_noirr[var].sum(                    dim="rgi_id").values/resp_value*100 * factors[v]))                # print(len(climate_run_output_noirr[var][0].values))                                #prepare data for saving                # Extract time values (ensure they're datetime64)                time_values = climate_run_output_noirr["time"].values                print(time_values)                series_values = (climate_run_output_noirr[var].sum(                    dim="rgi_id").values / resp_value * 100 * factors[v]).flatten()                # scenario = "committed" if "committed" in filepath or "comitted" in filepath else "historical"                da = xr.DataArray(                    series_values,                    dims=["time"],                    coords={                        "time": time_values,                        "model": model,                        "member": i,                        "scenario": scenario,                        "experiment":"NoIrr"                    },                    name=variable_names[v].lower() + "_percent"                )                # Initialize if needed                if variable_names[v].lower() not in all_var_data:                    all_var_data[variable_names[v].lower()] = []                                all_var_data[variable_names[v].lower()].append(da)        # stack the member data        stacked_member_data = np.stack(member_data_noirr)        all_nan_rgi_ids = np.unique(nan_runs_noirr)        df_nan_rgi_ids = pd.DataFrame({'rgi_id': all_nan_rgi_ids})        output_csv_path = os.path.join(            wd_path, "masters", f"error_rgi_ids.csv")        df_nan_rgi_ids.to_csv(output_csv_path, index=False)        # calculate and plot volume/area 10-member mean        mean_values_noirr = np.median(stacked_member_data, axis=0).flatten()        # mean_values_noirr = np.mean(stacked_member_data, axis=0).flatten()        ax.plot(climate_run_output_noirr["time"].values, mean_values_noirr,                color=colors[f"noirr{color_id}"][0], linestyle=linestyles[f], lw=2, label=f"NoIrr ({sum(members_averages)}-member avg) {legend_id}")        da = xr.DataArray(            mean_values_noirr,            dims=["time"],            coords={                "time": climate_run_output_noirr["time"].values,                "model": "avg",                "member": 0,                "scenario": scenario,                "experiment":"NoIrr"            },            name=variable_names[v].lower() + "_percent"        )        # Initialize if needed        if variable_names[v].lower() not in all_var_data:            all_var_data[variable_names[v].lower()] = []                all_var_data[variable_names[v].lower()].append(da)                # calculate and plot volume/area 10-member min and max for ribbon        min_values_noirr = np.min(stacked_member_data, axis=0).flatten()        max_values_noirr = np.max(stacked_member_data, axis=0).flatten()        ax.fill_between(climate_run_output_noirr["time"].values, min_values_noirr, max_values_noirr,                        color=colors[f"noirr{color_id}"][1], alpha=0.3, label=f"NoIrr ({sum(members_averages)}-member range) {legend_id}", zorder=16)        if f == 0:            var_value_1985 = mean_values_irr[0]    # Set labels and title for the combined plot    ax.set_ylabel(variable_axes[v])    ax.set_xlabel("Time [year]")    ax.set_title(f"Summed {variable_names[v]}, RGI 13-15, A >1 km$^2$")    # Adjust the legend    handles, labels = ax.get_legend_handles_labels()    fig.legend(handles, labels, loc='lower center',               bbox_to_anchor=(0.5, -0.15), ncol=3)    plt.tight_layout()    # specify and create a folder for saving the data (if it doesn't exists already) and save the plot    o_folder_data = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/02. OGGM simulations/01. Modelled output/3r_a1/0{v + 1}. {variable_names[v]}/00. Combined"    os.makedirs(o_folder_data, exist_ok=True)    o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.{variable_names[v]}.committed_random.png"    # o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.{variable_names[v]}.comitted_cst_test.png"    # plt.savefig(o_file_name, bbox_inches='tight')    dataset_vars = {}        for var_key, da_list in all_var_data.items():        # Concatenate over a fake 'stacked' dimension        combined_da = xr.concat(da_list, dim="stacked")        combined_da = combined_da.assign_coords(            model=("stacked", [da.coords["model"].item() for da in da_list]),            member=("stacked", [da.coords["member"].item() for da in da_list]),            scenario=("stacked", [da.coords["scenario"].item() for da in da_list]),            experiment=("stacked", [da.coords["experiment"].item() for da in da_list])        )                # Set MultiIndex with all keys used        combined_da = combined_da.set_index(stacked=["model", "member", "scenario","experiment"])                # Unstack cleanly since entries are now unique        combined_da = combined_da.unstack("stacked")                dataset_vars[var_key + "_percent"] = combined_da    # Build the dataset    final_ds = xr.Dataset(dataset_vars)        # Save the dataset    output_nc_path = os.path.join(wd_path, "masters", f"master_comitted_{var}_timeseries_individual_member.nc")    # output_nc_path = os.path.join(wd_path, "masters", f"master_comitted_{var}_timeseries_individual_member_noIPSL.nc")    final_ds.to_netcdf(output_nc_path)    print(f"Saved dataset with dimensions (model, member, time) to:\n{output_nc_path}")#%% Cell 4f: Comitted mass loss boxplots totals - 1 plot, Marzeion edit# Function to plot the boxplotsylim=50y = ylim-10x=1.3def plot_boxplot(data, position, label, color, colorline, alpha, is_noirr):    if position<5:        width=2    else:        width=1    q1 = np.percentile(data, 25)    q3 = np.percentile(data, 75)    median = np.median(data)    iqr_height = q3 - q1    lower_bound = q1 - 1.5 * iqr_height    upper_bound = q3 + 1.5 * iqr_height    filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]    if len(filtered_data) > 0:        lower_whisker = filtered_data.min()        upper_whisker = filtered_data.max()    else:        lower_whisker = q1        upper_whisker = q3    # Draw the shaded IQR rectangle    rect = plt.Rectangle((position - width / 2, q1),                         width, iqr_height,                         facecolor=color, alpha=alpha,                         edgecolor='none', zorder=1)    ax.add_patch(rect)        rect = plt.Rectangle((position - width / 2, lower_whisker),                             width, upper_whisker - lower_whisker,                             facecolor=color, alpha=0.4, edgecolor='none', zorder=0)    ax.add_patch(rect)    # Draw the median line    ax.plot([position - width / 2 + 0.09, position + width / 2 -0.09],            [median, median],            color=colorline, linewidth=3, zorder=2)    # Optional styling    ax.tick_params(labelsize=10)    if p > 0:        fontweight = "medium"    else:        for spine in ax.spines.values():            spine.set_linewidth(2)        fontweight = "bold"    ax.set_ylim(-75, ylim)    return rectregions = [13, 14, 15]subregions = [9, 3, 3]df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a5_rgi_date_A_V_RGIreg_B_hugo_Vcom_noIPSL.csv")# f"{wd_path}masters/master_gdirs_r3_a5_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]master_ds=master_ds[~master_ds['sample_id'].str.startswith('IPSL')]master_ds_tot_vol = master_ds.groupby(['rgi_subregion', 'sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    'rgi_id': lambda _: "regional total",    # lamda is anonmous functions, returns 11 member average    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr', 'sample_id', 'rgi_id']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_tot_vol['V_2264_noirr_delta'] = ((master_ds_tot_vol['V_2264_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr'])*100master_ds_tot_vol['V_2264_irr_delta'] = (master_ds_tot_vol['V_2264_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_noirr_delta'] = (master_ds_tot_vol['V_2014_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_irr_delta'] = (master_ds_tot_vol['V_2014_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol = master_ds_tot_vol[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]master_ds = master_ds[master_ds['V_2264_irr'].notna()]master_ds_tot_vol=master_ds_tot_vol[master_ds_tot_vol['V_2264_irr'].notna()]# master_ds_area_weighted=master_ds_area_weighted[master_ds_area_weighted['V_2264_irr'].notna()]use_weights = True# v_space_noi2 = 1.9  # Vertical space between irr and noirr boxplotsv_space_com = 1.6  # Vertical space between irr and noirr boxplotsv_space_hist = 0.4  # Vertical space between irr and noirr boxplotsv_space_com_all = 2.3  # Vertical space between irr and noirr boxplotsv_space_hist_all = 0.1  # Vertical space between irr and noirr boxplots# v_space_irr1 = 0.1  # Vertical space between irr and noirr boxplots# Storage for combined dataall_noirr, all_irr = [], []position_counter = 1cumulative_index = 14# Initialize plotfig = plt.figure(figsize=(16, 6))gs = gridspec.GridSpec(1, 16, width_ratios=[1.5] + [1]*15)  # First axis twice as wide# axes = [fig.add_subplot(gs[i]) for i in range(16)]fig, axes = plt.subplots(1,1, figsize=(16, 6), sharey=True)print(axes)# axes = axes.flatten()p=5.4tick_positions=[]tick_labels=[]mean_list=[]# Example usage: Main loop through regions and subregionsfor r, region in enumerate((regions)):    for sub in range(list((subregions))[r]):        # Filter subregion-specific data        ax = axes        region_id = f"{region}.0{sub+1}"        print(region_id)               subregion_ds = master_ds_tot_vol[master_ds_tot_vol['rgi_subregion'].str.contains(            f"{region}-0{sub+1}")]        # Calculate mean values for Noirr and Irr        # noirr_mean = master_ds_area_weighted['V_2264_noirr'][cumulative_index]        # irr_mean = master_ds_area_weighted['V_2264_irr'][cumulative_index]        # Set color and label based on the region        try:            label = subregion_ds.full_name.iloc[0]        except:            label = f"{region}-0{sub+1}"                if region==13:            if sub ==1:                label = "Pamir"            elif sub ==3:                label = "East Tien Shan"            elif sub ==5:                label = "East Kun Lun"        # Plot Noirr and Irr boxplots        print(position_counter)        print(subregion_ds['V_2014_irr_delta'].iloc[0])        print(subregion_ds['V_2264_irr_delta'].iloc[0])                tick_positions.append(p +1 )#+ v_space_com -0.2)  # or average of hist + com if you want centered        tick_labels.append(label)                mean_hist = subregion_ds['V_2014_noirr_delta'].mean()        mean_com_irr = subregion_ds['V_2264_irr_delta'].mean()         mean_com_noirr = subregion_ds['V_2264_noirr_delta'].mean()           mean_list.append((region_id, mean_com_irr, mean_com_noirr))                box_noirr = plot_boxplot(            subregion_ds['V_2014_noirr_delta'], p + v_space_hist, label, color=colors['noirr'][1],colorline=colors['cline'][0], alpha=1, is_noirr=True)        box_noirr = plot_boxplot(             subregion_ds['V_2264_noirr_delta'], p + v_space_com, label, color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)                box_irr = ax.scatter([p +                               v_space_hist], subregion_ds['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="o", s=60, linewidths=3, zorder=10)        box_irr = ax.scatter([p +                               v_space_com], subregion_ds['V_2264_irr_delta'].iloc[1], color=colors['irr_com'][0], marker="o", s=60, linewidths=3, zorder=10)        # ax.hlines(mean_hist,         #   xmin=p + v_space_hist - 0.5,  # adjust width        #   xmax=p + v_space_hist + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][0],        #   linewidth=3,        #   zorder=9)                # ax.hlines(mean_com_noirr,         #   xmin=p + v_space_com - 0.5,  # adjust width        #   xmax=p + v_space_com + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][1],        #   linewidth=3,        #   zorder=9)                # Annotate the number of glaciers and delta between the two columns        num_glaciers = len(master_ds[master_ds.rgi_subregion==f"{region}-0{sub+1}"][master_ds.sample_id=="IPSL-CM6.000"])                # delta = noirr_mean - irr_mean        # Display number of glaciers and delta        initial_volume = round(subregion_ds['V_1985_irr'].iloc[0]*1e-9)        # ax.text(p, y,         #          f'{initial_volume}\n({num_glaciers})',  # \nΔ = {delta:.2f}',        #          va='center', ha='left', fontsize=12, color='black',        #          backgroundcolor="white",  zorder=10)        ax.axvline(p-0.5, color='lightgrey', linestyle='--',                    linewidth=1, zorder=1)         p+=3            # # Plot overall average boxplots for Irr and Noirrp=1.2ax = axesmaster_ds_hma = master_ds.groupby(['sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr']}#,  'rgi_id', 'rgi_subregion','rgi_region']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_hma['V_2264_noirr_delta'] = ((master_ds_hma['V_2264_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr'])*100master_ds_hma['V_2264_irr_delta'] = (master_ds_hma['V_2264_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_noirr_delta'] = (master_ds_hma['V_2014_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_irr_delta'] = (master_ds_hma['V_2014_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma = master_ds_hma[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]avg_irr = ax.scatter([p +                       v_space_hist_all], master_ds_hma['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="o", s=100, linewidths=4)avg_irr = ax.scatter([p +                       v_space_com_all], master_ds_hma['V_2264_irr_delta'].iloc[0], color=colors['irr_com'][0], marker="o", s=100, linewidths=4)avg_noi_hist = plot_boxplot( master_ds_hma['V_2014_noirr_delta'], p + v_space_hist_all, [    "High Mountain Asia"], color=colors['noirr'][1], colorline=colors['cline'][0], alpha=1, is_noirr=True)avg_noi_com = plot_boxplot(master_ds_hma['V_2264_noirr_delta'], p + v_space_com_all, [    "High Mountain Asia"], color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)initial_total_volume = round(master_ds_hma['V_1985_irr'][0]*1e-9)# Annotate the number of glaciers for the overall averagelength=len(master_ds[master_ds.sample_id == "CNRM.001"])# ax.text(v_space_hist, y, #          f"V:{initial_total_volume}\n(#:{length})",# Display total number of glaciers#          va='center', ha='left', fontsize=12, color='black',#           backgroundcolor="white", zorder=10)#fontstyle='italic',mean_hist = master_ds_hma['V_2014_noirr_delta'].mean()mean_com_irr = master_ds_hma['V_2264_irr_delta'].mean()mean_com_noirr = master_ds_hma['V_2264_noirr_delta'].mean()mean_list.append(("High Mountain Asia", mean_com_irr, mean_com_noirr))df_means = pd.DataFrame(mean_list, columns=["subregion", "V_2264_irr_delta","V_2264_noirr_delta"])df_means.to_csv(f"{wd_path}masters/mean_deltaV_Comitted.csv")# Add a legend for regions, mean (dot), and median (stripe)region_legend_patches = [Line2D([0], [0], marker='o', color=colors['irr'][0], linestyle='None', markersize=8, lw=20, label='Historical (W5E5)'),                        mpatches.Patch(color=colors['noirr'][1], label='Historical NoIrr'),                        Line2D([0], [0], marker='o', color=colors['irr_com'][0], linestyle='None', markersize=8, lw=20, label='Committed (historical)'),                        mpatches.Patch(color=colors['noirr_com'][0], label='Committed (Historical NoIrr)'),                        # Line2D([0], [0], color='black', linestyle='dashed', linewidth=2, label=f'NoIrr, {len(master_ds_hma)}-member mean')                        ]tick_positions.append(1 + v_space_com)tick_labels.append("High\nMountain\nAsia")# Set tick positions and rotated labels (except HMA)ax.set_xticks(tick_positions)ax.set_xticklabels(tick_labels, fontsize=14, rotation=30)# Override HMA rotation to be flat (if needed)for label in ax.get_xticklabels():    if label.get_text() == "High\nMountain\nAsia":        label.set_rotation(0)        label.set_fontweight("bold")        fig.legend(handles=region_legend_patches, loc='upper center',          bbox_to_anchor=(0.512, 0.96), ncols=5, fontsize=14,columnspacing=1.5)ax.set_ylabel('Volume change (%, vs. 1985 historic)', labelpad=15, fontsize=14)fig.subplots_adjust(wspace=0.1)#, hspace=0.1)ax.set_xlim(0, 3*15+5)ax.tick_params(labelsize=12)# Include the panel plotax.axhline(y=0, color='grey', linestyle='--', linewidth=1, zorder=0)output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member.nc")# output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member_noIPSL.nc")tlines = xr.open_dataset(output_nc_path)ax_inset = inset_axes(ax, width="24%", height="32%", loc='lower left',bbox_to_anchor=(286,100,1800, 700))  # width/height can be % or floatfor m,model in enumerate(models_shortlist):    member_series = []    times = None        for x in range(members_averages[m]):        if x>0:            ax_inset.plot(                tlines.sel(scenario='committed', model=model, member=x, experiment="NoIrr").time.values,               tlines.sel(scenario='committed', model=model, member=x,experiment="NoIrr").volume_percent-100, ls=':', lw=1, color=colors['noirr_com'][0], zorder=1            )                        data = tlines.sel(scenario='committed', model=model, member=x, experiment="NoIrr")            volume = data.volume_percent - 100            member_series.append(volume)            if times is None:                times = data.time.values                if member_series:        stacked = xr.concat(member_series, dim='member')        q25 = stacked.quantile(0.25, dim='member')        q75 = stacked.quantile(0.75, dim='member')        # Fill between 25th and 75th percentile        ax_inset.fill_between(            times, q25, q75,            color=colors['noirr_com'][1],            alpha=0.7,            zorder=0        )                                                                ax_inset.plot(tlines.sel(scenario='committed', model="W5E5", member=0, experiment="Irr").time.values,tlines.sel(scenario='committed', model="W5E5", member=0,experiment="Irr").volume_percent-100, ls='-', lw=2, color=colors['irr_com'][0])ax_inset.plot(tlines.sel(scenario='committed', model="avg", member=0, experiment="NoIrr").time.values,tlines.sel(scenario='committed', model="avg", member=0,experiment="NoIrr").volume_percent-100, ls='--', lw=2, color=colors['noirr_com'][0], zorder=0)# ax_inset.text(0.2, 0.95, "Committed evolution", fontsize=12,#               transform=ax_inset.transAxes, va='top', ha='left')# Set axis ticks and labelsax_inset.set_xlim(2014,2264)ax_inset.set_xticks([2114, 2214])ax_inset.set_xticklabels(['100', '200 yrs'])ax_inset.set_ylim(-30,10)ax_inset.set_yticks([ 0, -10, -20])#, -30])ax_inset.set_yticklabels([ '0%', '-10%', '-20%'])#, '-30%'])# ax_inset.axhline(0, color='grey', linestyle='--', linewidth=1)# ax_inset.yaxis.label.set_zorder(10)# Tick marks and labels setupax_inset.tick_params(axis='both', which='both',                     direction='in',     # tick lines point inside                     length=5,           # visible tick length                     top=False, bottom=True, left=False, right=True,                     labelsize=12)# Tick labels on right side of y-axisax_inset.yaxis.set_ticks_position('left')ax_inset.yaxis.set_label_position("left")ax_inset.grid(True, axis='both', which='major', linestyle=':', linewidth=0.5, color='gray')for label in ax_inset.get_xticklabels():    label.set_verticalalignment('top')    label.set_y(0.2)  # smaller = closer to axis line (inside)# Shift y-axis labels left into the plotfor label in ax_inset.get_yticklabels():    label.set_horizontalalignment('left')    label.set_x(0.05)  # smaller = closer to axis line (inside)    rect_x = p + v_space_hist_all + 1.05rect_y = -24rect_w=2.25rect_height=24square = mpatches.Rectangle((rect_x, rect_y), rect_w, rect_height,                               linewidth=0.7, edgecolor='black', facecolor='none')ax.add_patch(square)rect_x_zoom = 0.2rect_y_zoom = ylim-110rect_w_zoom=12rect_height_zoom=30square_zoom = mpatches.Rectangle((rect_x_zoom, rect_y_zoom), rect_w_zoom, rect_height_zoom,                               linewidth=1, edgecolor='none', facecolor='none')ax.add_patch(square_zoom)# --- Convert rectangle data coordinates to figure coordinates ---# Lower left and lower right corners of the rectanglell_data = (rect_x, rect_y)lr_data = (rect_x + rect_w, rect_y)ll_disp = ax.transData.transform(ll_data)lr_disp = ax.transData.transform(lr_data)ll_fig = fig.transFigure.inverted().transform(ll_disp)lr_fig = fig.transFigure.inverted().transform(lr_disp)# --- Get upper left and right of the inset in figure coordinates ---# These are in the inset's axes coordinates, so (0, 1) is upper-left, (1, 1) is upper-rightul_data_zoom = (rect_x_zoom, rect_y_zoom+rect_height_zoom)ur_data_zoom = (rect_x_zoom + rect_w_zoom, rect_y_zoom+rect_height_zoom)ul_disp_zoom = ax.transData.transform(ul_data_zoom)ur_disp_zoom = ax.transData.transform(ur_data_zoom)ul_fig_zoom = fig.transFigure.inverted().transform(ul_disp_zoom)ur_fig_zoom = fig.transFigure.inverted().transform(ur_disp_zoom)# --- Draw lines from rectangle to inset corners ---line_left = mlines.Line2D([ll_fig[0], ul_fig_zoom[0]], [ll_fig[1], ul_fig_zoom[1]],                          transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)line_right = mlines.Line2D([lr_fig[0], ur_fig_zoom[0]], [lr_fig[1], ur_fig_zoom[1]],                           transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)ax.grid(True, axis='y', which='major', linestyle=':', linewidth=0.5, color='gray')fig.lines.extend([line_left, line_right])plt.savefig(    f"{fig_path}/Boxplot_Comitted_Mass_Loss_marzeion_noIPSL.png")plt.show()           #%% Table# Specify Pathso_folder_data = (    "/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output Files/02. OGGM/02. Volume Area simulations A+1km2")o_file_data = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions.csv"unit="km3"factors = [10**-9]if unit=="Gt":    rho=0.85 #Gt/km³    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_gt.csv"else:    rho=1    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute.csv"# Load datasetds = pd.read_csv(o_file_data)ds_subregions_initial = ds[ds.time == 1985.0][[    "subregion",    "volume_irr",]].reset_index(drop=True)total_row = pd.DataFrame({    'subregion': ['total'],    'volume_irr': [ds_subregions_initial['volume_irr'].sum()]})ds_subregions_initial = pd.concat([ds_subregions_initial, total_row], ignore_index=True)ds_subregions = ds[ds.time == 2014.0][[    "subregion",    "volume_loss_percentage_irr",    "volume_loss_percentage_noirr",    "volume_irr",    "volume_noirr",]]# Process total volumesds_total = ds.groupby("time").sum()[[    "volume_irr",    "volume_noirr",]].round(2)# Calculate total rowdf_total = pd.DataFrame({    "subregion": ["total"],    "volume_loss_percentage_irr": ((ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_loss_percentage_noirr": ((ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_irr": (ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"])*rho,    "volume_noirr": (ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"])*rho})# Combine subregions with totalds_all_losses = pd.concat([ds_subregions, df_total], ignore_index=True)# Calculate deltasds_all_losses["delta_irr"] = ds_all_losses["volume_loss_percentage_irr"] - ds_all_losses["volume_loss_percentage_noirr"]ds_all_losses["delta_irr_abs"] = ds_all_losses["volume_irr"] - ds_all_losses["volume_noirr"]# Load RGI datadf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")df["subregion"] = df["rgi_subregion"].str.replace(    "-", ".").str.strip().str.lower()# Aggregate RGI dataareas = df.groupby("subregion")["rgi_area_km2"].sum().reset_index(name="area")volume = df.groupby("subregion")[    "rgi_volume_km3"].sum().reset_index(name="volume")nr_glaciers = df.groupby("subregion")[    "rgi_id"].count().reset_index(name="nr_glaciers")subregion_name = df.groupby("subregion")["full_name"].first().reset_index(name="name")subregion_map = {    "13.02": "Pamir",    "13.04": "East Tien Shan",    "13.06": "East Kun Lun"}subregion_name["name"] = subregion_name["subregion"].map(subregion_map).fillna(subregion_name["name"]) # Add total rowsareas = pd.concat([areas, pd.DataFrame(    {"subregion": ["total"], "area": [areas["area"].sum()]})], ignore_index=True)volume = pd.concat([volume, pd.DataFrame({"subregion": ["total"], "volume": [                   volume["volume"].sum()]})], ignore_index=True)nr_glaciers = pd.concat([nr_glaciers, pd.DataFrame({"subregion": [                        "total"], "nr_glaciers": [nr_glaciers["nr_glaciers"].sum()]})], ignore_index=True)names = pd.concat([subregion_name, pd.DataFrame({"subregion": [                        "total"], "name": ["High Mountain Asia"]})], ignore_index=True)areas['area']=areas['area'].round().astype(int)volume['volume']=volume['volume'].round().astype(int)# Merge RGI info into ds_all_lossesds_all_losses['subregion'] = areas["subregion"]ds_all_losses = ds_all_losses.merge(areas, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(volume, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(nr_glaciers, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(names, on="subregion", how="left")# ds_all_losses['error_margin_noi_rel'] = ds_all_losses['error_margin_noi_abs'] / \    # ds_all_losses['volume_noirr']*100# ds_all_losses['error_margin_cf_rel'] = ds_all_losses['error_margin_cf_abs'] / \    # ds_all_losses['volume_cf']*100# Final selection of columns and save to CSVds_all_losses = ds_all_losses[[    "subregion", "name", "nr_glaciers", "area", f"volume",    "volume_loss_percentage_irr", "volume_irr",    "volume_loss_percentage_noirr", "volume_noirr",  "delta_irr", "delta_irr_abs"#"error_margin_noi_rel", "error_margin_noi_abs",    # "volume_loss_percentage_cf", "volume_cf", "error_margin_cf_rel", "error_margin_cf_abs", "delta_cf",]].round(1)#Add comitted mass lossesdf_means_com = pd.read_csv(f"{wd_path}masters/mean_deltaV_Comitted.csv").reset_index(drop=True).drop('Unnamed: 0', axis=1)df_means_com['subregion'] = df_means_com['subregion'].replace('High Mountain Asia', 'total')df_means_com['delta_com'] =df_means_com['V_2264_irr_delta'] - df_means_com['V_2264_noirr_delta']df_all_losses = ds_all_losses.merge(df_means_com.round(1), on="subregion", how="left")df_means_future = xr.open_dataset(f"{wd_path}masters/master_volume_subregion_future_noi_bias_incl_total.nc")df_means_future = df_means_future.sel(time=2074).sel(sample_id='3-member-avg')for ssp in ['126','370']:    df_future=df_means_future.sel(ssp=ssp).subregion.values    formatted_codes = [code.replace('-', '.') for code in df_future]    df_future_vhist=df_means_future.sel(ssp=ssp).sel(exp="IRR").volume.values*factors    df_future_vhist_noirr=df_means_future.sel(ssp=ssp).sel(exp="NOI").volume.values*factors    df = pd.DataFrame({    'subregion': formatted_codes,    f'volume_irr_ssp{ssp}': df_future_vhist,    f'volume_noi_ssp{ssp}': df_future_vhist_noirr #- ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    })        df[f'volume_noi_ssp{ssp}'] = round(((df[f'volume_noi_ssp{ssp}'] - ds_subregions_initial.volume_irr )/ ds_subregions_initial.volume_irr) * 100,1)    df[f'volume_irr_ssp{ssp}'] = round(((df[f'volume_irr_ssp{ssp}'] - ds_subregions_initial.volume_irr )/ ds_subregions_initial.volume_irr) * 100,1)    df[f'delta_irr_ssp{ssp}'] = round(df[f'volume_irr_ssp{ssp}'] -df[f'volume_noi_ssp{ssp}'],1)    df_all_losses = df_all_losses.merge(df, on="subregion", how="left")    df_all_losses.to_csv(o_file_data_processed)   o_file_data_processed_2 = f"{fig_path}/Table_1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute.csv"df_all_losses.to_csv(o_file_data_processed_2)       #%% Table SMALL# Specify Pathso_folder_data = (    "/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output Files/02. OGGM/02. Volume Area simulations A+1km2")o_file_data = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions.csv"unit="km3"factors = [10**-9]if unit=="Gt":    rho=0.85 #Gt/km³    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_gt_small.csv"else:    rho=1    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute_small.csv"# Load datasetds = pd.read_csv(o_file_data)ds_subregions_initial = ds[ds.time == 1985.0][[    "subregion",    "volume_irr",]].reset_index(drop=True)total_row = pd.DataFrame({    'subregion': ['total'],    'volume_irr': [ds_subregions_initial['volume_irr'].sum()]})ds_subregions_initial = pd.concat([ds_subregions_initial, total_row], ignore_index=True)ds_subregions = ds[ds.time == 2014.0][[    "subregion",    "volume_loss_percentage_irr",    "volume_loss_percentage_noirr",    "volume_irr",    "volume_noirr",]]# Process total volumesds_total = ds.groupby("time").sum()[[    "volume_irr",    "volume_noirr",]].round(2)# Calculate total rowdf_total = pd.DataFrame({    "subregion": ["total"],    "volume_loss_percentage_irr": ((ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_loss_percentage_noirr": ((ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_irr": (ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"])*rho,    "volume_noirr": (ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"])*rho})# Combine subregions with totalds_all_losses = pd.concat([ds_subregions, df_total], ignore_index=True)# Calculate deltasds_all_losses["delta_irr"] = ds_all_losses["volume_loss_percentage_irr"] - ds_all_losses["volume_loss_percentage_noirr"]ds_all_losses["delta_irr_abs"] = ds_all_losses["volume_irr"] - ds_all_losses["volume_noirr"]# Load RGI datadf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")df["subregion"] = df["rgi_subregion"].str.replace(    "-", ".").str.strip().str.lower()# Aggregate RGI dataareas = df.groupby("subregion")["rgi_area_km2"].sum().reset_index(name="area")volume = df.groupby("subregion")[    "rgi_volume_km3"].sum().reset_index(name="volume")nr_glaciers = df.groupby("subregion")[    "rgi_id"].count().reset_index(name="nr_glaciers")subregion_name = df.groupby("subregion")["full_name"].first().reset_index(name="name")subregion_map = {    "13.02": "Pamir",    "13.04": "East Tien Shan",    "13.06": "East Kun Lun"}subregion_name["name"] = subregion_name["subregion"].map(subregion_map).fillna(subregion_name["name"]) # Add total rowsareas = pd.concat([areas, pd.DataFrame(    {"subregion": ["total"], "area": [areas["area"].sum()]})], ignore_index=True)volume = pd.concat([volume, pd.DataFrame({"subregion": ["total"], "volume": [                   volume["volume"].sum()]})], ignore_index=True)nr_glaciers = pd.concat([nr_glaciers, pd.DataFrame({"subregion": [                        "total"], "nr_glaciers": [nr_glaciers["nr_glaciers"].sum()]})], ignore_index=True)names = pd.concat([subregion_name, pd.DataFrame({"subregion": [                        "total"], "name": ["High Mountain Asia"]})], ignore_index=True)areas['area']=areas['area'].round().astype(int)volume['volume']=volume['volume'].round().astype(int)# Merge RGI info into ds_all_lossesds_all_losses['subregion'] = areas["subregion"]ds_all_losses = ds_all_losses.merge(areas, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(volume, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(nr_glaciers, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(names, on="subregion", how="left")# ds_all_losses['error_margin_noi_rel'] = ds_all_losses['error_margin_noi_abs'] / \    # ds_all_losses['volume_noirr']*100# ds_all_losses['error_margin_cf_rel'] = ds_all_losses['error_margin_cf_abs'] / \    # ds_all_losses['volume_cf']*100# Final selection of columns and save to CSVds_all_losses = ds_all_losses[[    "subregion", "name", "nr_glaciers", "area", f"volume",    "volume_loss_percentage_irr", "volume_irr",    "volume_loss_percentage_noirr", "volume_noirr",  "delta_irr", "delta_irr_abs"#"error_margin_noi_rel", "error_margin_noi_abs",    # "volume_loss_percentage_cf", "volume_cf", "error_margin_cf_rel", "error_margin_cf_abs", "delta_cf",]].round(1)#Add comitted mass lossesds_all_losses.to_csv(o_file_data_processed)   o_file_data_processed_2 = f"{fig_path}/Table_1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute_small.csv"ds_all_losses.to_csv(o_file_data_processed_2)               #%% Cell 6: Temperature plot - 2d legenddef create_precip_temp_colormap(    precip_range=(-20, 20), temp_range=(-1.5, 1.5), grid_size=256):    color_dry_warm = np.array([1.0, 0.4, 0.0])    color_dry_cold = np.array([0.6, 0.0, 0.6])    color_wet_warm = np.array([0.6, 1.0, 0.2])    color_wet_cold = np.array([0.0, 0.4, 1.0])    color_center  = np.array([1.0, 1.0, 1.0])    custom_colormap = np.zeros((grid_size, grid_size, 3))    for i in range(grid_size):        for j in range(grid_size):            x = -1 + 2 * j / (grid_size - 1)            y = -1 + 2 * i / (grid_size - 1)            wx = (x + 1) / 2            wy = (y + 1) / 2            top = (1 - wx) * color_dry_warm + wx * color_wet_warm            bottom = (1 - wx) * color_dry_cold + wx * color_wet_cold            color = (1 - wy) * bottom + wy * top            r = np.sqrt(x**2 + y**2)            fade = np.clip(1 - r, 0, 1)            color = (1 - fade) * color + fade * color_center            custom_colormap[i, j] = color    def colormap_callable(delta_temp, delta_precip):        rows = ((delta_temp - temp_range[0]) / (temp_range[1] - temp_range[0]) * (grid_size - 1)).astype(int)        cols = ((delta_precip - precip_range[0]) / (precip_range[1] - precip_range[0]) * (grid_size - 1)).astype(int)        rows = np.clip(rows, 0, grid_size - 1)        cols = np.clip(cols, 0, grid_size - 1)        return custom_colormap[rows, cols]    return custom_colormap, colormap_callabledef plot_colormap_legend(ax,colormap_grid, precip_range=(-20, 20), temp_range=(-1.5, 1.5)):    """    Plot the 2D color legend for the custom ΔPrecip–ΔTemp colormap.    Parameters:        colormap_grid: The [grid_size x grid_size x 3] RGB array from create_precip_temp_colormap()        precip_range: Tuple of (min, max) precipitation anomaly        temp_range: Tuple of (min, max) temperature anomaly    """    ax.imshow(colormap_grid, origin='lower',              extent=[precip_range[0], precip_range[1], temp_range[0], temp_range[1]],              aspect='auto')    ax.set_xlabel("ΔPrecipitation (%)", fontsize=12)    ax.set_ylabel("ΔTemperature (°C)", fontsize=12)    ax.tick_params(labelsize=12)    ax.set_title("Legend", fontsize=12)    ax.grid(True)    plt.tight_layout()    # plt.show()def plot_subplots(index, subplots, annotation, diff, timestamps, axes, shp, custom_cmap, timeframe, scale, title, vmin=None, vmax=None):    for time_idx, timestamp_name in enumerate(timestamps):        # Determine subplot location based on timeframe        if timeframe == 'monthly':            row = time_idx // 4  # Calculate row index            col = time_idx % 4            ax = axes[row, col]        elif timeframe == 'seasonal':            row = time_idx // 2            col = time_idx % 2            ax = axes[row, col]        elif timeframe == 'annual':            row, col = 0, 0            ax = axes        # Select time dimension based on scale and timeframe        if scale == "Local":            time_dim_name = list(diff.dims)[0]        elif scale == "Global" and timeframe != 'annual':            time_dim_name = list(diff.dims)[2]        # Select relevant data slice        if timeframe == 'annual':            diff_sel = diff        else:            diff_sel = diff.isel({time_dim_name: time_idx})        # Convert Dataset to DataArray if necessary        if isinstance(diff_sel, xr.Dataset):            diff_sel = diff_sel[list(diff_sel.data_vars.keys())[0]]                    # --- NEW: If colormap is callable, apply it manually ---        if callable(custom_cmap):            # Assuming `diff_sel` contains both temp and precip deltas            delta_temp = diff_sel['tas'] if 'tas' in diff_sel else diff_sel            delta_precip = diff_sel['pr'] if 'pr' in diff_sel else diff_sel            delta_temp_np = delta_temp.values            delta_precip_np = delta_precip.values            rgb_img = custom_cmap(delta_temp_np, delta_precip_np)            im = ax.imshow(rgb_img,                           extent=[diff_sel.lon.min(), diff_sel.lon.max(),                                   diff_sel.lat.min(), diff_sel.lat.max()],                           origin='lower',                           transform=ccrs.PlateCarree())        else:            im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',                                      transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False)        # # Plot data and the Karakoram outline        # im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',        #                           transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False)        if scale=="Local":            shp.plot(ax=ax, edgecolor='black', linewidth=1, facecolor='none')        ax.coastlines(resolution='10m')        # Set gridlines and labels        gl = ax.gridlines(draw_labels=True)        gl.top_labels = False        gl.right_labels = False            gl.ylabel_style = {'size': 12}        gl.ylocator = plt.MaxNLocator(nbins=3)            gl.xlabel_style = {'size': 12}        gl.xlocator = plt.MaxNLocator(nbins=3)                gl.xformatter = LONGITUDE_FORMATTER        gl.yformatter = LATITUDE_FORMATTER        """4 Include labels for the cbar and for the y and x axis"""        ax.set_title(title)    return im# def plot_P_T_perturbations_avg(scale, var, timeframe, mode, diftype, plotsave):""" Part 0 - Set plotting parameters"""# y0 = 1985  # if running from 1901 to 1985, than indicate extra id of counterfactual to access the data# ye = 2014# if running from 1901 to 1985, than indicate extra id of counterfactual to access the datay0s = [1985]#, 1901]yes = [2014]#, 1985]extra_ids = [""]#, "_counterfactual"]ptb_types = ["Irr"]#, "NoForcing"]scale = "Global"subplots = "off"all_vars_p = []all_vars_t = []all_vars_local_p = []all_vars_local_t = []# "Temperature"]:  # ,"Temperature"]:for var in ["Temperature", "Precipitation"]:    for timeframe in ["annual"]:  # :, "seasonal", "monthly"]:        for mode in ['dif']:  # , 'std']:            for y, y0 in enumerate(y0s):                ye = yes[y]                extra_id = extra_ids[y]                ptbtype = ptb_types[y]                if var == "Precipitation" and mode == 'dif':                    diftypes = ['rel']                else:                    diftypes = ['abs']                for dif in diftypes:                    print(var, timeframe, dif)                    diftype = dif                    timestamps = "YEAR"                    time_averaging = 'time.year'                    time_type = 'year'                    col_wrap = 1                    # Provide cbar ranges and colors for plots for different variables, modes (dif/std) and difference types (abs/rel)                    if var == "Precipitation":                        variable_name = "pr"                        var_suffix = "PR"                        if mode == 'dif' and diftype == 'rel':                            mode_suff = 'total'                            vmin = -20                            vmax = 20                            zero_scaled = (abs(vmin)/(abs(vmin)+abs(vmax)))                            colors = [(0, 'xkcd:mocha'), (zero_scaled,                                                          'xkcd:white'), (1, 'xkcd:aquamarine')]                            # custom_cmap = LinearSegmentedColormap.from_list(                            #     'custom_cmap', colors)                            unit = '%'                    elif var == "Temperature":                        variable_name = "tas"                        var_suffix = "TEMP"                                            mode_suff = 'total'                        vmin = -1.5                        vmax = 1.5                        zero_scaled = (abs(vmin)/(abs(vmin)+abs(vmax)))                        colors = [(0, 'cornflowerblue'), (zero_scaled,                                                          'xkcd:white'), (1, 'xkcd:tomato')]                        custom_cmap = LinearSegmentedColormap.from_list(                            'custom_cmap', colors)                        unit = '°C'                    members = [1, 3, 4, 6, 4]                    # members = [1, 1, 1, 1]                    all_diff = []  # create a dataset to add all member differences                    all_model_diffs = []                    models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]                    lonmin, lonmax, latmin, latmax = [60,109,22,52]                    for (m, model) in enumerate(models):                        model_diff = []                        for member in range(members[m]):                            # only open data for non model averages (except for IPSL-CM6 as only one member)                            if model == "IPSL-CM6" or member != 0:                                # Part 1: Delete                                diff_folder_in = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/01. Climate data/03. Regridded Perturbations/{var}/{timeframe}/{model}/{member}"                                ifile_diff = f"{diff_folder_in}/REGRID.{model}.{var_suffix}.DIF.00{member}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.nc"                                diff = xr.open_dataset(ifile_diff)                                 if scale == "Local":  # scale the data to the local scale                                    diff = diff.where((diff.lon >= 60) & (diff.lon <= 109) & (                                        diff.lat >= 22) & (diff.lat <= 52), drop=True)                                                                                                        # loose all the filtered data (nan)                                diff_clean = diff.dropna(dim="lon", how="all")                                # include the values in the list for caluclating the avg difference by model                                model_diff.append(diff_clean)                                # include the values in the list for caluclating the avg difference over all models                                all_diff.append(diff_clean)                        all_model_diff = xr.concat(                            model_diff, dim="models").mean(dim="models")  # concatenate all models into a list averaged by model                        all_model_diffs.append(all_model_diff)                    all_model_diffs_avg = xr.concat(                        all_model_diffs, dim="models")  # concatenate all models                    all_diffs_avg = xr.concat(all_diff, dim="models").mean(                        dim="models")  # concatenate all members and calculate the mean over all the models                    all_diffs_avg_local = all_diffs_avg.where((all_diffs_avg.lon >= 60) & (all_diffs_avg.lon <= 109) & (                        all_diffs_avg.lat >= 22) & (all_diffs_avg.lat <= 52), drop=True)                    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"                    os.makedirs(o_folder, exist_ok=True)                    o_file=f"{o_folder}/{scale}_{var}_processed.nc"                    # all_diffs_avg.to_netcdf(o_file)                                        # Unpack axes                    if var == "Precipitation":                        all_vars_p = all_diffs_avg                        all_vars_local_p = all_diffs_avg_local                    elif var =="Temperature":                        all_vars_t = all_diffs_avg                        all_vars_local_t = all_diffs_avg_local                                    all_vars_2d = xr.merge([all_vars_p, all_vars_t])all_vars_local_2d = xr.merge([all_vars_local_p, all_vars_local_t])# 2D blended colormapcolormap_grid, custom_cmap = create_precip_temp_colormap(    precip_range=(-20, 20), temp_range=(-1.5, 1.5))""" Part 2 - Shapefile outline for Karakoram Area to be included"""# path to  shapefileshapefile_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/03. Shapefile/Karakoram/Pan-Tibetan Highlands/Pan-Tibetan Highlands (Liu et al._2022)/Shapefile/Pan-Tibetan Highlands (Liu et al._2022)_P.shp'shp = gpd.read_file(shapefile_path)target_crs = 'EPSG:4326'shp = shp.to_crs(target_crs)indices = ["A", "B"]#, "E", "F"]# Create the mosaic plotlayout = """AB"""figsize = (18.15,5) #based on aspect ratio of global and zoomedfig, axes = plt.subplot_mosaic(layout, subplot_kw={'projection': ccrs.PlateCarree()},                               figsize=figsize,                               gridspec_kw={'wspace': 0.05, 'width_ratios': [2.0, 1.63]})#,'height_ratios': [1, 1]})axes['A'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['B'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())# plot the irrmip differenceim = plot_subplots(indices[0], subplots, (sum(members)-len(models)+1),                   all_vars_2d, timestamps, axes[indices[0]], shp, custom_cmap, timeframe, "Global", "Global", vmin=vmin, vmax=vmax)im = plot_subplots(indices[1], subplots, (sum(members)-len(models)+1),                   all_vars_local_2d, timestamps, axes[indices[1]], shp, custom_cmap, timeframe, "Local", "High Mountain Asia", vmin=vmin, vmax=vmax)bbox = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['A'].transData,    xyB=(lonmin, latmax), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['A'].transData,    xyB=(lonmin, latmin), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)# Add to the global axes (they own the lines)axes['A'].add_artist(line_top)axes['A'].add_artist(line_bottom)axes['A'].add_patch(bbox)sign_diff = xr.concat([np.sign(diff[variable_name])                      for diff in all_model_diffs], dim="models")agreement_on_sign = (    abs(sign_diff.mean(dim="models")) > 0.8)# Step 3: Combine the conditions to create the final maskwithin_threshold = agreement_on_sign.sel(lon=slice(lonmin, lonmax), lat=slice(latmax, latmin))# Step 4: Convert the mask to 2D by removing the singleton 'variable' dimension if neededwithin_threshold_2d = within_threshold.astype(    int).squeeze()# Step 6: Overlay the mask with dots in areas where agreement criteria are metaxes[indices[1]].contourf(    all_diffs_avg_local.lon, all_diffs_avg_local.lat, within_threshold_2d,    levels=[0.5, 1.5], colors='none', hatches=['////'], transform=ccrs.PlateCarree())""" 3C Add color bar for entire plot"""cbar_ax = fig.add_axes([0.94, 0.1, 0.1, 0.78])plot_colormap_legend(cbar_ax, colormap_grid, precip_range=(-20, 20), temp_range=(-1.5, 1.5))# add cbar in the figure, for overall figure, not subplots# Define the position of the colorbar# # cbar = fig.colorbar(im, cax=cbar_ax, extend='both')# # Increase distance between colorbar label and colorbar# cbar.ax.yaxis.labelpad = 20# if mode == 'dif':#     # cbar.set_label(f'$\Delta_{{ptb_type}}_{,{var}}$ [{unit}]', size='15')#     cbar.set_label(#         rf'$\Delta_{{{ptbtype}, {var}}}$ [{unit}]', size=15)#     if mode == 'std':#         cbar.set_label(#             f'{var} - model member std [{unit}]', size='15')# cbar.ax.tick_params(labelsize=12)# # adjust subplot spacing to be smaller# plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1,#                     top=0.9, wspace=0.05, hspace=0.05)hedging_patch = mpatches.Patch(    label='< 80% of model members agree on sign of change', hatch='////', edgecolor='black', facecolor='none')# Add the custom legend to the plotfig.legend(handles=[                        hedging_patch], loc='lower center', bbox_to_anchor=(0.5, -0.05), fontsize=12)# if plotsave == 'save':o_folder_diff = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/01. Climate data/02. Perturbations/"os.makedirs(f"{o_folder_diff}/", exist_ok=True)o_file_name = f"{o_folder_diff}/Mosaic.{var}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"# plt.savefig(o_file_name, bbox_inches='tight')plt.show()            #%% Cell6b: TP plot - 1d legendplt.rcParams.update({'font.size': 16})def plot_subplots(index, subplots, annotation, diff, timestamps, axes, shp, custom_cmap, timeframe, scale, title, vmin=None, vmax=None):    for time_idx, timestamp_name in enumerate(timestamps):        # Determine subplot location based on timeframe        if timeframe == 'monthly':            row = time_idx // 4  # Calculate row index            col = time_idx % 4            ax = axes[row, col]        elif timeframe == 'seasonal':            row = time_idx // 2            col = time_idx % 2            ax = axes[row, col]        elif timeframe == 'annual':            row, col = 0, 0            ax = axes        # Select time dimension based on scale and timeframe        if scale == "Local":            time_dim_name = list(diff.dims)[0]        elif scale == "Global" and timeframe != 'annual':            time_dim_name = list(diff.dims)[2]        # Select relevant data slice        if timeframe == 'annual':            diff_sel = diff        else:            diff_sel = diff.isel({time_dim_name: time_idx})        # Convert Dataset to DataArray if necessary        if isinstance(diff_sel, xr.Dataset):            diff_sel = diff_sel[list(diff_sel.data_vars.keys())[0]]                        # Plot data and the Karakoram outline        im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',                                  transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False)        if scale=="Local":            shp.plot(ax=ax, edgecolor='black', linewidth=1, facecolor='none')        ax.coastlines(resolution='10m')        # Set gridlines and labels        gl = ax.gridlines(draw_labels=True)        gl.top_labels = False        gl.right_labels = False        if index in ["A","C"]:            gl.left_labels = False        if index in ["A","C","B"]:            gl.bottom_labels = False            gl.ylabel_style = {'size': 14}        gl.ylocator = plt.MaxNLocator(nbins=3)            gl.xlabel_style = {'size': 14}        gl.xlocator = plt.MaxNLocator(nbins=3)                gl.xformatter = LONGITUDE_FORMATTER        gl.yformatter = LATITUDE_FORMATTER        """4 Include labels for the cbar and for the y and x axis"""        ax.set_title(title, fontsize=18)    return im# def plot_P_T_perturbations_avg(scale, var, timeframe, mode, diftype, plotsave):""" Part 0 - Set plotting parameters"""# y0 = 1985  # if running from 1901 to 1985, than indicate extra id of counterfactual to access the data# ye = 2014# if running from 1901 to 1985, than indicate extra id of counterfactual to access the datay0s = [1985]#, 1901]yes = [2014]#, 1985]extra_ids = [""]#, "_counterfactual"]ptb_types = ["Irr"]#, "NoForcing"]scale = "Global"subplots = "off"all_vars_p = []all_vars_t = []all_vars_local_p = []all_vars_local_t = []all_model_diffs_avg_p = []all_model_diffs_avg_t = []all_member_diff_p = []all_member_diff_t = []# "Temperature"]:  # ,"Temperature"]:for var in ["Temperature", "Precipitation"]:    for timeframe in ["annual"]:  # :, "seasonal", "monthly"]:        for mode in ['dif']:  # , 'std']:            for y, y0 in enumerate(y0s):                ye = yes[y]                extra_id = extra_ids[y]                ptbtype = ptb_types[y]                if var == "Precipitation" and mode == 'dif':                    diftypes = ['rel']                else:                    diftypes = ['abs']                for dif in diftypes:                    print(var, timeframe, dif)                    diftype = dif                    timestamps = "YEAR"                    time_averaging = 'time.year'                    time_type = 'year'                    col_wrap = 1                    # Provide cbar ranges and colors for plots for different variables, modes (dif/std) and difference types (abs/rel)                    if var == "Precipitation":                        variable_name = "pr"                        var_suffix = "PR"                        if mode == 'dif' and diftype == 'rel':                            mode_suff = 'total'                            unit = '%'                    elif var == "Temperature":                        variable_name = "tas"                        var_suffix = "TEMP"                        mode_suff = 'total'                                                unit = '°C'                    vmin_t = -1#-1.5                    vmax_t = 1#1.5                    zero_scaled_t = (abs(vmin_t)/(abs(vmin_t)+abs(vmax_t)))                    # colors = [(0, 'xkcd:mocha'), (zero_scaled_t,                    #                                   'xkcd:white'), (1, 'xkcd:aquamarine')]                    # custom_cmap_p =  LinearSegmentedColormap.from_list(                    #     'custom_cmap', colors)                    brbg11 = plt.get_cmap('BrBG', 11)                    colors = brbg11([i for i in range(brbg11.N)[2:-2]])  # Get list of 11 RGBA colors                    custom_cmap_p = LinearSegmentedColormap.from_list('BrBG11', colors)                     # If you want a ListedColormap for use in pcolormesh, imshow, etc.                    # custom_cmap_p = ListedColormap(brbg11.colors)                                        vmin_p = -20                    vmax_p = 20                    zero_scaled_p = (abs(vmin_p)/(abs(vmin_p)+abs(vmax_p)))                                               colors = [(0, 'cornflowerblue'), (zero_scaled_p,                                                      'xkcd:white'), (1, 'xkcd:tomato')]                    custom_cmap_t = LinearSegmentedColormap.from_list(                         'custom_cmap', colors)                    members = [1, 3, 4, 6, 4]                    # members = [1, 1, 1, 1]                    all_diff = []  # create a dataset to add all member differences                    all_model_diffs = []                    models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]                    lonmin, lonmax, latmin, latmax = [60,109,22,52]                    all_member_diffs = []                    for (m, model) in enumerate(models):                        model_diff = []                        for member in range(members[m]):                            member_diff=[]                            # only open data for non model averages (except for IPSL-CM6 as only one member)                            if model == "IPSL-CM6" or member != 0:                                # Part 1: Delete                                diff_folder_in = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/01. Climate data/03. Regridded Perturbations/{var}/{timeframe}/{model}/{member}"                                ifile_diff = f"{diff_folder_in}/REGRID.{model}.{var_suffix}.DIF.00{member}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.nc"                                diff =  xr.open_dataset(ifile_diff)                                if scale == "Local":  # scale the data to the local scale                                    diff = diff.where((diff.lon >= 60) & (diff.lon <= 109) & (                                        diff.lat >= 22) & (diff.lat <= 52), drop=True)                                    print(diff.lon)                                                                                                        # loose all the filtered data (nan)                                diff_clean = diff.dropna(dim="lon", how="all")                                # include the values in the list for caluclating the avg difference by model                                model_diff.append(diff_clean)                                member_diff.append(diff_clean)                                all_diff.append(diff_clean)                                all_member_diff = xr.concat(                                    member_diff, dim="member")                                 all_member_diffs.append(all_member_diff)                                                                # include the values in the list for caluclating the avg difference over all models                                                       all_model_diff = xr.concat(                            model_diff, dim="models").mean(dim="models")  # concatenate all models into a list averaged by model                        all_model_diffs.append(all_model_diff)                     # concatenate all models into a list averaged by model                                        all_model_diffs_avg = xr.concat(                        all_model_diffs, dim="models")  # concatenate all models                    all_diffs_avg = xr.concat(all_diff, dim="models").mean(                        dim="models")  # concatenate all members and calculate the mean over all the models                    all_diffs_avg_local = all_diffs_avg.where((all_diffs_avg.lon >= 60) & (all_diffs_avg.lon <= 109) & (                        all_diffs_avg.lat >= 22) & (all_diffs_avg.lat <= 52), drop=True)                    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"                    os.makedirs(o_folder, exist_ok=True)                    o_file=f"{o_folder}/{scale}_{var}_processed.nc"                    # all_diffs_avg.to_netcdf(o_file)                                        # Unpack axes                    if var == "Precipitation":                        all_vars_p = all_diffs_avg                        all_vars_local_p = all_diffs_avg_local                        all_model_diffs_avg_p = all_model_diffs_avg                        all_member_diff_p = all_member_diffs                    elif var =="Temperature":                        all_vars_t = all_diffs_avg                        all_vars_local_t = all_diffs_avg_local                        all_model_diffs_avg_t = all_model_diffs_avg                        all_member_diff_t = all_member_diffs                                    """ Part 2 - Shapefile outline for Karakoram Area to be included"""# path to  shapefileshapefile_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/03. Shapefile/Karakoram/Pan-Tibetan Highlands/Pan-Tibetan Highlands (Liu et al._2022)/Shapefile/Pan-Tibetan Highlands (Liu et al._2022)_P.shp'shp = gpd.read_file(shapefile_path)target_crs = 'EPSG:4326'shp = shp.to_crs(target_crs)indices = ["A", "B","C","D"]#, "E", "F"]# Create the mosaic plotlayout = """ABCD"""figsize = (18.15,10) #based on aspect ratio of global and zoomedfig, axes = plt.subplot_mosaic(layout, subplot_kw={'projection': ccrs.PlateCarree()},                               figsize=figsize,                               gridspec_kw={'wspace': 0.05, 'width_ratios': [2.0, 1.63], 'hspace': 0.05})#,'height_ratios': [1, 1]})axes['A'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['C'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['B'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())axes['D'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())# plot the irrmip differenceim_t_g = plot_subplots(indices[0], subplots, (sum(members)-len(models)+1),                   all_vars_t, timestamps, axes[indices[0]], shp, custom_cmap_t, timeframe, "Global", "Global", vmin=vmin_t, vmax=vmax_t)im_t_l = plot_subplots(indices[1], subplots, (sum(members)-len(models)+1),                   all_vars_local_t, timestamps, axes[indices[1]], shp, custom_cmap_t, timeframe, "Local", "High Mountain Asia", vmin=vmin_t, vmax=vmax_t)im_p_g = plot_subplots(indices[2], subplots, (sum(members)-len(models)+1),                   all_vars_p, timestamps, axes[indices[2]], shp, custom_cmap_p, timeframe, "Global", "", vmin=vmin_p, vmax=vmax_p)im_p_l = plot_subplots(indices[3], subplots, (sum(members)-len(models)+1),                   all_vars_local_p, timestamps, axes[indices[3]], shp, custom_cmap_p, timeframe, "Local", "", vmin=vmin_p, vmax=vmax_p)bbox = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['A'].transData,    xyB=(lonmin, latmax), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['A'].transData,    xyB=(lonmin, latmin), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)# Add to the global axes (they own the lines)axes['A'].add_artist(line_top)axes['A'].add_artist(line_bottom)axes['A'].add_patch(bbox)bbox2 = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['C'].transData,    xyB=(lonmin, latmax), coordsB=axes['D'].transData,    axesA=axes['C'], axesB=axes['D'],    color='black', linestyle='-', linewidth=1)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['C'].transData,    xyB=(lonmin, latmin), coordsB=axes['D'].transData,    axesA=axes['C'], axesB=axes['D'],    color='black', linestyle='-', linewidth=1)# Add to the global axes (they own the lines)axes['C'].add_artist(line_top)axes['C'].add_artist(line_bottom)axes['C'].add_patch(bbox2)        for i, item in enumerate(all_member_diff_p):    print(f"Item {i}: type = {type(item)}")    # signed_p = [np.sign(diff_p['pr']) for diff_p in all_member_diff_p]# signed_t = [np.sign(diff_t['tas']) for diff_t in all_member_diff_t]# # Concatenate along 'members' (existing dim)# sign_diff_p = xr.concat(signed_p, dim='members')# sign_diff_t = xr.concat(signed_t, dim='members')sign_diff_p = xr.concat(    [np.sign(ds['pr']) for ds in all_member_diff_p if isinstance(ds, xr.Dataset) and 'pr' in ds],    dim='members')sign_diff_t = xr.concat(    [np.sign(ds['tas']) for ds in all_member_diff_t if isinstance(ds, xr.Dataset) and 'tas' in ds],    dim='members')# 2. Subset the region of interest: South Asia (approx.)region_bounds = {    "lon_min": 60,    "lon_max": 109,    "lat_min": 22,    "lat_max": 52,}sign_diff_p = sign_diff_p.sel(    lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),    lat=slice(region_bounds["lat_max"], region_bounds["lat_min"]))sign_diff_t = sign_diff_t.sel(    lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),    lat=slice(region_bounds["lat_max"], region_bounds["lat_min"]))# 3. Compute agreement mask: where >80% of members agree on signwithin_threshold_p = (abs(sign_diff_p.mean(dim="members")) > 0.8)within_threshold_t = (abs(sign_diff_t.mean(dim="members")) > 0.8)# 4. Ensure masks are 2D, squeeze any singleton dimensionswithin_threshold_2d_p = within_threshold_p.astype(int).squeeze()within_threshold_2d_t = within_threshold_t.astype(int).squeeze()# 5. Prepare meshgrid for plottinglon2d, lat2d = np.meshgrid(all_diffs_avg_local.lon, all_diffs_avg_local.lat)# 6. Plotting with contourf and hatchingfor key, mask in zip(["B", "D"], [within_threshold_2d_t, within_threshold_2d_p]):    axes[key].contourf(        lon2d, lat2d, mask,        levels=[0.5, 1.5],        colors='none',        hatches=['////'],        transform=ccrs.PlateCarree()    )""" 3C Add color bar for entire plot"""# add cbar in the figure, for overall figure, not subplots# Define the position of the colorbar# # cbar_ax = fig.add_axes([0.92, 0.1, 0.02, 0.78])cbar_ax_t = fig.add_axes([0.9, 0.53, 0.015, 0.35])  # [left, bottom, width, height]cbar_ax_p = fig.add_axes([0.9, 0.1, 0.015, 0.35])cbar_t = fig.colorbar(im_t_l, cax=cbar_ax_t, extend='both')cbar_p = fig.colorbar(im_p_l, cax=cbar_ax_p, extend='both')# Increase distance between colorbar label and colorbarcbar_t.ax.yaxis.labelpad = 20cbar_t.set_label('∆ Temperature [°C]', size=16)cbar_t.ax.tick_params(labelsize=14)cbar_p.ax.yaxis.labelpad = 20cbar_p.set_label('∆ Precipitation [%]', size=16)cbar_p.ax.tick_params(labelsize=14)hedging_patch = mpatches.Patch(    label='> 80% of model members agree on sign of change', hatch='////', edgecolor='black', facecolor='none')# Add the custom legend to the plotfig.legend(handles=[hedging_patch], loc='lower center', bbox_to_anchor=(0.53, 0.02), fontsize=16)# if plotsave == 'save':# o_folder_diff = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/04. Figures/01. Climate data/02. Perturbations/"# os.makedirs(f"{o_folder_diff}/", exist_ok=True)# o_file_name = f"{o_folder_diff}/Mosaic.{var}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"o_file_name = f"{fig_folder}/Mosaic.∆PT.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"# o_file_name = f"{fig_folder}/01. EGU25/Mosaic.∆PT.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png" #without hedgingplt.savefig(o_file_name, bbox_inches='tight')plt.show()#%% Cell 6c: Read out values for irrigationfig,ax = plt.subplots(figsize=(9,10))scale ="Local"variables=["Precipitation","Temperature"]short_variables=['pr','tas']types=['min','max']for v,var in enumerate(variables):    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Coding/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"    # os.makedirs(o_folder, exist_ok=True)    o_file=f"{o_folder}/{scale}_{var}_processed.nc"    svar = short_variables[v]        with xr.open_dataset(o_file) as ds:        print(svar)        for m in range(2):            if m==0:                val_max = ds[svar].max().values            else:                val_max = ds[svar].min().values            max_loc = ds[svar].where(ds[svar] == val_max, drop=True)            # Extract coordinates of the maximum value            lon_max = max_loc.lon.values[0]            lat_max = max_loc.lat.values[0]            print(types[m],svar,':', val_max, 'lon:', lon_max,'lat:',  lat_max)   #%% Cell 7b: Presentation plot - Increased Irrigation Areafolder_data=("/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Coding/01. IRRMIP/03. Data/01. Input files/Irrigation_Data_Table.csv"                       )inputdata=pd.read_csv(folder_data, header=0, index_col="Year")# Define blue color shades and hatch patterns# colors = ['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087']cmap = cm.get_cmap('Blues', 6)  # Get 5 different shades of bluecolors = [cmap(i) for i in np.linspace(0.2, 1, 6)]  # Adjust brightnesshatch_patterns = ['//', '\\', '//', '\\', '//']plt.figure(figsize=(8,5))stacked_areas = plt.stackplot(inputdata.index,              inputdata["USA (Mha)"],              inputdata["Pakistan (Mha)"],              inputdata["China (Mha)"],              inputdata["India (Mha)"],              inputdata["Other countries (Mha)"],              labels=["USA", "Pakistan", "China", "India", "Other countries"],              alpha=0.7, colors=colors)  # Adjust transparency# Annotate country names on the plotcountry_labels = ["USA", "Pakistan", "China", "India", "Other \n countries","total"]mid_year = inputdata.index[len(inputdata) // 2]  # Select a middle year for annotationcumulative_values = np.zeros(len(inputdata))  # Initialize cumulative sum for stackingstacked_data = np.cumsum(inputdata.values, axis=1)for i, country in enumerate(inputdata.columns[:-1]):    mid_year = inputdata.index[len(inputdata) // 2]  # Middle year for annotation    mid_index = 10#len(inputdata) // 2  # Middle index of data    country_label=country_labels[i]        # Compute the middle value of the stacked region    if i == 0:        mid_value = inputdata.iloc[mid_index, i] / 2  # First country is at the bottom    else:        mid_value = (stacked_data[mid_index, i] + stacked_data[mid_index, i - 1]) / 2  # Middle of stacked area    # Annotate the country in the correct position    plt.text(1998, mid_value, country_label, fontsize=12,  ha='right', va='center')             # bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))# Labels and titleplt.xlabel("Year")# Set xticks to match the "Year" column (index-based ticks)plt.ylabel("Area Equipped for Irrigation (Mha)")plt.title("Global Irrigation Expansion Over Time")# plt.legend(loc="upper left")plt.savefig(f"{fig_folder}/00. Appendix/Timeline_Irrigation_Increase.png")# Show the plotplt.show()#%% Cell 8b Plot figure comparison reference datafig,axes=plt.subplots(2,1, figsize=(15,7.5), sharex=True) axes = axes.flatten()years_hugo=np.arange(2000,2020)axes[0].plot(years_hugo, np.ones(len(years_hugo))*weighted_B_hugo, color='crimson', ls=':',lw=3, label="Hugonnet et al., reg 13-15, A>5km$^{2}$, avg., ref period")draw_std_boxplot( axes[0],years_hugo, np.ones(len(years_hugo))*weighted_B_hugo, np.ones(len(years_hugo))*weighted_errB_hugo, box_width=1,  color='crimson', alpha=0.5, ls=':',label="Hugonnet et al., reg 13-15, A>5km$^{2}$, std, ref period")# ds_zemp_1314['Year'] = ds_zemp_1314.Year.astype(int) # draw_std_boxplot(axes, ds_zemp_1314.Year, ds_zemp_1314[' annual'], ds_zemp_1314[' sig_annual'], box_width=1, color='blue', ls='-',alpha=0.5, label="Zemp et al, reg 13-14")# draw_std_boxplot(axes, ds_zemp_1315.Year, ds_zemp_1315[' annual'], ds_zemp_1315[' sig_annual'], box_width=1, color='slateblue', ls='-',alpha=0.5, label="Zemp et al, reg 13-15")# axes.boxplot([ds_zemp_1314.Year.values, ds_zemp_1314[' annual'].values])# color=region_colors[14], marker= "^",ls=":", label="Zemp et al, reg 13-14")#,marker="o")# draw_std_boxplot(axes, ds_zemp_tot.Year, ds_zemp_tot[' annual'], ds_zemp_tot[' sig_annual'], box_width=1, color='blue', ls='-',alpha=0.5, label="Zemp et al, reg 13-14")# draw_std_boxplot(axes[0], ds_zemp_tot.Year, ds_zemp_tot[' annual_mya'], ds_zemp_tot[' sig_annual_mya'], box_width=1, color='blue', ls='-',alpha=0.3, label="Zemp et al. std, reg 13-15")# axes[0].plot( ds_zemp_tot[ds_zemp_tot['Year'].between(2000, 2019)].Year, ds_zemp_tot[ds_zemp_tot['Year'].between(2000, 2019)][' annual_mya_ref'], color='blue', ls=':',label="Zemp et al., reg 13-15 avg, ref period")# axes[1].plot(ds_zemp_tot.Year, ds_zemp_tot[' annual'], color='blue', ls='-',label="Zemp et al., reg 13-15, avg", lw=2) #ds_zemp_tot[' sig_annual'], box_width=1, draw_std_boxplot(axes[0], ds_zemp_tot_reg.Year, ds_zemp_tot[' annual_mya'], ds_zemp_tot_reg[' sig_annual_mya'], box_width=1, color='blue', ls='-',alpha=0.3, label="Zemp et al. std, reg 13-15")axes[0].plot( ds_zemp_tot_reg[ds_zemp_tot_reg['Year'].between(2000, 2019)].Year, ds_zemp_tot_reg[ds_zemp_tot['Year'].between(2000, 2019)][' annual_mya_ref'], color='blue', ls=':',lw=3, label="Zemp et al., reg 13-15 avg, ref period", zorder=100)axes[1].plot(ds_zemp_tot_reg.Year, ds_zemp_tot_reg['annual_mya'], color='blue', ls='-',label="Zemp et al., reg 13-15, avg", lw=2) #ds_zemp_tot[' sig_annual'], box_width=1, # axes.plot(ds_zemp_1315.Year, ds_zemp_1315[' annual'], color=region_colors[15], marker= "^",ls=":", label="Zemp et al, reg 13-15")#,marker="o")# draw_std_boxplot(axes, glambie_ds_13.start_dates, glambie_ds_13['combined_mwe'], glambie_ds_13['combined_mwe_errors'], box_width=1, color='pink', alpha=0.5, label="GlaMBIE, reg 13")# draw_std_boxplot(axes, glambie_ds_14.start_dates, glambie_ds_14['combined_mwe'], glambie_ds_14['combined_mwe_errors'], box_width=1, color='violet', alpha=0.5, label="GlaMBIE, reg 14")# draw_std_boxplot(axes, glambie_ds_15.start_dates, glambie_ds_15['combined_mwe'], glambie_ds_15['combined_mwe_errors'], box_width=1, color='orchid', alpha=0.5, label="GlaMBIE, reg 15")# draw_std_boxplot(axes, glambie_weighted.start_dates, glambie_weighted['combined_mwe'], glambie_weighted['combined_mwe_errors'], box_width=1, color='orange', alpha=0.5, label="GlaMBIE, reg. 13-15")draw_std_boxplot(axes[0], glambie_weighted.start_dates, glambie_weighted['combined_mwe_mya'], glambie_weighted['combined_mwe_errors_mya'], box_width=1, color='orange', ls='-',alpha=0.3, label="GlaMBIE, reg. 13-15")axes[0].plot(glambie_weighted[glambie_weighted['start_dates'].between(2000, 2019)].start_dates, glambie_weighted[glambie_weighted['start_dates'].between(2000, 2019)]['combined_mwe_mya_ref'], color='orange', ls = '--', lw=3, label="GlaMBIE, reg. 13-15, ref period")axes[1].plot(glambie_weighted.start_dates, glambie_weighted['combined_mwe'], color='orange', ls = '-', lw=2, label="GlaMBIE, reg. 13-15, ref period")# axes.plot(glambie_ds_13.start_dates,glambie_ds_13.combined_mwe, color=region_colors[13], ls=":",marker= ".",label="GlaMBIE, reg 13")# axes.plot(glambie_ds_14.start_dates,glambie_ds_14.combined_mwe, color=region_colors[14], ls=":", marker= ".",label="GlaMBIE, reg 14")# axes.plot(glambie_ds_15.start_dates,glambie_ds_15.combined_mwe, color=region_colors[15], ls=":", marker= ".",label="GlaMBIE, reg 15")#Hugo: Glacier-specific geodetic mass balance (m w.e. a-1) 2000_2020/ according to Hugonnet et al. (2021) # axes.plot( [1985,2013], np.ones(2)*weighted_B_irr, color=colors['irr'1][0], ls='--', label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# draw_std_boxplot(axes, weighted_B_irr_ts.Year, weighted_B_irr_ts.values, np.zeros(len(weighted_B_irr)), box_width=1, color='grey', ls='-',alpha=0.4, label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# axes.plot(weighted_B_irr_ts.Year, weighted_B_irr_ts.values, color=colors['irr'][0], ls='-', label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")axes[0].plot(weighted_B_irr_ts.Year, np.ones(len(weighted_B_irr_ts))*weighted_B_irr_ts.mean().values, color=colors['irr'][0], ls='-', lw=3, label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# axes.plot(weighted_B_irr_ref_ts.Year, weighted_B_irr_ref_ts.values, color=colors['irr'][0], ls='--', label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$")axes[0].plot(weighted_B_irr_ref_ts.Year, np.ones(len(weighted_B_irr_ref_ts))*weighted_B_irr_ref_ts.mean().values, color=colors['irr'][0], ls=':', lw=3, label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$, ref period")axes[1].plot(weighted_B_irr_ts.Year, weighted_B_irr_ts.values, color=colors['irr'][0], ls='-', lw=2, label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$ avg.,  ref period")# axes.plot( [1985,2014], np.ones(2)*weighted_B_noirr, color=colors['noirr'][0], ls='--', label="Modelled Historic NoIrr (W5E5), reg 13-15, A>5km$^{2}$")axes[0].set_title('Multi year average')axes[1].set_title('Annual timeseries')dataset_legend = [    mpatches.Patch(facecolor='crimson', label='Hugonnet et al.'),    mpatches.Patch(facecolor='orange', label='GlaMBIE'),    mpatches.Patch(facecolor='blue', label='Zemp et al.'),    mpatches.Patch(facecolor=colors['irr'][0], label='"Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$'),]# Legend for line styles (gray lines)style_legend = [    Line2D([0], [0], color='grey', lw=5, alpha=0.5, label='Average ± std'),  # thick grey    Line2D([0], [0], color='grey', lw=2, ls=':', label='Average (2000–2019)'),   # dotted    Line2D([0], [0], color='grey', lw=2, ls='-', label='Average (full period)'), # solid]# Combine both legendsall_legend_items = dataset_legend + style_legend# Add to the figure (adjust location as needed)legend1 = fig.legend(    handles=dataset_legend,    loc='lower center',    bbox_to_anchor=(0.5,-0.15 ),    ncol=4,    frameon=False,    fontsize=12)# Second row legend: line styleslegend2 = fig.legend(    handles=style_legend,    loc='lower center',    bbox_to_anchor=(0.5, -0.20),    ncol=3,    frameon=False,    fontsize=12)# Optional: make space for both legend rowsfig.subplots_adjust(bottom=0)area_glambie = round(glambie_weighted[glambie_weighted['start_dates'] == float(area_rgi_year)]['total_area'].values[0])area_hugo = round(df_hugo.Area.sum())basis=-0.5amp=2.5area_oggm = round(df[df['sample_id']=='CESM2.000'].rgi_area_km2.sum())axes[1].text(1957, basis-0*amp, 'Total Area (km$^2$):',fontsize=14, fontweight='bold')axes[1].text(1957, basis-0.05*amp, f'Zemp et al.: {round(ds_zemp_tot_area)} ({area_rgi_year})',fontsize=14)axes[1].text(1957, basis-0.1*amp, f'GlaMBIE.: {area_glambie} ({area_rgi_year})',fontsize=14)axes[1].text(1957, basis-0.15*amp, f'Hugonne et al.: {area_hugo} (rgi date)',fontsize=14)axes[1].text(1957, basis-0.2*amp, f'Modelled area: {area_oggm} (rgi date)',fontsize=14)axes[0].set_ylabel("∆B m w.e. yr$^{-1}$")axes[1].set_ylabel("∆B m w.e. yr$^{-1}$")axes[1].set_xlabel("Year")# fig.legend(loc='lower center', bbox_to_anchor=(0.5,-0.2), ncols =2)fig.subplots_adjust(wspace=0.05)plt.xlim(1956,2024)plt.show()plt.savefig(f'{fig_path}/Ref_B_Comparison_Figure.png')                                                #%% Cell 9: Fact check data bias correctedsrc = "/Users/magaliponds/Documents/00. Programming/03. Modelled perturbation-glacier interactions - R13-15 A+5km2/summary"fig,axes = plt.subplots(2,1,figsize=(10,5), sharex=True)axes=axes.flatten()ls = ["--", "-.", ":"]colors_ssps=["cornflowerblue","tomato"]for m in range(4):    for s, ssp in enumerate(["126","370"]):        if m>0:            ptb =  xr.open_dataset(f"{src}/climate_input_data_SSP{ssp}_IRR.00{m}.nc")            ptb = ptb.sel(rgi_id="RGI60-15.10342")            smoothed_T = ptb.rolling(time=5, center=True).mean()            smoothed_P = ptb            axes[0].plot(smoothed_T.time, smoothed_T.temp, label=f"CESM2.00{m}, ssp{ssp}", ls=ls[m-1], color=colors_ssps[s], lw=1)            axes[0].set_title("Temperature")            axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=ls[m-1], color=colors_ssps[s], lw=1)            axes[1].set_title("Precipitation")            ptb =  xr.open_dataset(f"{src}/climate_input_data_historical.nc")ptb = ptb.sel(rgi_id="RGI60-15.10342")smoothed_T = ptb.rolling(time=5, center=True).mean()smoothed_P = ptbaxes[0].plot(smoothed_T.time, smoothed_T.temp, label="Historical (W5E5)", ls=":", color="k", lw=2)axes[0].set_title("Temperature")axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=":", color="k",lw=2)axes[1].set_title("Precipitation")axes[0].text(1981,10,"IRR-experiment")axes[1].text(1981,1500,"IRR-experiment")plt.subplots_adjust(hspace=0.3)plt.xlim(1980,2015)fig.legend(ncols=3, loc='lower center', bbox_to_anchor=(0.5,-0.1))plt.show()plt.savefig(f"{fig_folder}/00. Appendix/Bias_correction_check_irr.png")#%% Cell 9b: create plot for noisrc = "/Users/magaliponds/Documents/00. Programming/03. Modelled perturbation-glacier interactions - R13-15 A+5km2/summary"fig,axes = plt.subplots(2,1,figsize=(10,5), sharex=True)axes=axes.flatten()ls = ["-", "-", "-"]colors_ssps=["cornflowerblue","tomato"]colors_ssps_no_bias=["turquoize","orange"]for m in range(4):    for s, ssp in enumerate(["126","370"]):        if m>0:            ptb =  xr.open_dataset(f"{src}/climate_input_data_SSP{ssp}_NOI.00{m}.nc")            ptb = ptb.sel(rgi_id="RGI60-15.10342")            smoothed_T = ptb.rolling(time=5, center=True).mean()            smoothed_P = ptb            axes[0].plot(smoothed_T.time, smoothed_T.temp, label=f"CESM2.00{m}, ssp{ssp}", ls=ls[m-1], color=colors_ssps[s], lw=1)            axes[0].set_title("Temperature")            axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=ls[m-1], color=colors_ssps[s], lw=1)            axes[1].set_title("Precipitation")            ptb =  xr.open_dataset(f"{src}/climate_input_data_historical.nc")ptb = ptb.sel(rgi_id="RGI60-15.10342")smoothed_T = ptb.rolling(time=5, center=True).mean()smoothed_P = ptbaxes[0].plot(smoothed_T.time, smoothed_T.temp, label="Historical (W5E5)", ls=":", color="k", lw=2)axes[0].set_title("Temperature")axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=":", color="k",lw=2)axes[1].set_title("Precipitation")axes[0].text(1981,10,"NOI-experiment")axes[1].text(1981,1450,"NOI-experiment")plt.subplots_adjust(hspace=0.3)plt.xlim(1980,2015)fig.legend(ncols=3, loc='lower center', bbox_to_anchor=(0.5,-0.1))# plt.show()colors_ssps_no_bias=["darkcyan","orange", "green"]for m in range(4):    for s, ssp in enumerate(["126","370"]):        if m>0:            ptb =  xr.open_dataset(f"{src}/climate_input_data_SSP{ssp}_NOI.00{m}_noi_bias.nc")            ptb = ptb.sel(rgi_id="RGI60-15.10342")            smoothed_T = ptb.rolling(time=5, center=True).mean()            smoothed_P = ptb            axes[0].plot(smoothed_T.time, smoothed_T.temp, label=f"CESM2.00{m}, ssp{ssp}, noi bias", ls=ls[m-1], color=colors_ssps_no_bias[s], lw=1)            axes[0].set_title("Temperature")            axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=ls[m-1], color=colors_ssps_no_bias[s], lw=1)            axes[1].set_title("Precipitation")                if m==1:        ptb =  xr.open_dataset(f"{src}/climate_input_data_historical_noi_CESM2.00{m}.nc")        ptb = ptb.sel(rgi_id="RGI60-15.10342")        smoothed_T = ptb.rolling(time=5, center=True).mean()        smoothed_P = ptb        axes[0].plot(smoothed_T.time, smoothed_T.temp, label="Historical (W5E5), noi bias", ls=":", color="k", lw=2)        axes[0].set_title("Temperature")        axes[1].plot(smoothed_P.time, smoothed_P.prcp, ls=":", color=colors_ssps_no_bias[2],lw=2)        axes[1].set_title("Precipitation")        # axes[0].text(1981,10,"IRR-experiment")        # axes[1].text(1981,1500,"IRR-experiment")plt.subplots_adjust(hspace=0.3)        plt.xlim(1980,2015)fig.legend(ncols=3, loc='lower center', bbox_to_anchor=(0.5,-0.2))plt.show()plt.savefig(f"{fig_folder}/00. Appendix/Bias_correction_check_noirr.png")              