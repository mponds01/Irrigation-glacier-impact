# Built insimport loggingimport osimport datetimeimport jsonimport warnings# External libsimport numpy as npimport xarray as xrimport netCDF4import pandas as pdfrom scipy import statsfrom scipy import optimize# Optional libstry:    import salemexcept ImportError:    pass# Localsfrom oggm import cfgfrom oggm import utilsfrom oggm.core import centerlinesfrom oggm import entity_task, global_taskfrom oggm.exceptions import (MassBalanceCalibrationError, InvalidParamsError,                             InvalidWorkflowError)from oggm.shop import ecmwf# Module loggerlog = logging.getLogger(__name__)@entity_task(log, writes=['gcm_data'])def process_perturbation_data(gdir, ds_path, y0=None, y1=None, output_filesuffix=None):        """Processes and writes the perturbation data for a glacier.    If y0>=1979, the temperature lapse rate gradient from ERA5dr is added.    Extracts the nearest timeseries and writes everything to a NetCDF file.    Parameters    ----------    y0 : int        the starting year of the timeseries to write. The default is to take        the entire time period available in the file, but with this kwarg        you can shorten it (to save space or to crop bad data). If y0>=1979,        it only uses W5E5 data!    y1 : int        the end year of the timeseries to write. The default is to take        the entire time period available in the file, but with this kwarg        you can shorten it (to save space or to crop bad data)    output_filesuffix : str, optional         None by default    """    tvar = 'temp'    pvar = 'prcp'    hvar = 'hgt'    # get the central longitude/latitudes of the glacier    lon = gdir.cenlon + 360 if gdir.cenlon < 0 else gdir.cenlon    lat = gdir.cenlat    # ds_path = ds_path['temp']    # path_prcp = ds_path['prcp']    # path_inv = get_gswp3_w5e5_file(dataset, 'inv')    # Use xarray to read the data    # would go faster with only netCDF -.-, but easier with xarray    # first temperature dataset    with xr.open_dataset(ds_path)[tvar] as ds:                # get the central longitude/latitudes of the glacier        assert ds.lon.min() >= 0        yrs = ds['time.year'].data        y0 = yrs[0] if y0 is None else y0        y1 = yrs[-1] if y1 is None else y1        if y1 > 2019 or y0 < 1901:            text = 'The climate files only go from 1901--2019'            raise InvalidParamsError(text)        ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))        try:            # computing all the distances and choose the nearest gridpoint            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            ds = ds.sel(lon=lon, lat=lat, method='nearest')            # normally if I do the flattening, this here should not occur        # because of the flattening, there is no time dependence of lon and lat anymore!        ds['lon'] = ds.lon  # .isel(time=0)        ds['lat'] = ds.lat  # .isel(time=0)        # temperature should be in degree Celsius for the glacier climate files        if ds.units.lower() not in ['degc', 'degrees','degree', 'c']:            temp = ds.data - 273.15        else:            temp = ds.data         time = ds.time.data        ref_lon = float(ds['lon'])        ref_lat = float(ds['lat'])        ref_lon = ref_lon - 360 if ref_lon > 180 else ref_lon    # precipitation: similar as temperature    with xr.open_dataset(ds_path)[pvar] as ds:        assert ds.lon.min() >= 0        # here we take the same y0 and y1 as given from the        # tmp dataset        ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))        try:            # ... prcp is also flattened            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            # this should not occur            ds = ds.sel(lon=lon, lat=lat, method='nearest')        prcp=ds    # w5e5 invariant file    with xr.open_dataset(ds_path)[hvar] as ds:        assert ds.lon.min() >= 0        # ds = ds.isel(time=0)        try:            # Flattened  (only possibility at the moment)            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            # this should not occur            ds = ds.sel(lon=lon, lat=lat, method='nearest')        # w5e5 inv ASurf/hgt is already in hgt coordinates        hgt = ds.data    # here we need to use the ERA5dr data ...    # there are no lapse rates from W5E5 !!!    # TODO: use updated ERA5dr files that goes until end of 2019    #  and update the code accordingly !!!    if y0 < 1979:        # just don't compute lapse rates as ERA5 only starts in 1979        # could use the average from 1979-2019, but for the moment        # gradients are not used in OGGM anyway, so just ignore it:        gradient = None    else:        path_lapserates = ecmwf.get_ecmwf_file('ERA5dr', 'lapserates')        with xr.open_dataset(path_lapserates) as ds:            ecmwf._check_ds_validity(ds)            # this has a different time span!            yrs = ds['time.year'].data            y0 = yrs[0] if y0 is None else y0            y1 = yrs[-1] if y1 is None else y1            ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))            # no flattening done for the ERA5dr gradient dataset            ds = ds.sel(longitude=lon, latitude=lat, method='nearest')            if y1 == 2019:                # missing some months of ERA5dr (which only goes till middle of 2019)                # otherwise it will fill it with large numbers ...                ds = ds.sel(time=slice(f'{y0}-01-01', '2018-12-01'))                # take for hydrological year 2019 just the avg. lapse rates                # this gives in month order Jan-Dec the mean laperates,                mean_grad = ds.groupby('time.month').mean().lapserate                # fill the last year with mean gradients                gradient = np.concatenate((ds['lapserate'].data, mean_grad),                                          axis=None)            else:                # get the monthly gradient values                gradient = ds['lapserate'].data    # path_temp_std = get_gswp3_w5e5_file(dataset, 'temp_std')    # with xr.open_dataset(path_temp_std) as ds:    #     ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))    #     try:    #         # ... prcp is also flattened    #         c = (ds.lon - lon)**2 + (ds.lat - lat)**2    #         ds = ds.isel(points=np.argmin(c.data))    #     except ValueError:    #         # this should not occur    #         ds = ds.sel(longitude=lon, latitude=lat, method='nearest')    #     temp_std = ds['temp_std'].data  # tas_std for W5E5!!!    # OK, ready to write    gdir.write_monthly_climate_file(time, prcp, temp, hgt, ref_lon, ref_lat,                                    filesuffix=output_filesuffix,                                    # temp_std=temp_std,                                    source="IRRMIP data ensemble")