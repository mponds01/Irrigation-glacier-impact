# Built insimport loggingimport osimport datetimeimport jsonimport warnings# External libsimport numpy as npimport xarray as xrimport netCDF4import pandas as pdfrom scipy import statsfrom scipy import optimize# Optional libstry:    import salemexcept ImportError:    pass# Localsfrom oggm import cfgfrom oggm import utilsfrom oggm.core import centerlinesfrom oggm import entity_task, global_taskfrom oggm.exceptions import (MassBalanceCalibrationError, InvalidParamsError,                             InvalidWorkflowError)from oggm.tasks import process_gcm_datafrom oggm.shop import ecmwf# Module loggerlog = logging.getLogger(__name__)@entity_task(log, writes=['gcm_data'])def process_perturbation_data(gdir, ds_path, y0=None, y1=None, output_filesuffix=None):    """Processes and writes the perturbation data for a glacier.    If y0>=1979, the temperature lapse rate gradient from ERA5dr is added.    Extracts the nearest timeseries and writes everything to a NetCDF file.    Parameters    ----------    y0 : int        the starting year of the timeseries to write. The default is to take        the entire time period available in the file, but with this kwarg        you can shorten it (to save space or to crop bad data). If y0>=1979,        it only uses W5E5 data!    y1 : int        the end year of the timeseries to write. The default is to take        the entire time period available in the file, but with this kwarg        you can shorten it (to save space or to crop bad data)    output_filesuffix : str, optional         None by default    """    tvar = 'temp'    pvar = 'prcp'    hvar = 'hgt'    # get the central longitude/latitudes of the glacier    lon = gdir.cenlon + 360 if gdir.cenlon < 0 else gdir.cenlon    lat = gdir.cenlat    # ds_path = ds_path['temp']    # path_prcp = ds_path['prcp']    # path_inv = get_gswp3_w5e5_file(dataset, 'inv')    # Use xarray to read the data    # would go faster with only netCDF -.-, but easier with xarray    # first temperature dataset    with xr.open_dataset(ds_path)[tvar] as ds:        # get the central longitude/latitudes of the glacier        assert ds.lon.min() >= 0        yrs = ds['time.year'].data        y0 = yrs[0] if y0 is None else y0        y1 = yrs[-1] if y1 is None else y1        if y1 > 2019 or y0 < 1901:            text = 'The climate files only go from 1901--2019'            raise InvalidParamsError(text)        ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))        try:            # computing all the distances and choose the nearest gridpoint            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            ds = ds.sel(lon=lon, lat=lat, method='nearest')            # normally if I do the flattening, this here should not occur        # because of the flattening, there is no time dependence of lon and lat anymore!        ds['lon'] = ds.lon  # .isel(time=0)        ds['lat'] = ds.lat  # .isel(time=0)        # temperature should be in degree Celsius for the glacier climate files        if ds.units.lower() not in ['degc', 'degrees', 'degree', 'c']:            temp = ds.data - 273.15        else:            temp = ds.data        time = ds.time.data        ref_lon = float(ds['lon'])        ref_lat = float(ds['lat'])        ref_lon = ref_lon - 360 if ref_lon > 180 else ref_lon    # precipitation: similar as temperature    with xr.open_dataset(ds_path)[pvar] as ds:        assert ds.lon.min() >= 0        # here we take the same y0 and y1 as given from the        # tmp dataset        ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))        try:            # ... prcp is also flattened            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            # this should not occur            ds = ds.sel(lon=lon, lat=lat, method='nearest')        prcp = ds    # w5e5 invariant file    with xr.open_dataset(ds_path)[hvar] as ds:        assert ds.lon.min() >= 0        # ds = ds.isel(time=0)        try:            # Flattened  (only possibility at the moment)            c = (ds.lon - lon)**2 + (ds.lat - lat)**2            ds = ds.isel(points=np.argmin(c.data))        except ValueError:            # this should not occur            ds = ds.sel(lon=lon, lat=lat, method='nearest')        # w5e5 inv ASurf/hgt is already in hgt coordinates        hgt = ds.data    # here we need to use the ERA5dr data ...    # there are no lapse rates from W5E5 !!!    # TODO: use updated ERA5dr files that goes until end of 2019    #  and update the code accordingly !!!    if y0 < 1979:        # just don't compute lapse rates as ERA5 only starts in 1979        # could use the average from 1979-2019, but for the moment        # gradients are not used in OGGM anyway, so just ignore it:        gradient = None    else:        path_lapserates = ecmwf.get_ecmwf_file('ERA5dr', 'lapserates')        with xr.open_dataset(path_lapserates) as ds:            ecmwf._check_ds_validity(ds)            # this has a different time span!            yrs = ds['time.year'].data            y0 = yrs[0] if y0 is None else y0            y1 = yrs[-1] if y1 is None else y1            ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))            # no flattening done for the ERA5dr gradient dataset            ds = ds.sel(longitude=lon, latitude=lat, method='nearest')            if y1 == 2019:                # missing some months of ERA5dr (which only goes till middle of 2019)                # otherwise it will fill it with large numbers ...                ds = ds.sel(time=slice(f'{y0}-01-01', '2018-12-01'))                # take for hydrological year 2019 just the avg. lapse rates                # this gives in month order Jan-Dec the mean laperates,                mean_grad = ds.groupby('time.month').mean().lapserate                # fill the last year with mean gradients                gradient = np.concatenate((ds['lapserate'].data, mean_grad),                                          axis=None)            else:                # get the monthly gradient values                gradient = ds['lapserate'].data    # path_temp_std = get_gswp3_w5e5_file(dataset, 'temp_std')    # with xr.open_dataset(path_temp_std) as ds:    #     ds = ds.sel(time=slice(f'{y0}-01-01', f'{y1}-12-01'))    #     try:    #         # ... prcp is also flattened    #         c = (ds.lon - lon)**2 + (ds.lat - lat)**2    #         ds = ds.isel(points=np.argmin(c.data))    #     except ValueError:    #         # this should not occur    #         ds = ds.sel(longitude=lon, latitude=lat, method='nearest')    #     temp_std = ds['temp_std'].data  # tas_std for W5E5!!!    # OK, ready to write    gdir.write_monthly_climate_file(time, prcp, temp, hgt, ref_lon, ref_lat,                                    filesuffix=output_filesuffix,                                    # temp_std=temp_std,                                    source="IRRMIP data ensemble")@entity_task(log, writes=['gcm_data'])def custom_process_cmip_data(gdir, fpath_temp=None, fpath_precip=None, y0=None, y1=None,                      filesuffix='', output_filesuffix='',                      **kwargs):    """Read, process and store the CMIP5 and CMIP6 climate data for this glacier.    It stores the data in a format that can be used by the OGGM mass balance    model and in the glacier directory.    Currently, this function is built for the CMIP5 and CMIP6 projection    simulations that are on the OGGM servers.    Parameters    ----------    fpath_temp : str        path to the temp file    fpath_precip : str        path to the precip file    y0 : int        start year of the CMIP data processing.        Default is None which processes the entire timeseries.        Set this to the beginning of your bias correction/        projection period minus half of bc period        to make process_cmip_data faster.    y1 : int        end year of the CMIP data processing.        Set this to the end of your projection period        plus half of bc period. Default is None to process        the entire time series, same as y0.    filesuffix : str        same as `output_filesuffix` but deprecated.    output_filesuffix : str        append a suffix to the filename (useful for ensemble experiments).    **kwargs: any kwarg to be passed to ref:`process_gcm_data`    """    if filesuffix:        output_filesuffix = filesuffix    # Glacier location    glon = gdir.cenlon    glat = gdir.cenlat    if y0 is not None:        y0 = str(y0)    if y1 is not None:        y1 = str(y1)    # Read the GCM files    with xr.open_dataset(fpath_temp, decode_times=True) as tempds, \            xr.open_dataset(fpath_precip,  decode_times=True) as precipds:        # only process and save the gcm data selected --> saves some time!        if (y0 is not None) or (y1 is not None):                            tempds = tempds.sel(time=slice(y0, y1))                        precipds = precipds.sel(time=slice(y0, y1))        # Check longitude conventions        if tempds.lon.min() >= 0 and glon <= 0:            glon += 360        # Take the closest to the glacier        # Should we consider GCM interpolation?        try:            # if gcms are not flattened, do:            # this is the default, so try this first            temp = tempds.tas.sel(lat=glat, lon=glon, method='nearest')            precip = precipds.pr.sel(lat=glat, lon=glon, method='nearest')        except:            # are the gcms flattened? if yes,            # compute all the distances and choose the            # nearest gridpoint            c_tempds = ((tempds.lon - glon) ** 2 + (tempds.lat - glat) ** 2)            c_precipds = ((precipds.lon - glon) ** 2 + (precipds.lat - glat) ** 2)            temp_0 = tempds.isel(points=np.argmin(c_tempds.data))            precip_0 = precipds.isel(points=np.argmin(c_precipds.data))            temp = temp_0.tas            temp['lon'] = temp_0.lon            temp['lat'] = temp_0.lat            precip = precip_0.pr            precip['lon'] = precip_0.lon            precip['lat'] = precip_0.lat        # Back to [-180, 180] for OGGM        temp.lon.values = temp.lon if temp.lon <= 180 else temp.lon - 360        precip.lon.values = precip.lon if precip.lon <= 180 else precip.lon - 360                # Convert kg m-2 s-1 to mm mth-1 => 1 kg m-2 = 1 mm !!!        assert 'kg m-2 s-1' in precip.units, 'Precip units not understood'        ny, r = divmod(len(temp.time), 12)                assert r == 0        # print( np.array(cfg.DAYS_IN_MONTH)[temp['time.month'].values -1])        dimo = [cfg.DAYS_IN_MONTH[m - 1] for m in temp['time.month']]        # dimo = np.array(cfg.DAYS_IN_MONTH)[temp['time.month'].values - 1]        precip = precip * dimo * (60 * 60 * 24)        month = temp['time.month']    custom_process_gcm_data(gdir, output_filesuffix=output_filesuffix, prcp=precip, temp=temp,                     source=output_filesuffix, **kwargs)        @entity_task(log, writes=['gcm_data'])def custom_process_gcm_data(gdir, prcp=None, temp=None,                     year_range=('1986', '2015'), scale_stddev=True, #original bias correction period 1961-1990                     filesuffix='', output_filesuffix='',                     time_unit=None, calendar=None, source='',                     apply_bias_correction=True):    """ Applies the anomaly method to GCM climate data    This function can be applied to any GCM data, if it is provided in a    suitable :py:class:`xarray.DataArray`. See Parameter description for    format details.    For CESM-LME a specific function :py:func:`tasks.process_cesm_data` is    available which does the preprocessing of the data and subsequently calls    this function.    Parameters    ----------    gdir : :py:class:`oggm.GlacierDirectory`        where to write the data    prcp : :py:class:`xarray.DataArray`        | monthly total precipitation [mm month-1]        | Coordinates:        | lat float64        | lon float64        | time: cftime object    temp : :py:class:`xarray.DataArray`        | monthly temperature [K]        | Coordinates:        | lat float64        | lon float64        | time cftime object    year_range : tuple of str        the year range for which you want to compute the anomalies. Default        is `('1961', '1990')`    scale_stddev : bool        whether or not to scale the temperature standard deviation as well    filesuffix : str        same as `output_filesuffix` but deprecated.    output_filesuffix : str        append a suffix to the filename (useful for ensemble experiments).    time_unit : str        The unit conversion for NetCDF files. It must be adapted to the        length of the time series. The default is to choose        it ourselves based on the starting year.        For example: 'days since 0850-01-01 00:00:00'    calendar : str        If you use an exotic calendar (e.g. 'noleap')    source : str        For metadata: the source of the climate data    apply_bias_correction : boolean        if a bias-correction should be applied. Default is True, only set it to False        if the GCM has already been externally bias-corrected to the applied        observational calibration dataset (true for ISIMIP 3b that is bias-corrected        to W5E5). !!! We assume that temp is in Kelvin and convert to CELSIUS !!!    """    if filesuffix:        output_filesuffix = filesuffix    # Standard sanity checks    months = temp['time.month']    if months[0] != 1:        raise ValueError('We expect the files to start in January!')    if months[-1] != 12:        raise ValueError('We expect the files to end in December!')    if (np.abs(temp['lon']) > 180) or (np.abs(prcp['lon']) > 180):        raise ValueError('We expect the longitude coordinates to be within '                         '[-180, 180].')    assert len(prcp) // 12 == len(prcp) / 12, "Somehow we didn't get full years"    assert len(temp) // 12 == len(temp) / 12, "Somehow we didn't get full years"    # Get the reference data to apply the anomaly to    fpath = gdir.get_filepath('climate_historical')    with xr.open_dataset(fpath) as ds_ref:        ds_ref = ds_ref.sel(time=slice(*year_range))        if apply_bias_correction:            # compute monthly anomalies            # of temp            if scale_stddev:                # This is a bit more arithmetic                ts_tmp_sel = temp.sel(time=slice(*year_range))                                # ts_tmp_sel = xr.decode_cf(ts_tmp_sel)                if len(ts_tmp_sel) // 12 != len(ts_tmp_sel) / 12:                    raise InvalidParamsError('year_range cannot contain the first'                                             'or last calendar year in the series')                if ((len(ts_tmp_sel) // 12) % 2) == 1:                    raise InvalidParamsError('We need an even number of years '                                             'for this to work')                                ts_tmp_std = ts_tmp_sel.groupby('time.month').std(dim='time')                                std_fac = ds_ref.temp.groupby('time.month').std(dim='time') / ts_tmp_std                std_fac = np.tile(std_fac.data, len(temp) // 12)                                # We need an even number of years for this to work                win_size = len(ts_tmp_sel) + 1                def roll_func(x, axis=None):                    x = x[:, ::12]                    n = len(x[0, :]) // 2                    xm = np.nanmean(x, axis=axis)                    return xm + (x[:, n] - xm) * std_fac                temp = temp.rolling(time=win_size, center=True,                                    min_periods=1).reduce(roll_func)            ts_tmp_sel = temp.sel(time=slice(*year_range))            if len(ts_tmp_sel.time) != len(ds_ref.time):                raise InvalidParamsError('The reference climate period and the '                                         'GCM period after window selection do '                                         'not match.')            ts_tmp_avg = ts_tmp_sel.groupby('time.month').mean(dim='time')            ts_tmp = temp.groupby('time.month') - ts_tmp_avg            # of precip -- scaled anomalies            ts_pre_avg = prcp.sel(time=slice(*year_range))            ts_pre_avg = ts_pre_avg.groupby('time.month').mean(dim='time')            ts_pre_ano = prcp.groupby('time.month') - ts_pre_avg            # scaled anomalies is the default. Standard anomalies above            # are used later for where ts_pre_avg == 0            ts_pre = prcp.groupby('time.month') / ts_pre_avg            # for temp            loc_tmp = ds_ref.temp.groupby('time.month').mean()            ts_tmp = ts_tmp.groupby('time.month') + loc_tmp                        # for prcp            loc_pre = ds_ref.prcp.groupby('time.month').mean()            # scaled anomalies            ts_pre = ts_pre.groupby('time.month') * loc_pre            # standard anomalies            ts_pre_ano = ts_pre_ano.groupby('time.month') + loc_pre            # Correct infinite values with standard anomalies            ts_pre.values = np.where(np.isfinite(ts_pre.values),                                     ts_pre.values,                                     ts_pre_ano.values)                    assert np.all(np.isfinite(ts_pre.values))            assert np.all(np.isfinite(ts_tmp.values))        else:            # do no bias correction at all            # (!!! only if GCM is already externally bias corrected)            ts_tmp = temp - 273.15  # convert K to Celsius            ts_pre = prcp  # mm month-1            source = source + '_no_OGGM_bias_correction'        gdir.write_monthly_climate_file(temp.time.values,                                        ts_pre.values, ts_tmp.values,                                        float(ds_ref.ref_hgt),                                        prcp.lon.values, prcp.lat.values,                                        time_unit=time_unit,                                        calendar=calendar,                                        file_name='gcm_data',                                        source=source,                                        filesuffix=output_filesuffix)