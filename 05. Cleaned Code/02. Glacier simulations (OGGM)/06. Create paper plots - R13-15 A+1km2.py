# -*- coding: utf-8 -*-# %% Cell 0: Load data packages#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Wed Jul  3 15:36:52 2024@author: magalipondsThis script runs makes plots of the selected 3 regions (13-14-15) with Areas larger than 1km2 (3r_a1)It runs trhough the following cells:    Cell 0: Load packages Cell 1: set dictionaries for plotting Cell 1b: Load gdirsCell 2a: Prepare data for map plot (for scatters  in map B)Cell 2b: Process ∆V data for plotting (for scatters in map V )Cell 2c: Prepare voluem evolution for map plotCell 2d: Create map plot (using data created in step 2)Cell 3a: Prepare the data per model individually (for scatters in map B)Cell 3b: Prepare ∆V data per model Cell 3c: Create map plot per model with subfigures - appendix figureCell 4a: Create a nan mask for the gdirs without successful commited run (e.g. exceeding domain boundaries)Cell 4b: Create mass loss plot with transient panels (needed as inset axis in final plot)Cell 4c: Create comitted mass loss boxplots in Marzeion styleCell 5: Create table (with all data included, also uncertainties)Cell 5b: create small table (used in paper)Cell 6: Create temperature plot with 2D legend (global and local, not used)Cell 6b: Plot ∆T ∆P plot with 1D legendCell 6c: Read out subregional average perturbation in precipitation and temperature (for input in manuscript)Cell 6d: Read out values for irrigation onlyCell 7: Presentation plot - area equipped for irrigation growthCell 8: Reference plot to mass balance data from ohter studies (e.g. Zemp, Hugonnet, Dussaillant)"""#%% Cell 0: Load packages# -*- coding: utf-8 -*-import oggm# from OGGM_data_processing import process_perturbation_data# import mpl_axes_alignerimport concurrent.futuresfrom shapely.geometry import LineString, MultiLineStringimport astimport stringfrom matplotlib.lines import Line2Dimport oggmfrom oggm import utils, cfg, workflow, tasks, DEFAULT_BASE_URL, graphics, global_tasksfrom oggm.core import massbalance, flowlinefrom oggm.utils import floatyear_to_date, hydrodate_to_calendardatefrom oggm.sandbox import distribute_2dfrom oggm.sandbox.edu import run_constant_climate_with_biasimport geopandas as gpdimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport matplotlib.colors as clrsfrom matplotlib.colors import ListedColormap, BoundaryNormfrom matplotlib.patches import ConnectionPatchimport xarray as xrimport osimport seaborn as snsimport salemfrom matplotlib.ticker import FuncFormatterimport regionmaskimport pandas as pdimport numpy as npfrom matplotlib import animationfrom IPython.display import HTML, displayimport matplotlib.pyplot as pltimport cartopy.crs as ccrsimport cartopy.feature as cfeatureimport geopandas as gpdimport pandas as pdfrom scipy.optimize import curve_fitfrom scipy import statsfrom matplotlib.legend_handler import HandlerTuplefrom matplotlib.colors import LightSourcefrom tqdm import tqdmimport pickleimport sysimport textwrapimport matplotlib.patches as mpatchesimport matplotlib.lines as mlinesfrom matplotlib.colors import LinearSegmentedColormap, TwoSlopeNorm, Normalize, ListedColormapfrom mpl_toolkits.axes_grid1.inset_locator import inset_axesfrom shapely.geometry import Pointimport matplotlib.gridspec as gridspecfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTERfrom matplotlib.patches import Rectangle, ConnectionPatchfrom mpl_toolkits.axes_grid1.inset_locator import inset_axesimport matplotlib.patheffects as path_effectsfunction_directory = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/src/03. Glacier simulations"sys.path.append(function_directory)# %% Cell 1: Set base parameters# colors = {#     "irr": ["#000000", "#777777"],#     "noirr": ["#6363FF", "#B1B1FF"],#     # much lighter versions of noirr colors#     "noirr_com": ["#A6A6FF", "#E0E0FF"],#     # much lighter grey fade of irr colors#     "irr_com": ["#C0C0C0", "#E0E0E0"],#     "cf": ["#FF5722", "#FFA780"],#     "Yellow": ["#FFC107", "#FFE08A"]# }xkcd_colors = clrs.XKCD_COLORS# colors = {#     "irr": ["#000000", "#555555"],  # Black and dark gray#     # Darker brown and golden yellow for contrast#     # "noirr": ["#f5bf03","#fbeaac"],#["#8B5A00", "#D4A017"], #fdde6c#     "noirr": ["dimgrey","darkgrey"],#["#8B5A00", "#D4A017"], #fdde6c#     # "noirr_com": ["#E3C565", "#F6E3B0"],  # Lighter, distinguishable tan shades ##     # "noirr_com": ["#380282", "#ceaefa"],  # Lighter, distinguishable tan shades ##     "noirr_com": ["#FFC107", "#FFF3CD"],  # Lighter, distinguishable tan shades ##     "irr_com": ["#fe4b03", "#D0D0D0"],  # Light gray, no change#     # "irr_com": ["#B5B5B5", "#D0D0D0"],  # Light gray, no change#     "cf": ["#004C4C", "#40E0D0"],#     "cf_com": ["#008B8B", "#40E0D0"],#     "cline": ["dimgrey", '#FFC107']# }colors = {    "irr": ["#000000", "#555555"],  # Black and dark gray    "noirr": ["dimgrey","darkgrey"],#["#8B5A00", "#D4A017"], #fdde6c    # "noirr_com": ["#FFC107", "#FFF3CD"],  # Lighter, distinguishable tan shades #    # "irr_com": ["#fe4b03", "#D0D0D0"],  # Light gray, no change    # "noirr_com": ["#FFA500", "#FFD580"],    # "irr_com": ["#00796B", "#388E3C"],    # Teal and deep green (distinct from SSPs)    # "noirr_com": ["#80CBC4", "#A5D6A7"],  # Light turquoise and mint    "irr_com": ["#A0522D", "#BF6C38"],     # Dry, earthy rust tones    "noirr_com": ["#E6A87D", "#F3D4B3"],   # Muted peach and tan    "cf": ["#004C4C", "#40E0D0"],    "cf_com": ["#008B8B", "#40E0D0"],    "cline": ["dimgrey", '#E6A87D']}region_colors = {13: 'blue', 14: 'crimson', 15: 'orange'}colors_models = {    "W5E5": ["#000000"],  # "#000000"],  # Black    # Darker to lighter shades of purple    "E3SM": ["#785EF0", "#8F7BF1", "#A6A8F2"],    # Darker to lighter shades of pink    "CESM2": ["#DC267F", "#E58A9E", "#F0A2B6", "#F7BCC4"],    # Darker to lighter shades of orange    "CNRM": ["#FE6100", "#FE7D33", "#FE9A66", "#FEB799", "#FECDB5", "#FEF1E1"],    "IPSL-CM6": ["#FFB000"],    "NorESM": ["#FFC107", "#FFE08A"]  # Dark purple to lighter shades}members = [1, 3, 4, 6, 4, 1]members_averages = [1, 2, 3, 5, 3]models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]timeframe = "monthly"y0_clim = 1985ye_clim = 2014fig_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/04. Figures/98. Final Figures A+1km2/'folder_path = '/Users/magaliponds/Documents/00. Programming'wd_path = f'{folder_path}/04. Modelled perturbation-glacier interactions - R13-15 A+1km2/'sum_dir = os.path.join(wd_path, 'summary')# %% Cell 1b: Load gdirswd_path_pkls = f'{wd_path}/pkls_subset_success/'gdirs_3r_a1 = []for filename in os.listdir(wd_path_pkls):    if filename.endswith('.pkl'):        # f'{gdir.rgi_id}.pkl')        file_path = os.path.join(wd_path_pkls, filename)        with open(file_path, 'rb') as f:            gdir = pickle.load(f)            gdirs_3r_a1.append(gdir)#%% Cell 2a: map plot - prepare data"""Process master for map plot """# Load datasetsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]# master_ds = df#df[(~df['sample_id'].str.endswith('0')) ]# master_ds = master_ds[(master_ds['sample_id'].str.startswith('E3SM'))] #Temporarymaster_ds = master_ds.reset_index(drop=True)# Normalize values# divide all the B values with 1000 to transform to m w.e. average over 30 yrs# master_ds[['B_noirr', 'B_irr', 'B_delta_irr', 'B_cf',  "B_delta_cf"]] /= 1000master_ds.loc[:, ['B_noirr', 'B_irr', 'B_delta']] /= 1000master_ds = master_ds[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta']]# Define custom aggregation functions for grouping over the 11 member dataaggregation_functions = {    'rgi_region': 'first',    'rgi_subregion': 'first',    'full_name': 'first',    'cenlon': 'first',    'cenlat': 'first',    'rgi_date': 'first',    'rgi_area_km2': 'first',    'rgi_volume_km3': 'first',    'B_noirr': 'mean',    'B_irr': 'mean',    'B_delta': 'mean',    'sample_id': 'first'}#average mass balance over the different models master_ds_avg = master_ds.groupby(['rgi_id'], as_index=False).agg({    'B_delta': 'mean',    'B_noirr': 'mean',    'B_irr': 'mean',    # lamda is anonmous functions, returns 11 member average    'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id']}})master_ds_avg.to_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")# Aggregate data for scatter plotmaster_ds_avg['grid_lon'] = np.floor(master_ds_avg['cenlon'])master_ds_avg['grid_lat'] = np.floor(master_ds_avg['cenlat'])# Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member averageaggregated_ds = master_ds_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({    'B_delta': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'B_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'B_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),    'rgi_area_km2': 'sum',  # Sum for area    'rgi_volume_km3': 'sum',  # Sum for volume    # 'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id', 'rgi_area_km2', 'rgi_volume_km3']}})aggregated_ds.rename(    columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)aggregated_ds.to_csv(    f"{wd_path}masters/complete_master_processed_for_map_plot.csv")# f"{wd_path}masters/complete_master_processed_for_map_plot_E3SM.csv")#%% Cell 2b: map plot - Process ∆ volume datamembers_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]climate_run_output_irr = xr.open_dataset(os.path.join(    sum_dir, f'climate_run_output_baseline_W5E5.000.nc'))initial_volume=climate_run_output_irr.volume.sel(time=1985) #only select first year for initial valuedelta_volume_irr = climate_run_output_irr.volume - initial_volumeall_datasets = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                climate_run_output_noirr = xr.open_dataset(os.path.join(            sum_dir, f'climate_run_output_perturbed_{sample_id}.nc'))        delta_volume_noirr = climate_run_output_noirr.volume - initial_volume        delta_volume_noirr_rel = xr.where(delta_volume_irr != 0, delta_volume_noirr / delta_volume_irr, 0)                        yearly_mean_volume = climate_run_output_noirr.mean(dim='time')                ds_new = xr.Dataset({            "delta_volume_irr": delta_volume_irr,            "delta_volume_noirr": delta_volume_noirr,            "sample_id": [sample_id],            "delta_volume_noirr_rel": delta_volume_noirr_rel        })        # Add a new coordinate for sample_id        # ds_new = ds_new.expand_dims(dim={"sample_id": [sample_id]})        # Append to list        all_datasets.append(ds_new)        volume_change_ds = xr.concat(all_datasets, dim="sample_id")volume_change_ds["delta_volume_noirr_rel"].attrs["description"] = "Relative volume change ∆ NoIrr (resp. Irr-1985)/ ∆ Irr (resp. Irr-1985)"# Group by rgi_id and time, then compute the mean for numeric variablesvolume_ds_avg = volume_change_ds.mean(dim="sample_id").mean(dim="time") #30-yr average values for all 14 members in the model ensembleopath_bymember = os.path.join(wd_path,"masters", 'delta_volume_evolution_bymember.nc')volume_change_ds.to_netcdf(opath_bymember)opath_averaged = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average.nc')opath_averaged_csv = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average.csv')# opath_averaged = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average_CESM2.nc')# opath_averaged_csv = os.path.join(wd_path,"masters", 'delta_volume_evolution_ensemble_average_CESM2.csv')volume_ds_avg.to_netcdf(opath_averaged)volume_df_avg = volume_ds_avg.to_dataframe()master_df = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")[['rgi_id', 'cenlon','cenlat', 'rgi_area_km2']]volume_df_avg = volume_df_avg.merge(    master_df, on="rgi_id", how="left")# Aggregate data for scatter plotvolume_df_avg['grid_lon'] = np.floor(volume_df_avg['cenlon'])volume_df_avg['grid_lat'] = np.floor(volume_df_avg['cenlat'])# Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member averagevolume_df_avg = volume_df_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({    'delta_volume_irr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'delta_volume_noirr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'delta_volume_noirr_rel': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),    'rgi_area_km2': 'sum',  # Sum for area    # 'sample_id': lambda _: "11 member average",    # take first value for all columns that are not in the list    **{col: 'first' for col in volume_df_avg.columns if col not in ['delta_volume_irr', 'delta_volume_noirr', 'delta_volume_noirr_rel', 'rgi_area_km2']}})volume_df_avg.rename(    columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)volume_df_avg.to_csv(opath_averaged_csv)#%% Cell 2c: map plot - prepare volume evolution#tracker regions = [13, 14, 15]subregions = [9, 3, 3]region_data = []members_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2", "CNRM", "IPSL-CM6","NorESM"]        # Storage for time seriessubregion_series = {}  # subregion → DataArray[time, member]global_series = []     # total average over all subregions per membermembers_all = []       # (model, member_id) pairs to track 14-member averagefor m, model in enumerate(models_shortlist):    for j in range(members_averages[m]):        member_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        sample_id = member_id  # used in filename        print(sample_id)        # Open NetCDF file        path = os.path.join(sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')        ds = xr.open_dataset(path, engine="h5netcdf")        members_all.append(member_id)        # Per subregion        for reg, region in enumerate(regions):            for sub in range(subregions[reg]):                region_id = f"{region}-0{sub+1}"                subregion_ds = master_ds_avg[                    master_ds_avg['rgi_subregion'].str.contains(region_id)                ]                if subregion_ds.empty:                    continue                # Filter data                ds_filtered = ds.where(                    ds['rgi_id'].isin(subregion_ds.rgi_id.values), drop=True)                # Sum volume over glaciers in this subregion                vol_timeseries = ds_filtered["volume"].sum(dim="rgi_id")  # dims: time                key = region_id                if key not in subregion_series:                    subregion_series[key] = []                subregion_series[key].append(vol_timeseries.assign_coords(member=member_id))        # Add global average (sum over all glaciers, no filter)        global_total = ds["volume"].sum(dim="rgi_id")  # dims: time        global_series.append(global_total.assign_coords(member=member_id))# ---------------------------------------#  Convert per-subregion data to Datasetsubregion_datasets = {}subregion_datasets_med = {}for region_id, series_list in subregion_series.items():    da = xr.concat(series_list, dim="member")    da = da.transpose("time", "member")    da_avg = da.mean(dim="member").assign_coords(member="14-member-avg")    da_med = da.median(dim="member").assign_coords(member="14-member-med")    da_full = xr.concat([da, da_avg.expand_dims(dim="member")], dim="member")    da_full_med = xr.concat([da, da_med.expand_dims(dim="member")], dim="member")    subregion_datasets[region_id] = da_full    subregion_datasets_med[region_id] = da_full_med# Merge all subregions into one dataset with region_id as a new dimensionds_subregions = xr.concat(    [ds.expand_dims(subregion=[key]) for key, ds in subregion_datasets.items()],    dim="subregion")ds_subregions_med = xr.concat(    [ds.expand_dims(subregion=[key]) for key, ds in subregion_datasets_med.items()],    dim="subregion")# ---------------------------------------# Create global average datasetglobal_all = xr.concat(global_series, dim="member").transpose("time", "member")global_avg = global_all.mean(dim="member").assign_coords(member="14-member-avg")global_median = global_all.median(dim="member").assign_coords(member="14-member-med")global_full = xr.concat([global_all, global_avg.expand_dims(dim="member")], dim="member")global_full_med = xr.concat([global_all, global_median.expand_dims(dim="member")], dim="member")# ---------------------------------------# Now you have:# - ds_subregions: [time, member, subregion]# - global_full:   [time, member]# You can save them if needed:ds_subregions.to_netcdf(f"{wd_path}masters/master_volume_ts_subregions_members.nc")ds_subregions_med.to_netcdf(f"{wd_path}masters/master_volume_ts_subregions_members_median.nc")# global_full.to_netcdf(f"{wd_path}masters/master_volume_ts_global_members.nc")# global_full_med.to_netcdf(f"{wd_path}masters/master_volume_ts_global_members_median.nc")# ds_subregions.to_netcdf(f"{wd_path}masters/master_volume_ts_subregions_members_CESM2.nc")# global_full.to_netcdf(f"{wd_path}masters/master_volume_ts_global_members_CESM2.nc")#%% Cell 2d: Create map plot - ∆B or ∆Vmembers_averages = [2, 3,    5 ,1,3]models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]""" Load data"""aggregated_ds = pd.read_csv(    f"{wd_path}masters/complete_master_processed_for_map_plot.csv")aggregated_ds_vol = pd.read_csv(    f"{wd_path}masters/delta_volume_evolution_ensemble_average.csv")gdf = gpd.GeoDataFrame(aggregated_ds, geometry=gpd.points_from_xy(    aggregated_ds['lon'] +0.5 , aggregated_ds['lat'] + 0.5),    crs="EPSG:4326")gdf_volume = gpd.GeoDataFrame(aggregated_ds_vol, geometry=gpd.points_from_xy(    aggregated_ds_vol['lon'] +0.5 , aggregated_ds_vol['lat'] + 0.5),    crs="EPSG:4326")""" Plot shaded relief """fig, ax = plt.subplots(figsize=(15,12), subplot_kw={                       'projection': ccrs.PlateCarree()})dem_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/02. DEM/ETOPO2v2c_f4.nc"with xr.open_dataset(dem_path) as ds:    dem=ds['z']# Print dataset info to find variable namesdem = dem.where((dem.x >= 53.5) & (dem.x <= 116) & (dem.y >= 15) & (dem.y <= 56), drop=True)dx =(dem['x'][1]-dem['x'][0]).valuesdy =(dem['y'][1]-dem['y'][0]).valuesz = dem.valuesx, y = np.meshgrid(dem.x, dem.y)# Create a LightSource object for hillshadingls = LightSource(azdeg=315, altdeg=45)# Overlay the hillshade (for relief effect)im = ax.imshow(ls.hillshade(z, vert_exag=10, dx=0.01, dy=0.01),extent=[dem.x.min(), dem.x.max(), dem.y.min(), dem.y.max()],          transform=ccrs.PlateCarree(), alpha=0.1, cmap="grey_r")  # Adjust alpha for effect# Add geographical features# ax.add_feature(cfeature.COASTLINE, linewidth=0.7)# ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle="--")# ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='none')ax.add_feature(cfeature.RIVERS, edgecolor='blue', facecolor='none', linewidth=0.6, alpha=0.6)# Overlay land to hide ocean data (acts like a visual mask)# ax.add_feature(cfeature.LAND, facecolor='white', zorder=10)"""Set up plot incl shapefile"""subregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"subregions = gpd.read_file(subregions_path).to_crs('EPSG:4326')ax.spines['geo'].set_visible(False)# Optionally, remove gridlines# Remove gridlines properlygl = ax.gridlines(draw_labels=False, visible=False)  # Create gridlines without labelsgl.xlines = False  # Remove longitude linesgl.ylines = False  # Remove latitude lines""" Plot subregions"""# define movements for the annotation of subregions - for letters# movements = {#     '13-01': [0, -0.2], #A#     '13-02': [4.8, 2], ##     '13-03': [-0.2, -0.6], #B#     '13-04': [-0.6, 0], #C#     '13-05': [-4.6,2.5],#-0.2, 0.8], #E#     '13-06': [-10, 0], #F#     '13-07': [-1, 1], #G#     '13-08': [-2, -0.5], #J#     '13-09': [1.5, 0], #O#     '14-01': [0.5, -0.8], #H#     '14-02': [2,-1.8],#[1, -0.4], #I#     '14-03': [3.4, -3.5], #K#     '15-01': [0, 0], #L#     '15-02': [-2, -1], #M#     '15-03': [3.5, 4.5], #N# }#movements for subregion names MARKERmovements = {    '13-01': [0, -0.7], #A    '13-02': [-2.7, 1], #    '13-03': [1.2, -0.8], #B    '13-04': [-2.6, -1], #C    '13-05': [-4.6,0.5],#-0.2, 0.8], #E    '13-06': [-14, 1], #F    '13-07': [-5.5, 2.2], #G    '13-08': [-8.5, -5.2], #J    '13-09': [2, 2], #O    '14-01': [1.5, -0.8], #H    '14-02': [2,-1.8],#[1, -0.4], #I    '14-03': [3.9, -2.5], #K    '15-01': [2, 0], #L    '15-02': [-2.8, -0.2], #M    '15-03': [5, 4.5], #N}# annotate subregions# i=0   # Create a ListedColormap with some colors (example)""" Include the Mass Balance plot including color bar"""h_leg = 0.65w_leg = 0.646cax = fig.add_axes([0.76, h_leg-0.015, 0.03, 0.06])  # [left, bottom, width, height]scatter_data = "B"# Define vmin and vmax for the color scale (asymmetric)vmin, vmax = -0.2, 0.7  # Asymmetric rangestart =(abs(vmin)/(vmax-vmin))#Trim cmap so that white is at 0n_colors=256half=n_colors//2original_cmap = plt.cm.bwr.reversed()  # Blue-White-Red colormap# colors_trimmed = [#     (0.0, "#fdc1c5"),   # lighter red at -0.2#     (0.222, "white"),   # white at 0.0 → (0 - (-0.2)) / (0.7 - (-0.2)) = ~0.222#     (1.0, "#0203e2")    # dark blue at 0.7# ]colors_trimmed = [    (0.0,  "#fdc1c5"),   # Light red at far negative    (0.222, "white"),    # White at 0    (0.5, "#6a79f7"),   # Deep blue at moderate positive #0203e2 0.65 limit before    (1.0,  "#110954")    # Purple at strong positive    ]# Create custom colormapcustom_cmap = LinearSegmentedColormap.from_list("custom_trimmed_rwb", colors_trimmed, N=256)# Normalize to match data rangenorm = Normalize(vmin=-0.2, vmax=0.7)# Create the colorbarcbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm,cmap=custom_cmap ),                    cax=cax, orientation="vertical")# Define **evenly spaced** tick positionsnum_ticks = 2  # Adjust as neededtick_values = np.linspace(vmin, vmax, num_ticks)  # Ensures equal spacing# Apply ticks and labelscbar.set_ticks(tick_values)cbar.set_ticklabels([f'{b:.1f}' for b in tick_values])# Define the boundaries for each color blockcbar.ax.tick_params(labelsize=18) # Set the colorbar labelcbar.set_label('∆B$_{Irr}$ \nm yr$^{-1}$', fontsize=18, ha='left', rotation=0, labelpad=0, linespacing=0.8)cbar.ax.yaxis.label.set_position((1.1, 1.4))  # (X, Y) - Adjust Y to move upcbar.ax.yaxis.set_label_position('left')gdf["marker_size"]=(np.sqrt(gdf['rgi_area_km2'])*5)**1.3scatter = ax.scatter(gdf.geometry.x, gdf.geometry.y,                     s=gdf['marker_size'], c=gdf['B_delta'], cmap=custom_cmap, norm=norm, edgecolor='k', alpha=0.8)#plot subregion linesfor attribute, subregion in subregions.groupby('o2region'):   # Uncomment for coloring of specific subregions    linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"    # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,    subregion.plot(ax=ax, edgecolor='black', linewidth=1.2,                  facecolor='none') # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion    # i+=1    """ Add volume legend"""custom_sizes = [200, 2000]#500, 1000, 2000, 3000]  # Example sizes for the legend (area)size_labels = [f"{size:.0f}" for size in custom_sizes]  # Create labels for sizes# Create legend handles (scatter points with different sizes)legend_handles = [    plt.scatter([], [], s=(np.sqrt(size)*5)**1.3, edgecolor='k', facecolor='none')    for size in custom_sizes]  # Adjust size factor if neededtext_handles = [Line2D([0], [0], linestyle="none", label=label) for label in size_labels]# Create a separate axis for the legendcax_legend = fig.add_axes([w_leg -0.015, h_leg, 0.15, 0.1])  # [left, bottom, width, height]# Remove axis visualscax_legend.set_frame_on(False)  # Hide framecax_legend.set_xticks([])  # Remove x-tickscax_legend.set_yticks([])  # Remove y-ticks# Add legend to the separate axislegend = cax_legend.legend(    legend_handles, size_labels, loc="center",    fontsize=18, ncol=1,frameon=False, title="Total Area \n km$^2$", title_fontsize=18, scatterpoints=1, labelspacing=1)    # handler_map={tuple: HandlerTuple(ndivide=None)}, labelspacing=0.3, columnspacing=1, handletextpad=0.3)# Adjust legend marker positions (stacked effect)"""Comment from here to remove all panels"""""" Add call out plots for the different regions"""# Define and iterate over grid layoutlayout = [["13-01", "13-03", "13-04", "14-02", "13-05", "13-06"], ["13-02", "", "", "","", "13-07"], [    "14-01", "", "", "","", "13-08"], ["","","", "","", "13-09"], ["","","14-03", "15-01", "15-02", "15-03"]]w = 0.14w_space=0.014#0.035h = w/0.17*0.17#w/0.17*0.15 h_space=0.04 #0.06start_x=0.07#-0.12start_y = 0.82 #0.9y_buffer=0.03 #0.05nr_cols=6nr_rows=5# grid_positions = [[-0.15+ col * (w + 0.06), 0.82 - (h + 0.09) * row - 0.05, w, h]#                   if layout[row][col] else None for row in range(4) for col in range(5)]grid_positions = [[start_x + col * (w + w_space), start_y - (h + h_space) * row - y_buffer, w, h]                  if layout[row][col] else None for row in range(nr_rows) for col in range(nr_cols)]x_min, x_max = start_x, start_x + nr_cols * (w + w_space)y_min, y_max = start_y, start_y - (h + h_space) * nr_rows - y_buffersubregion_ids = list(string.ascii_uppercase)  i=0# print(subregion_ids)  #change in case of median/mean# ds_subregions = xr.open_dataset(f"{wd_path}masters/master_volume_ts_subregions_members.nc")# global_full = xr.open_dataset(f"{wd_path}masters/master_volume_ts_global_members.nc")ds_subregions = xr.open_dataset(f"{wd_path}masters/master_volume_ts_subregions_members_median.nc")global_full = xr.open_dataset(f"{wd_path}masters/master_volume_ts_global_members_median.nc")for idx, pos in enumerate(grid_positions):    if pos:        subregion_id=subregion_ids[i]        i+=1        # ax_callout = fig.add_axes(pos, ylim=(-32,20),facecolor='white')#'#E9E9E9'whitesmoke            region_id = layout[idx // nr_cols][idx % nr_cols] #index columns and rows, // is rounded by full nrs                        # # Find the corresponding subregion to add axes        subregion = subregions[subregions['o2region'] == region_id] #convert to meters        #load data in the plots        subregion_ds = master_ds_avg[master_ds_avg['rgi_subregion'].str.contains(            f"{region_id}")]        mask = (subregion_ds["rgi_subregion"] == region_id)        subregion_name = subregion_ds.full_name.iloc[0] #if error reload data above        # Baseline and model plotting        baseline_path = os.path.join(            wd_path, "summary", f"climate_run_output_baseline_W5E5.000.nc")        baseline = xr.open_dataset(baseline_path, engine='netcdf4')        rgi_ids_in_baseline = baseline['rgi_id'].values        matching_rgi_ids = np.intersect1d(            rgi_ids_in_baseline, subregion_ds.rgi_id.values)        baseline_filtered = baseline.sel(rgi_id=matching_rgi_ids)        # Check if there are any matching rgi_id values        # Ensure that rgi_id values exist in both datasets                initial_volume = baseline_filtered['volume'].sum(dim="rgi_id")[0].values        # Plot model member data        ax_callout.plot(baseline_filtered["time"].values, baseline_filtered['volume'].sum(dim="rgi_id")/initial_volume*100-100,                        label="W5E5.000", color=colors['irr'][0], linewidth=3, zorder=15)        filtered_member_data = []                        # Temporarily commented to speed up the plotting        for m, model in enumerate(models_shortlist):            for j in range(members_averages[m]):                sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                # with xr.open_dataset(os.path.join(                #     sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')) as climate_run_output:                #     climate_run_output = climate_run_output.where(                #         climate_run_output['rgi_id'].isin(subregion_ds.rgi_id.values), drop=True)                climate_run_output = ds_subregions.sel(member=sample_id).sel(subregion=region_id)                 # ax_callout.plot(climate_run_output["time"].values, climate_run_output["volume"]/initial_volume*100-100, label=sample_id, color="grey", linewidth=1, linestyle="dotted")                # ax_callout.plot(climate_run_output["time"].values, climate_run_output["volume"].sum(                #     dim="rgi_id")/initial_volume*100-100, label=sample_id, color="grey", linewidth=1, linestyle="dotted")                # filtered_member_data.append(                #     climate_run_output["volume"].sum(dim="rgi_id").values/initial_volume*100-100)                        # Mean and range plotting        # mean_values = ds_subregions.sel(subregion=region_id).sel(member='14-member-avg').volume.values/initial_volume*100-100 #np.mean(filtered_member_data, axis=0).flatten()        median_values = ds_subregions.sel(subregion=region_id).sel(member='14-member-med').volume.values/initial_volume*100-100 #np.mean(filtered_member_data, axis=0).flatten()        min_values = ds_subregions.sel(subregion=region_id).min(dim="member").volume.values/initial_volume*100-100 #np.min(filtered_member_data, axis=0).flatten()        max_values = ds_subregions.sel(subregion=region_id).max(dim="member").volume.values/initial_volume*100-100# np.max(filtered_member_data, axis=0).flatten()        std_values = ((ds_subregions.sel(subregion=region_id).volume / initial_volume) * 100 - 100).std(dim="member").values        ax_callout.plot(climate_run_output["time"].values, median_values,                        color=colors['noirr'][0], linestyle='dashed', lw=3, label=f"{sum(members_averages)}-member average")                #Change these lines if you want to switch from mean to median        # ax_callout.fill_between(        #     climate_run_output["time"].values, min_values, max_values, color=colors['noirr'][1], alpha=0.3)#color="lightblue", alpha=0.3)        ax_callout.fill_between(climate_run_output["time"].values, (median_values-std_values), (median_values+std_values), color=colors['noirr'][1], alpha=0.3, label=r"Historical NoIrr 1$\sigma$" )        ax_callout.plot(climate_run_output["time"].values, np.ones(len(climate_run_output["time"]))*100-100, color="black", linewidth=1, linestyle="dashed")                # ax_callout.fill_between(climate_run_output["time"].values, (mean_values-std_values), (mean_values+std_values), color=colors['noirr'][1], alpha=0.3, label=r"Historical NoIrr 1$\sigma$" )        # ax_callout.plot(climate_run_output["time"].values, np.ones(len(climate_run_output["time"]))*100-100, color="black", linewidth=1, linestyle="dashed")        Subplot formatting        if region_id=="13-02":            subregion_title ="Pamir"        elif region_id=="13-04":            subregion_title ="East Tien Shan"        elif region_id=="13-06":            subregion_title ="East Kun Lun"        else:            subregion_title=subregion_name                    ax_callout.set_title(f"{subregion_title}", bbox=dict(            facecolor='none', edgecolor='none', pad=1), fontsize=18) # fontweight="bold"        # Count the number of glaciers (assuming each 'rgi_id' represents a glacier)        glacier_count = subregion_ds['rgi_id'].nunique()        subregion_volume = round(subregion_ds['rgi_volume_km3'].sum())        # Add number of glaciers as a text annotation in the lower left corner        ax_callout.text(0.05, 0.05, f"#:{glacier_count}, V:{subregion_volume}",                        transform=ax_callout.transAxes, fontsize=18, verticalalignment='bottom', fontstyle='italic')        ax_callout.text(0.05, 0.83, region_id,#f"{subregion_id}",                        transform=ax_callout.transAxes, fontsize=18, verticalalignment='bottom'  , ha='left', color='black') # fontweight='bold',        #change for letters instead of numbers        # ax_callout.text(0.85, 0.8, f"{subregion_id}",        #                 transform=ax_callout.transAxes, fontsize=18, verticalalignment='bottom'  , fontweight='bold')                boundary = subregion.geometry.boundary.iloc[0]        if isinstance(boundary, MultiLineString):            boundary = list(boundary.geoms)[0]        boundary_coords = list(boundary.coords)        boundary_x, boundary_y = boundary_coords[0]  # First point on the boundary        boundary_x -= movements[region_id][0]        boundary_y -= movements[region_id][1]        # Annotate or place text near the boundary        # ax.text(boundary_x, boundary_y, f"{region_id}", #subregion_id for letters        #         horizontalalignment='center', fontsize=18, color='black', linecolor='white', fontweight='bold') #letter fontsize 18        ax.text(            boundary_x, boundary_y, f"{region_id}",  # or subregion_id if letters            ha='center', va='center',            fontsize=18, color='black', fontweight='bold',            path_effects=[                path_effects.Stroke(linewidth=6, foreground='white'),  # outline color                path_effects.Normal()            ]        )                ax_callout.tick_params(axis='x', labelbottom=False)        if idx % nr_cols != 0 and subregion_name != "West Himalaya":            ax_callout.tick_params(axis='y', labelleft=False)        ax_callout.xaxis.set_tick_params(labelsize=18)        ax_callout.yaxis.set_tick_params(labelsize=18)                    callout_x, callout_y, callout_w, callout_h = pos# Create a new figure for the\small legend plot# fig_legend = fig.add_axes([start_x, 0.069, w*2.1, h*2.2], ylim=(-8,4), facecolor="white")##e9e9e9")  # make twice as large as the callout plots #w*2.4, h*2.15fig_legend = fig.add_axes([start_x, 0.069, w*1.9, h*2.2], ylim=(-8,4), facecolor="white")##e9e9e9")  # make twice as large as the callout plots #w*2.4, h*2.15total_initial_volume = baseline['volume'].sum(dim="rgi_id")[0].valuesvol_display="relative"fig_legend.plot(baseline["time"].values, baseline['volume'].sum(dim="rgi_id")/total_initial_volume*100-100,                label="Historical, W5E5", color= colors['irr'][0], linewidth=3, zorder=15)member_data = []# Temporarily commented to speed up the plottingfor m, model in enumerate(models_shortlist):    for j in range(members_averages[m]):        if j ==1 and m==1:            legendlabel = 'Historical NoIrr member'        else:            legendlabel = '_nolegend_'        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"        # with xr.open_dataset(os.path.join(        #     sum_dir, f'climate_run_output_perturbed_{sample_id}.nc')) as climate_run_output:        climate_run_output = global_full.sel(member=sample_id)        fig_legend.plot(climate_run_output["time"].values, climate_run_output["volume"]/total_initial_volume*100-100, label=legendlabel, color=colors['noirr'][0], linewidth=1, linestyle="dotted")            # fig_legend.plot(climate_run_output["time"].values, climate_run_output["volume"].sum(            #     dim="rgi_id")/total_initial_volume*100-100, label=legendlabel, color=colors['noirr'][0], linewidth=2, linestyle="dotted")        # member_data.append(        #     climate_run_output["volume"].sum(dim="rgi_id").values/total_initial_volume*100-100)# Mean and range plotting# mean_values = global_full.sel(member='14-member-avg').volume.values/total_initial_volume*100-100 #np.mean(member_data, axis=0).flatten()median_values = global_full.sel(member='14-member-med').volume.values/total_initial_volume*100-100 #np.mean(member_data, axis=0).flatten()min_values = global_full.volume.min(dim='member').values/total_initial_volume*100-100 #np.min(member_data, axis=0).flatten()max_values = global_full.volume.max(dim='member').values/total_initial_volume*100-100 #np.max(member_data, axis=0).flatten()std_values = ((global_full.volume / global_full.volume.isel(time=0)) * 100 - 100).std(dim="member").values# std_values = global_full.volume.std(dim='member').values #np.std(member_data, axis=0).flatten()  # Replace min with std# fig_legend.plot(climate_run_output["time"].values, mean_values,#                 color=colors['noirr'][0], linestyle='dashed', lw=3, label=f"Historical NoIrr avg.")fig_legend.plot(climate_run_output["time"].values, median_values,                color=colors['noirr'][0], linestyle='dashed', lw=3, label=f"Historical NoIrr med.")fig_legend.fill_between(climate_run_output["time"].values, (median_values-std_values), (median_values+std_values), color=colors['noirr'][1], alpha=0.3, label=r"Historical NoIrr 1$\sigma$" )fig_legend.plot(climate_run_output["time"].values, np.zeros(len(climate_run_output["time"])), color="k", linewidth=1, linestyle="dashed")total_nr = len(master_ds_avg)total_volume = round(master_ds_avg['rgi_volume_km3'].sum())# Annotate number of glaciers (just for example)fig_legend.text(0.05, 0.05, f'#:{total_nr}, V:{total_volume} km$^3$', transform=fig_legend.transAxes,                fontsize=18, verticalalignment='bottom', horizontalalignment='left',                bbox=dict(facecolor='none', edgecolor='none', pad=2), fontstyle='italic')handles, labels = fig_legend.get_legend_handles_labels()# Define the custom order (indices correspond to handles/labels order)custom_order = [0,3,2,1]#[0,1,2]#,1]#[0,3,2,1]  # Example: Rearrange items by index# Apply the custom orderfig_legend.legend([handles[i] for i in custom_order],           [labels[i] for i in custom_order],           loc='lower left', bbox_to_anchor=(0.0, 0.1), ncol=1, fontsize=18,                            frameon=False, labelspacing=0.4)# Set labels for axes# fig_legend.set_xlabel('Time', fontsize=20, labelpad=10 )#fontweight="bold"fig_legend.set_ylabel('∆ Volume [%, vs. 1985]', fontsize=18, labelpad=10) #, fontweight="bold"fig_legend.set_title('High Mountain Asia', fontsize=18, fontweight='bold')fig_legend.xaxis.set_tick_params(labelsize=18)fig_legend.yaxis.set_tick_params(labelsize=18)# Remove tick marks but keep the tick labelsfig.tight_layout()fig_folder = os.path.join(fig_path)#, "04. Map"os.makedirs(fig_folder, exist_ok=True)plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR.png", dpi=300, bbox_inches="tight", pad_inches=0.1)# plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_2000_2014.png", dpi=300, bbox_inches="tight", pad_inches=0.1)plt.show()#%% Cell 3a: map plot - prepare data per model"""Process master for map plot """# Load datasetsdf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg_B_hugo.csv")models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]for m,model in enumerate(models_shortlist):    print(model)    master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL                   (df['sample_id'].str.startswith('IPSL'))]    # master_ds = df#df[(~df['sample_id'].str.endswith('0')) ]    master_ds = master_ds[(master_ds['sample_id'].str.startswith(f'{model}'))] #Temporary    master_ds = master_ds.reset_index(drop=True)        # Normalize values    # divide all the B values with 1000 to transform to m w.e. average over 30 yrs    # master_ds[['B_noirr', 'B_irr', 'B_delta_irr', 'B_cf',  "B_delta_cf"]] /= 1000    master_ds.loc[:, ['B_noirr', 'B_irr', 'B_delta']] /= 1000        master_ds = master_ds[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                           'rgi_area_km2', 'rgi_volume_km3', 'sample_id', 'B_noirr', 'B_irr', 'B_delta']]        # Define custom aggregation functions for grouping over the 11 member data    aggregation_functions = {        'rgi_region': 'first',        'rgi_subregion': 'first',        'full_name': 'first',        'cenlon': 'first',        'cenlat': 'first',        'rgi_date': 'first',        'rgi_area_km2': 'first',        'rgi_volume_km3': 'first',        'B_noirr': 'mean',        'B_irr': 'mean',        'B_delta': 'mean',        'sample_id': 'first'    }            master_ds_avg = master_ds.groupby(['rgi_id'], as_index=False).agg({        'B_delta': 'mean',        'B_noirr': 'mean',        'B_irr': 'mean',        # lamda is anonmous functions, returns 11 member average        'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id']}    })        master_ds_avg.to_csv(        f"{wd_path}masters/master_lon_lat_rgi_id_{model}.csv")    # Aggregate data for scatter plot    master_ds_avg['grid_lon'] = np.floor(master_ds_avg['cenlon'])    master_ds_avg['grid_lat'] = np.floor(master_ds_avg['cenlat'])            # Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member average    aggregated_ds = master_ds_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({        'B_delta': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'B_irr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'B_noirr': lambda x: (x * master_ds.loc[x.index, 'rgi_area_km2']).sum() / master_ds.loc[x.index, 'rgi_area_km2'].sum(),        'rgi_area_km2': 'sum',  # Sum for area        'rgi_volume_km3': 'sum',  # Sum for volume        # 'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in master_ds.columns if col not in ['B_noirr', 'B_irr', 'B_delta', 'sample_id', 'rgi_area_km2', 'rgi_volume_km3']}    })        aggregated_ds.rename(        columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)    print(aggregated_ds.head())        aggregated_ds.to_csv(        f"{wd_path}masters/complete_master_processed_for_map_plot_{model}.csv")    # f"{wd_path}masters/complete_master_processed_for_map_plot_E3SM.csv")#%% Cell 3b: map plot - Process ∆ volume data, per modelmembers_averages = [2, 3,   5, 1,3] models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]climate_run_output_irr = xr.open_dataset(os.path.join(    sum_dir, f'climate_run_output_baseline_W5E5.000.nc'))initial_volume=climate_run_output_irr.volume.sel(time=1985) #only select first year for initial valuedelta_volume_irr = climate_run_output_irr.volume - initial_volumemaster_df = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv")[['rgi_id', 'cenlon','cenlat', 'rgi_area_km2']]all_datasets = []for m, model in enumerate(models_shortlist): #Load the data for other models and calculate respective loss for each     for j in range(members_averages[m]):        sample_id = f"{model}.00{j + 1}" if members_averages[m] > 1 else f"{model}.000"                climate_run_output_noirr = xr.open_dataset(os.path.join(            sum_dir, f'climate_run_output_perturbed_{sample_id}.nc'))        delta_volume_noirr = climate_run_output_noirr.volume - initial_volume        delta_volume_noirr_rel = xr.where(delta_volume_irr != 0, delta_volume_noirr / delta_volume_irr, 0)                        yearly_mean_volume = climate_run_output_noirr.mean(dim='time')                ds_new = xr.Dataset({            "delta_volume_irr": delta_volume_irr,            "delta_volume_noirr": delta_volume_noirr,            "sample_id": [sample_id],            "delta_volume_noirr_rel": delta_volume_noirr_rel        })        # Append to list        all_datasets.append(ds_new)            volume_change_ds = xr.concat(all_datasets, dim="sample_id")    volume_change_ds["delta_volume_noirr_rel"].attrs["description"] = "Relative volume change ∆ NoIrr (resp. Irr-1985)/ ∆ Irr (resp. Irr-1985)"    # Group by rgi_id and time, then compute the mean for numeric variables    volume_ds_avg = volume_change_ds.mean(dim="sample_id").mean(dim="time") #30-yr average values for all 14 members in the model ensemble            opath_bymember = os.path.join(wd_path,"masters", 'delta_volume_evolution_bymember.nc')    volume_change_ds.to_netcdf(opath_bymember)        opath_averaged = os.path.join(wd_path,"masters", f'delta_volume_evolution_ensemble_average_{model}.nc')    opath_averaged_csv = os.path.join(wd_path,"masters", f'delta_volume_evolution_ensemble_average_{model}.csv')    print(opath_averaged_csv)    print(model)        volume_ds_avg.to_netcdf(opath_averaged)        volume_df_avg = volume_ds_avg.to_dataframe()            volume_df_avg = volume_df_avg.merge(        master_df, on="rgi_id", how="left")        # Aggregate data for scatter plot    volume_df_avg['grid_lon'] = np.floor(volume_df_avg['cenlon'])    volume_df_avg['grid_lat'] = np.floor(volume_df_avg['cenlat'])        # Aggregate dataset, area-weighted BDelta Birr and Bnoirr, Sample id is replaced by 11 member average    volume_df_avg = volume_df_avg.groupby(['grid_lon', 'grid_lat'], as_index=False).agg({        'delta_volume_irr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'delta_volume_noirr': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'delta_volume_noirr_rel': lambda x: (x * volume_df_avg.loc[x.index, 'rgi_area_km2']).sum() / volume_df_avg.loc[x.index, 'rgi_area_km2'].sum(),        'rgi_area_km2': 'sum',  # Sum for area        # 'sample_id': lambda _: "11 member average",        # take first value for all columns that are not in the list        **{col: 'first' for col in volume_df_avg.columns if col not in ['delta_volume_irr', 'delta_volume_noirr', 'delta_volume_noirr_rel', 'rgi_area_km2']}    })        volume_df_avg.rename(        columns={'grid_lon': 'lon', 'grid_lat': 'lat'}, inplace=True)        volume_df_avg.to_csv(opath_averaged_csv)#%% Cell 3c: Create map plot per model subfigures - appendixmembers_averages = [2, 3,    5 ,1,3]models_shortlist = ["E3SM", "CESM2",    "CNRM", "IPSL-CM6","NorESM"]fig,axes = plt.subplots(2,3,figsize=(13,6),subplot_kw={                       'projection': ccrs.PlateCarree()})axes=axes.flatten()for m,model in enumerate(models_shortlist):    ax = axes[m]        """ Load data"""    aggregated_ds = pd.read_csv(        f"{wd_path}masters/complete_master_processed_for_map_plot_{model}.csv")        aggregated_ds_vol = pd.read_csv(        f"{wd_path}masters/delta_volume_evolution_ensemble_average_{model}.csv")        gdf = gpd.GeoDataFrame(aggregated_ds, geometry=gpd.points_from_xy(        aggregated_ds['lon'] +0.5 , aggregated_ds['lat'] + 0.5),        crs="EPSG:4326")        gdf_volume = gpd.GeoDataFrame(aggregated_ds_vol, geometry=gpd.points_from_xy(        aggregated_ds_vol['lon'] +0.5 , aggregated_ds_vol['lat'] + 0.5),        crs="EPSG:4326")        """ Plot shaded relief """    # fig, ax = plt.subplots(figsize=(15,12), subplot_kw={    #                        'projection': ccrs.PlateCarree()})    dem_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/02. DEM/ETOPO2v2c_f4.nc"    with xr.open_dataset(dem_path) as ds:        dem=ds['z']        # Print dataset info to find variable names        dem = dem.where((dem.x >= 65.5) & (dem.x <= 108) & (dem.y >= 23) & (dem.y <= 48), drop=True)    # dem = dem.where((dem.x >= 54.5) & (dem.x <= 115) & (dem.y >= 16) & (dem.y <= 55), drop=True)    dx =(dem['x'][1]-dem['x'][0]).values    dy =(dem['y'][1]-dem['y'][0]).values    z = dem.values    x, y = np.meshgrid(dem.x, dem.y)        # Create a LightSource object for hillshading    ls = LightSource(azdeg=315, altdeg=45)            # Overlay the hillshade (for relief effect)    im = ax.imshow(ls.hillshade(z, vert_exag=10, dx=0.01, dy=0.01),extent=[dem.x.min(), dem.x.max(), dem.y.min(), dem.y.max()],              transform=ccrs.PlateCarree(), alpha=0.1, cmap="grey_r")  # Adjust alpha for effect        # Add geographical features    # ax.add_feature(cfeature.COASTLINE, linewidth=0.7)    # ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle="--")    # ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='none')    ax.add_feature(cfeature.RIVERS, edgecolor='blue', facecolor='none', linewidth=0.6, alpha=0.6)        """Set up plot incl shapefile"""            subregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"    subregions = gpd.read_file(subregions_path).to_crs('EPSG:4326')    ax.spines['geo'].set_visible(True)        # Optionally, remove gridlines    # Remove gridlines properly    gl = ax.gridlines(draw_labels=False)  # Create gridlines without labels    gl.xlines = False  # Remove longitude lines    gl.ylines = False  # Remove latitude lines            """ Plot subregions"""    # define movements for the annotation of subregions    movements = {        '13-01': [0, -0.2], #A        '13-02': [4.8, 2], #        '13-03': [-0.2, -0.6], #B        '13-04': [-0.6, 0], #C        '13-05': [-4.6,2.5],#-0.2, 0.8], #E        '13-06': [-10, 0], #F        '13-07': [-1, 1], #G        '13-08': [-2, -0.5], #J        '13-09': [1.5, 0], #O        '14-01': [0.5, -0.8], #H        '14-02': [2,-1.8],#[1, -0.4], #I        '14-03': [3.4, -3.5], #K        '15-01': [0, 0], #L        '15-02': [-2, -1], #M        '15-03': [3.5, 4.5], #N    }    # annotate subregions    # i=0           # Create a ListedColormap with some colors (example)    """ Include the Mass Balance plot including color bar"""        h_leg = 0.65    w_leg = 0.646            scatter_data = "B"        # Define vmin and vmax for the color scale (asymmetric)    vmin, vmax = -0.2, 0.7  # Asymmetric range    start =(abs(vmin)/(vmax-vmin))    #Trim cmap so that white is at 0    n_colors=256    half=n_colors//2    original_cmap = plt.cm.bwr.reversed()  # Blue-White-Red colormap        # colors_trimmed = [    #     (0.0, "#fdc1c5"),   # lighter red at -0.2    #     (0.222, "white"),   # white at 0.0 → (0 - (-0.2)) / (0.7 - (-0.2)) = ~0.222    #     (1.0, "#0203e2")    # dark blue at 0.7    # ]    colors_trimmed = [        (0.0,  "#fdc1c5"),   # Light red at far negative        (0.222, "white"),    # White at 0        (0.5, "#6a79f7"),   # Deep blue at moderate positive #0203e2 0.65 limit before        (1.0,  "#110954")    # Purple at strong positive        ]        # Create custom colormap    custom_cmap = LinearSegmentedColormap.from_list("custom_trimmed_rwb", colors_trimmed, N=256)        # Normalize to match data range    norm = Normalize(vmin=-0.2, vmax=0.7)                gdf["marker_size"]=(np.sqrt(gdf['rgi_area_km2'])*5)**1.3/3    scatter = ax.scatter(gdf.geometry.x, gdf.geometry.y,                         s=gdf['marker_size'], c=gdf['B_delta'], cmap=custom_cmap, norm=norm, edgecolor='k', linewidths=0.5, alpha=0.8)    #plot subregion lines                for attribute, subregion in subregions.groupby('o2region'):           # Uncomment for coloring of specific subregions            linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"        # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,        subregion.plot(ax=ax, edgecolor='black', linewidth=0.5,                      facecolor='none') # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion        # i+=1    ids = ['a', 'b', 'c', 'd', 'e'][m]    # ax.set_title(f'{ids} {model}', fontsize=14, fontweight='bold', loc='right')    ax.text(0.98,0.93,ids, transform=ax.transAxes,fontsize=14, fontweight='bold', ha='right')    ax.text(0.02,0.93,f'{model} ({members_averages[m]})', transform=ax.transAxes,fontsize=14, fontweight='bold', ha='left')        # for spine in ax.spines.values():    #     spine.set_edgecolor('black')  # border color    #     spine.set_linewidth(2)        # border width """Create the colorbar"""cax = fig.add_axes([0.7, 0.13, 0.05, 0.17])  # [left, bottom, width, height]# cax = fig.add_axes([0.76, h_leg-0.015, 0.03, 0.06])  # [left, bottom, width, height]cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm,cmap=custom_cmap ),                    cax=cax, orientation="vertical")# Define **evenly spaced** tick positionsnum_ticks = 2  # Adjust as neededtick_values = np.linspace(vmin, vmax, num_ticks)  # Ensures equal spacing# Apply ticks and labelscbar.set_ticks(tick_values)cbar.set_ticklabels([f'{b:.1f}' for b in tick_values])# Define the boundaries for each color blockcbar.ax.tick_params(labelsize=14) # Set the colorbar labelcbar.set_label('∆B$_{Irr}$ \nm [yr$^{-1}]$', fontsize=14, ha='left', rotation=0, labelpad=0, linespacing=1)cbar.ax.yaxis.label.set_position((1.1, 1.2))  # (X, Y) - Adjust Y to move up yprev = 1.4cbar.ax.yaxis.set_label_position('left')       """ Add volume legend"""custom_sizes = [200, 2000]#500, 1000, 2000, 3000]  # Example sizes for the legend (area)size_labels = [f"{size:.0f}" for size in custom_sizes]  # Create labels for sizes# Create legend handles (scatter points with different sizes)legend_handles = [    axes[5].scatter([], [], s=(np.sqrt(size)*5)**1.3/3, edgecolor='k', facecolor='none')    for size in custom_sizes]  # Adjust size factor if neededtext_handles = [Line2D([0], [0], linestyle="none", label=label) for label in size_labels]# Create a separate axis for the legendcax_legend = axes[5] #fig.add_axes([w_leg -0.015, h_leg, 0.15, 0.1])  # [left, bottom, width, height]# Remove axis visualscax_legend.set_frame_on(False)  # Hide framecax_legend.set_xticks([])  # Remove x-tickscax_legend.set_yticks([])  # Remove y-ticks# Add legend to the separate axislegend = cax_legend.legend(    legend_handles, size_labels, loc="center",    fontsize=14, ncol=1,frameon=False, title="Total Area \n [km$^2]$", title_fontsize=14, scatterpoints=1, labelspacing=1.5, bbox_to_anchor=(0.7,0.59))# Remove tick marks but keep the tick labelsfig.tight_layout()fig_folder = os.path.join(fig_path)#, "04. Map"os.makedirs(fig_folder, exist_ok=True)plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_permodel.png", dpi=300, bbox_inches="tight", pad_inches=0.1)plt.tight_layout# plt.savefig(f"{fig_folder}/Map_Plot_sA_c{scatter_data}_boxV_IRR_2000_2014.png", dpi=300, bbox_inches="tight", pad_inches=0.1)plt.show()# %% Cell 4a: Create nan mask for ids without succesful comitted runmembers = [3, 4, 6, 4, 1, 1]models = ["E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]#, "IPSL-CM6"]overview_df = pd.DataFrame()for m, model in enumerate(models):    for member in range(members[m]):        df_tot = pd.DataFrame()        sample_id = f"{model}.00{member}"        for f, filepath in enumerate([f"climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_comitted_random.nc"]):            calendar_year = 2014            if model != "W5E5":                filepath = [f'climate_run_output_perturbed_{sample_id}.nc',                            f'climate_run_output_perturbed_{sample_id}_comitted_random.nc'][f]                # f'climate_run_output_perturbed_{sample_id}_comitted_cst.nc'][f]            ds = xr.open_dataset(os.path.join(                # log_dir, f"stats_perturbed_{sample_id}_climate_run_test.csv"))                sum_dir, filepath))            ds_zeros = ds.volume.sel(time=2014).to_dataframe()            df_individual = ds_zeros[["calendar_year", "volume"]].reset_index()            # df_tot = pd.concat([df_tot, df_individual], ignore_index=True)            if df_tot.empty:                df_tot = df_individual            if f == 1:                # Merge based on rgi_id                df_tot = pd.merge(                    df_tot,                    df_individual[["rgi_id", "volume", "calendar_year"]],                    on="rgi_id",                    how="outer",                    suffixes=("", "_noirr")                )            # if f == 2:            #     # Merge based on rgi_id            #     df_tot = pd.merge(            #         df_tot,            #         df_individual[["rgi_id", "volume", "calendar_year"]],            #         on="rgi_id",            #         how="outer",            #         suffixes=("", "_noforcing")            #     )        df_zero_volume = df_tot[            pd.isna(df_tot["volume_noirr"])|  pd.isna(df_tot["volume"])] #| pd.isna(df_tot["volume_noforcing"]) |    overview_df = pd.concat([overview_df, df_zero_volume], ignore_index=True)unique_rgi_ids = overview_df['rgi_id'].unique()# print(len(unique_rgi_ids))unique_rgi_ids = pd.DataFrame(unique_rgi_ids, columns=['rgi_ids'])unique_rgi_ids.to_csv(os.path.join(    wd_path, 'masters', 'nan_mask_comitted_random.csv'))    # wd_path, 'masters', 'nan_mask_comitted_random_noIPSL.csv'))# Save the result to a CSVoutput_path = os.path.join(    wd_path, "masters", "nan_mask_all_models_volume_comitted_random.csv")# wd_path, "masters", "nan_mask_all_models_volume_comitted_random_noIPSL.csv")unique_rgi_ids.to_csv(output_path, index=False)#%% Cell 4b: Create comitted mass loss plot transient with panelsregions = [13, 14, 15]subregions = [9, 3, 3]fig, ax = plt.subplots(figsize=(15,10))  # create a new figuremembers_averages = [2, 3,  3,  6, 1]models_shortlist = ["E3SM", "CESM2",  "NorESM",  "CNRM", "IPSL-CM6"]# define the variables for p;lottingfactors = [10**-9]variable_names = ["Volume"]variable_axes = ["Volume compared to 1985 All Forcings scenario [%]"]use_multiprocessing = Falsergi_ids_test=[]subset_gdirs = gdirs_3r_a1for gdir in subset_gdirs:     rgi_ids_test.append(gdir.rgi_id)      #Exclude error idsnan_mask = pd.read_csv(os.path.join(    wd_path, "masters", "nan_mask_all_models_volume_comitted_random.csv")).rgi_ids# Remove duplicates if needednan_mask = set(pd.DataFrame({'rgi_id': nan_mask.unique()}).rgi_id.to_numpy())rgi_ids_test = [rgi_id for rgi_id in rgi_ids_test if rgi_id not in nan_mask]   aggregated_ds_total = pd.read_csv(    f"{wd_path}masters/master_lon_lat_rgi_id.csv") #load dataset - needed to filter rgi_ids per region    aggregated_ds = aggregated_ds_total[aggregated_ds_total['rgi_id'].isin(rgi_ids_test)]plt.rcParams.update({'font.size': 14})#create a figure consisting of 1 large plot and several smaller ones for every subregionfig = plt.figure(figsize=(15,8))gs = gridspec.GridSpec(4, 6, figure=fig)  # Adjust the grid size as neededax_big = fig.add_subplot(gs[:3, :3])  # Spanning first 2 rows and 3 columnssmall_axes = []positions = [    (0, 3), (0, 4), (0, 5), (1, 3), (1, 4), (1, 5),  # First row right side    (2, 3),(2, 4),(2, 5),  # Third row    (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5)  # Fourth row]axes_dict = {}  # Dictionary to store axes for later useresp_values_dict={}linestyles = ['solid', 'dashed']for f, filepath in enumerate(["climate_run_output_baseline_W5E5.000.nc", f"climate_run_output_baseline_W5E5.000_comitted_random.nc"]):    if f != 0:        run_type = "noirr"        legend_id = "committed"        bar_values = 50        run_label = "NoIrr"    else:        run_type = "irr"        legend_id = ""        bar_values = 20    # load and plot the baseline data    baseline_path = os.path.join(        wd_path, "summary", filepath)    baseline_all = xr.open_dataset(baseline_path)    print(len(baseline_all.rgi_id))    baseline_all = baseline_all.where(        baseline_all.rgi_id.isin(rgi_ids_test), drop=True)        #define the 1985 value for relative volume calculation    if f == 0:        resp_values_dict['Total'] = (baseline_all['volume'].sum(dim="rgi_id")[0].values * factors)[0]                legendtext=f"W5E5.000"    else:        legendtext="W5E5.000, comitted run"    ax_big.plot(baseline_all["time"], (baseline_all["volume"].sum(dim="rgi_id") * factors)/resp_values_dict['Total']*100,            label=legendtext, color=colors[f"irr"][0], linewidth=2, zorder=15, linestyle=linestyles[f])    nan_runs_noirr = []        run_type = "noirr"    legend_id = "" #comitted    bar_values = 50    run_label = "NoIrr"    member_data_noirr_all = []       p=0 #set to zero for plot indices    for reg, region in enumerate(regions):        for sub in range(subregions[reg]):            member_data_noirr_region = []            region_id = f"{region}.0{sub+1}"            print(region_id)            pos = positions[p]            if f==0:                 ax = fig.add_subplot(gs[pos])                ax.set_ylim(0,150)                ax.set_title(region_id)                axes_dict[region_id] = ax                # if p not in {0,3,6,9}:                ax.set_yticks([])                # if p <9:                ax.set_xticks([])            p+=1 #loop trhough all the different subplots                        subregion_ds = aggregated_ds[aggregated_ds['rgi_subregion'].str.contains(                f"{region}.0{sub+1}")]            print(f"{region}.0{sub+1}")            subregion_mask = set(pd.DataFrame({'rgi_id': subregion_ds.rgi_id.unique()}).rgi_id.to_numpy())            # rgi_ids_region = [] #create filter for data not in that region            # for gdir in subset_gdirs:            #      rgi_ids_region.append(gdir.rgi_id)            rgi_ids_region = [rgi_id for rgi_id in rgi_ids_test if rgi_id in subregion_mask]                        #plot baseline data per region            baseline = baseline_all.where(                baseline_all.rgi_id.isin(rgi_ids_region), drop=True) #filter baseline to region            if f == 0:                resp_values_dict[region_id] = baseline['volume'].sum(dim="rgi_id")[0].values * factors                        axes_dict[region_id].plot(baseline["time"], (baseline['volume'].sum(dim="rgi_id") * factors)/resp_values_dict[region_id]*100,                    label=legendtext, color=colors[f"irr"][0], linewidth=2, zorder=15, linestyle=linestyles[f])            #define files for climate runs for comitted and normal run                         for m, model in enumerate(models_shortlist):                for i in range(members_averages[m]):                    sample_id = f"{model}.00{i}"                    filepath = [f'climate_run_output_perturbed_{sample_id}.nc',                                f'climate_run_output_perturbed_{sample_id}_comitted_random.nc',                                ][f]                    climate_run_opath_noirr = os.path.join(                        sum_dir, filepath)  # f'climate_run_output_perturbed_{sample_id}_comitted.nc')                    climate_run_output_noirr_all = xr.open_dataset(                        climate_run_opath_noirr)                    climate_run_output_noirr_selected = climate_run_output_noirr_all.where(                        climate_run_output_noirr_all.rgi_id.isin(rgi_ids_test), drop=True)                    if reg==0: #include the 3-region total climate run data for every model                        member_data_noirr_all.append((climate_run_output_noirr_selected["volume"].sum(                            dim="rgi_id").values)*factors/resp_values_dict['Total']*100)                      climate_run_output_noirr = climate_run_output_noirr_selected.where(                         climate_run_output_noirr_selected.rgi_id.isin(rgi_ids_region), drop=True)                       # add all the summed volumes/areas to the member list, so a multi-member average can be calculated                    member_data_noirr_region.append((climate_run_output_noirr["volume"].sum(                        dim="rgi_id").values)*factors/resp_values_dict[region_id]*100)                                          if members_averages[m] > 1:                        i += 1                        label = None                    else:                        label = "GCM member"                                # stack the member data            stacked_member_data_region = np.stack(member_data_noirr_region)            mean_values_noirr_region = np.median(                stacked_member_data_region, axis=0).flatten()                        # mean_values_noirr = np.mean(stacked_member_data, axis=0).flatten()            if f==0:                labeltext = None#f"{run_label} (14-member avg) {legend_id}"                # labeltext = f"{run_label} ({sum(members_averages)}-member avg) {legend_id}"            else:                labeltext = None                            axes_dict[region_id].plot(climate_run_output_noirr["time"].values, mean_values_noirr_region,                    color=colors[f"{run_type}"][0], linestyle=linestyles[f], lw=2, label=labeltext, zorder=5)            # calculate and plot volume/area 10-member min and max for ribbon            min_values_noirr = np.min(stacked_member_data_region, axis=0).flatten()            max_values_noirr = np.max(stacked_member_data_region, axis=0).flatten()            if f==0:                labeltext = "NoIrr 14-member avg" #f"{run_label} (14-member range) {legend_id}"                labeltext_range="NoIrr 14-member range"                # labeltext = f"{run_label} ({sum(members_averages)}-member range) {legend_id}"            else:                labeltext=label="NoIrr 14-member avg, comitted run"                labeltext_range=None            axes_dict[region_id].fill_between(climate_run_output_noirr["time"].values, min_values_noirr, max_values_noirr,                            color=colors[f"{run_type}"][f], alpha=0.3, label=None, zorder=16)            axes_dict[region_id].axhline(100, color='black', linestyle='--',                        linewidth=1, zorder=1)  # Dashed line at 0                       try:                subregion_name = subregion_ds.full_name.iloc[0]            except:                subregion_name="no ids in subset"            if region_id=="13.02":                subregion_title ="Pamir"            elif region_id=="13.04":                subregion_title ="East Tien Shan"            elif region_id=="13.06":                subregion_title ="East Kun Lun"            else:                subregion_title=subregion_name            print(region_id, subregion_title)                        axes_dict[region_id].set_title(subregion_title, fontweight="bold", bbox=dict(                facecolor='white', edgecolor='none', pad=1), fontsize=14)    stacked_member_data_all = np.stack(member_data_noirr_all)    mean_values_noirr_all = np.median(        stacked_member_data_all, axis=0).flatten()    ax_big.plot(climate_run_output_noirr_all["time"].values, mean_values_noirr_all,            color=colors[f"{run_type}"][0], linestyle=linestyles[f], lw=2, label=labeltext, zorder=5)    # calculate and plot volume/area 10-member min and max for ribbon    min_values_noirr_all = np.min(stacked_member_data_all, axis=0).flatten()    max_values_noirr_all = np.max(stacked_member_data_all, axis=0).flatten()    ax_big.fill_between(climate_run_output_noirr["time"].values, min_values_noirr_all, max_values_noirr_all,                    color=colors[f"{run_type}"][f], alpha=0.3, label=labeltext_range, zorder=16)    ax_big.axhline(100, color='black', linestyle='--',                linewidth=1, zorder=1)  # Dashed line at 0    # Set labels and title for the combined plot    ax_big.set_ylabel("∆Volume compared to 1985 All Forcings [%]", fontweight='bold')    ax_big.set_xlabel("Time [year]")    ax_big.set_title(f"All Regions ", fontweight="bold", bbox=dict(        facecolor='white', edgecolor='none', pad=1), fontsize=14)    ax_big.set_ylim(0,150)    # Adjust the legend    handles, labels = ax_big.get_legend_handles_labels()    ax_big.legend(handles, labels,               ncol=2)        plt.tight_layout()plt.subplots_adjust(hspace=0.4, wspace=0.2)plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0)# specify and create a folder for saving the data (if it doesn't exists already) and save the ploto_folder_data = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/04. Figures/02. OGGM simulations/01. Modelled output/3r_a1/1. Volume/00. Combined"os.makedirs(o_folder_data, exist_ok=True)o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.comitted_random_subplots.png"# o_file_name = f"{o_folder_data}/1985_2014.{timeframe}.delta.{variable_names[v]}.comitted_cst_test.png"plt.savefig(o_file_name, bbox_inches='tight')            plt.show()        #%% Cell 4c: Comitted mass loss boxplots totals - 1 plot, Marzeion edit# Function to plot the boxplotsylim=50y = ylim-10x=1.3def plot_boxplot(data, position, label, color, colorline, alpha, is_noirr):    if position<5:        width=2    else:        width=1    q1 = np.percentile(data, 25)    q3 = np.percentile(data, 75)    median = np.median(data)    iqr_height = q3 - q1    lower_bound = q1 - 1.5 * iqr_height    upper_bound = q3 + 1.5 * iqr_height    filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]    if len(filtered_data) > 0:        lower_whisker = filtered_data.min()        upper_whisker = filtered_data.max()    else:        lower_whisker = q1        upper_whisker = q3    # Draw the shaded IQR rectangle    rect = plt.Rectangle((position - width / 2, q1),                         width, iqr_height,                         facecolor=color, alpha=alpha,                         edgecolor='none', zorder=1)    ax.add_patch(rect)        rect = plt.Rectangle((position - width / 2, lower_whisker),                             width, upper_whisker - lower_whisker,                             facecolor=color, alpha=0.4, edgecolor='none', zorder=0)    ax.add_patch(rect)    # Draw the median line    ax.plot([position - width / 2 + 0.09, position + width / 2 -0.09],            [median, median],            color=colorline, linewidth=3, zorder=2)    # Optional styling    ax.tick_params(labelsize=10)    if p > 0:        fontweight = "medium"    else:        for spine in ax.spines.values():            spine.set_linewidth(2)        fontweight = "bold"    ax.set_ylim(-75, ylim)    return rectregions = [13, 14, 15]subregions = [9, 3, 3]df = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a5_rgi_date_A_V_RGIreg_B_hugo_Vcom_noIPSL.csv")# f"{wd_path}masters/master_gdirs_r3_a5_rgi_date_A_V_RGIreg_B_hugo_Vcom.csv")master_ds = df[(~df['sample_id'].str.endswith('0')) |  # Exclude all the model averages ending with 0 except for IPSL               (df['sample_id'].str.startswith('IPSL'))]master_ds=master_ds[~master_ds['sample_id'].str.startswith('IPSL')]master_ds_tot_vol = master_ds.groupby(['rgi_subregion', 'sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    'rgi_id': lambda _: "regional total",    # lamda is anonmous functions, returns 11 member average    # take first value for all columns that are not in the list    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr', 'sample_id', 'rgi_id']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_tot_vol['V_2264_noirr_delta'] = ((master_ds_tot_vol['V_2264_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr'])*100master_ds_tot_vol['V_2264_irr_delta'] = (master_ds_tot_vol['V_2264_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_noirr_delta'] = (master_ds_tot_vol['V_2014_noirr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol['V_2014_irr_delta'] = (master_ds_tot_vol['V_2014_irr']-master_ds_tot_vol['V_1985_irr'])/master_ds_tot_vol['V_1985_irr']*100master_ds_tot_vol = master_ds_tot_vol[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]master_ds = master_ds[master_ds['V_2264_irr'].notna()]master_ds_tot_vol=master_ds_tot_vol[master_ds_tot_vol['V_2264_irr'].notna()]# master_ds_area_weighted=master_ds_area_weighted[master_ds_area_weighted['V_2264_irr'].notna()]use_weights = True# v_space_noi2 = 1.9  # Vertical space between irr and noirr boxplotsv_space_com = 1.6  # Vertical space between irr and noirr boxplotsv_space_hist = 0.4  # Vertical space between irr and noirr boxplotsv_space_com_all = 2.3  # Vertical space between irr and noirr boxplotsv_space_hist_all = 0.1  # Vertical space between irr and noirr boxplots# v_space_irr1 = 0.1  # Vertical space between irr and noirr boxplots# Storage for combined dataall_noirr, all_irr = [], []position_counter = 1cumulative_index = 14# Initialize plotfig = plt.figure(figsize=(16, 6))gs = gridspec.GridSpec(1, 16, width_ratios=[1.5] + [1]*15)  # First axis twice as wide# axes = [fig.add_subplot(gs[i]) for i in range(16)]fig, axes = plt.subplots(1,1, figsize=(16, 6), sharey=True)print(axes)# axes = axes.flatten()p=5.4tick_positions=[]tick_labels=[]mean_list=[]# Example usage: Main loop through regions and subregionsfor r, region in enumerate((regions)):    for sub in range(list((subregions))[r]):        # Filter subregion-specific data        ax = axes        region_id = f"{region}.0{sub+1}"        print(region_id)               subregion_ds = master_ds_tot_vol[master_ds_tot_vol['rgi_subregion'].str.contains(            f"{region}-0{sub+1}")]        # Calculate mean values for Noirr and Irr        # noirr_mean = master_ds_area_weighted['V_2264_noirr'][cumulative_index]        # irr_mean = master_ds_area_weighted['V_2264_irr'][cumulative_index]        # Set color and label based on the region        try:            label = subregion_ds.full_name.iloc[0]        except:            label = f"{region}-0{sub+1}"                if region==13:            if sub ==1:                label = "Pamir"            elif sub ==3:                label = "East Tien Shan"            elif sub ==5:                label = "East Kun Lun"        # Plot Noirr and Irr boxplots        print(position_counter)        print(subregion_ds['V_2014_irr_delta'].iloc[0])        print(subregion_ds['V_2264_irr_delta'].iloc[0])                tick_positions.append(p +1 )#+ v_space_com -0.2)  # or average of hist + com if you want centered        tick_labels.append(label)                mean_hist = subregion_ds['V_2014_noirr_delta'].mean()        mean_com_irr = subregion_ds['V_2264_irr_delta'].mean()         mean_com_noirr = subregion_ds['V_2264_noirr_delta'].mean()           mean_list.append((region_id, mean_com_irr, mean_com_noirr))                box_noirr = plot_boxplot(            subregion_ds['V_2014_noirr_delta'], p + v_space_hist, label, color=colors['noirr'][1],colorline=colors['cline'][0], alpha=1, is_noirr=True)        box_noirr = plot_boxplot(             subregion_ds['V_2264_noirr_delta'], p + v_space_com, label, color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)                box_irr = ax.scatter([p +                               v_space_hist], subregion_ds['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="o", s=60, linewidths=3, zorder=10)        box_irr = ax.scatter([p +                               v_space_com], subregion_ds['V_2264_irr_delta'].iloc[1], color=colors['irr_com'][0], marker="o", s=60, linewidths=3, zorder=10)        # ax.hlines(mean_hist,         #   xmin=p + v_space_hist - 0.5,  # adjust width        #   xmax=p + v_space_hist + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][0],        #   linewidth=3,        #   zorder=9)                # ax.hlines(mean_com_noirr,         #   xmin=p + v_space_com - 0.5,  # adjust width        #   xmax=p + v_space_com + 0.5,        #   linestyles='dotted',         #   color=colors['cline'][1],        #   linewidth=3,        #   zorder=9)                # Annotate the number of glaciers and delta between the two columns        num_glaciers = len(master_ds[master_ds.rgi_subregion==f"{region}-0{sub+1}"][master_ds.sample_id=="IPSL-CM6.000"])                # delta = noirr_mean - irr_mean        # Display number of glaciers and delta        initial_volume = round(subregion_ds['V_1985_irr'].iloc[0]*1e-9)        # ax.text(p, y,         #          f'{initial_volume}\n({num_glaciers})',  # \nΔ = {delta:.2f}',        #          va='center', ha='left', fontsize=12, color='black',        #          backgroundcolor="white",  zorder=10)        ax.axvline(p-0.5, color='lightgrey', linestyle='--',                    linewidth=1, zorder=1)         p+=3            # # Plot overall average boxplots for Irr and Noirrp=1.2ax = axesmaster_ds_hma = master_ds.groupby(['sample_id'], as_index=False).agg({  # calculate the 14 member average for noirr and delta, for irr the first value can be taken as they are all the same    'V_2264_noirr': 'sum',    'V_2264_irr': 'sum',    'V_2014_noirr': 'sum',    'V_2014_irr': 'sum',    'V_1985_irr': 'sum',    **{col: 'first' for col in master_ds.columns if col not in ['V_2264_noirr', 'V_2264_irr','V_2014_noirr', 'V_2014_irr','V_1985_irr']}#,  'rgi_id', 'rgi_subregion','rgi_region']} #take first for rgi_id, rgi_region, full_name, cenlon, cenlat, rgi_date, rgi_areakm2})master_ds_hma['V_2264_noirr_delta'] = ((master_ds_hma['V_2264_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr'])*100master_ds_hma['V_2264_irr_delta'] = (master_ds_hma['V_2264_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_noirr_delta'] = (master_ds_hma['V_2014_noirr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma['V_2014_irr_delta'] = (master_ds_hma['V_2014_irr']-master_ds_hma['V_1985_irr'])/master_ds_hma['V_1985_irr']*100master_ds_hma = master_ds_hma[['rgi_id', 'rgi_region', 'rgi_subregion', 'full_name', 'cenlon', 'cenlat', 'rgi_date',                       'rgi_area_km2', 'rgi_volume_km3', 'sample_id',                        'V_2264_irr','V_2264_noirr','V_2014_irr','V_2014_noirr', 'V_1985_irr','V_2264_irr_delta','V_2264_noirr_delta','V_2014_irr_delta','V_2014_noirr_delta']]avg_irr = ax.scatter([p +                       v_space_hist_all], master_ds_hma['V_2014_irr_delta'].iloc[0], color=colors['irr'][0], marker="o", s=100, linewidths=4)avg_irr = ax.scatter([p +                       v_space_com_all], master_ds_hma['V_2264_irr_delta'].iloc[0], color=colors['irr_com'][0], marker="o", s=100, linewidths=4)avg_noi_hist = plot_boxplot( master_ds_hma['V_2014_noirr_delta'], p + v_space_hist_all, [    "High Mountain Asia"], color=colors['noirr'][1], colorline=colors['cline'][0], alpha=1, is_noirr=True)avg_noi_com = plot_boxplot(master_ds_hma['V_2264_noirr_delta'], p + v_space_com_all, [    "High Mountain Asia"], color=colors['noirr_com'][1], colorline=colors['cline'][1], alpha=1, is_noirr=True)initial_total_volume = round(master_ds_hma['V_1985_irr'][0]*1e-9)# Annotate the number of glaciers for the overall averagelength=len(master_ds[master_ds.sample_id == "CNRM.001"])# ax.text(v_space_hist, y, #          f"V:{initial_total_volume}\n(#:{length})",# Display total number of glaciers#          va='center', ha='left', fontsize=12, color='black',#           backgroundcolor="white", zorder=10)#fontstyle='italic',mean_hist = master_ds_hma['V_2014_noirr_delta'].mean()mean_com_irr = master_ds_hma['V_2264_irr_delta'].mean()mean_com_noirr = master_ds_hma['V_2264_noirr_delta'].mean()mean_list.append(("High Mountain Asia", mean_com_irr, mean_com_noirr))df_means = pd.DataFrame(mean_list, columns=["subregion", "V_2264_irr_delta","V_2264_noirr_delta"])df_means.to_csv(f"{wd_path}masters/mean_deltaV_Comitted.csv")# Add a legend for regions, mean (dot), and median (stripe)region_legend_patches = [Line2D([0], [0], marker='o', color=colors['irr'][0], linestyle='None', markersize=8, lw=20, label='Historical (W5E5)'),                        mpatches.Patch(color=colors['noirr'][1], label='Historical NoIrr'),                        Line2D([0], [0], marker='o', color=colors['irr_com'][0], linestyle='None', markersize=8, lw=20, label='Committed (historical)'),                        mpatches.Patch(color=colors['noirr_com'][0], label='Committed (Historical NoIrr)'),                        # Line2D([0], [0], color='black', linestyle='dashed', linewidth=2, label=f'NoIrr, {len(master_ds_hma)}-member mean')                        ]tick_positions.append(1 + v_space_com)tick_labels.append("High\nMountain\nAsia")# Set tick positions and rotated labels (except HMA)ax.set_xticks(tick_positions)ax.set_xticklabels(tick_labels, fontsize=14, rotation=30)# Override HMA rotation to be flat (if needed)for label in ax.get_xticklabels():    if label.get_text() == "High\nMountain\nAsia":        label.set_rotation(0)        label.set_fontweight("bold")        fig.legend(handles=region_legend_patches, loc='upper center',          bbox_to_anchor=(0.705, 0.87), ncols=2, fontsize=14,columnspacing=1.5)ax.set_ylabel('Volume change (%, vs. 1985 historic)', labelpad=15, fontsize=14)fig.subplots_adjust(wspace=0.1)#, hspace=0.1)ax.set_xlim(0, 3*15+5)ax.tick_params(labelsize=12)# Include the panel plotax.axhline(y=0, color='grey', linestyle='--', linewidth=1, zorder=0)output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member.nc")# output_nc_path = os.path.join(wd_path, "masters", "master_comitted_volume_timeseries_individual_member_noIPSL.nc")tlines = xr.open_dataset(output_nc_path)ax_inset = inset_axes(ax, width="24%", height="32%", loc='lower left',bbox_to_anchor=(286,100,1800, 700))  # width/height can be % or floatfor m,model in enumerate(models_shortlist):    member_series = []    times = None        for x in range(members_averages[m]):        if x>0:            ax_inset.plot(                tlines.sel(scenario='committed', model=model, member=x, experiment="NoIrr").time.values,               tlines.sel(scenario='committed', model=model, member=x,experiment="NoIrr").volume_percent-100, ls=':', lw=1, color=colors['noirr_com'][0], zorder=1            )                        data = tlines.sel(scenario='committed', model=model, member=x, experiment="NoIrr")            volume = data.volume_percent - 100            member_series.append(volume)            if times is None:                times = data.time.values                if member_series:        stacked = xr.concat(member_series, dim='member')        q25 = stacked.quantile(0.25, dim='member')        q75 = stacked.quantile(0.75, dim='member')        # Fill between 25th and 75th percentile        ax_inset.fill_between(            times, q25, q75,            color=colors['noirr_com'][1],            alpha=0.7,            zorder=0        )                                                                ax_inset.plot(tlines.sel(scenario='committed', model="W5E5", member=0, experiment="Irr").time.values,tlines.sel(scenario='committed', model="W5E5", member=0,experiment="Irr").volume_percent-100, ls='-', lw=2, color=colors['irr_com'][0])ax_inset.plot(tlines.sel(scenario='committed', model="avg", member=0, experiment="NoIrr").time.values,tlines.sel(scenario='committed', model="avg", member=0,experiment="NoIrr").volume_percent-100, ls='--', lw=2, color=colors['noirr_com'][0], zorder=0)# ax_inset.text(0.2, 0.95, "Committed evolution", fontsize=12,#               transform=ax_inset.transAxes, va='top', ha='left')# Set axis ticks and labelsax_inset.set_xlim(2014,2264)ax_inset.set_xticks([2114, 2214])ax_inset.set_xticklabels(['100', '200 yrs'])ax_inset.set_ylim(-30,10)ax_inset.set_yticks([ 0, -10, -20])#, -30])ax_inset.set_yticklabels([ '0%', '-10%', '-20%'])#, '-30%'])# ax_inset.axhline(0, color='grey', linestyle='--', linewidth=1)# ax_inset.yaxis.label.set_zorder(10)# Tick marks and labels setupax_inset.tick_params(axis='both', which='both',                     direction='in',     # tick lines point inside                     length=5,           # visible tick length                     top=False, bottom=True, left=False, right=True,                     labelsize=12)# Tick labels on right side of y-axisax_inset.yaxis.set_ticks_position('left')ax_inset.yaxis.set_label_position("left")ax_inset.grid(True, axis='both', which='major', linestyle=':', linewidth=0.5, color='gray')for label in ax_inset.get_xticklabels():    label.set_verticalalignment('top')    label.set_y(0.2)  # smaller = closer to axis line (inside)# Shift y-axis labels left into the plotfor label in ax_inset.get_yticklabels():    label.set_horizontalalignment('left')    label.set_x(0.05)  # smaller = closer to axis line (inside)    rect_x = p + v_space_hist_all + 1.05rect_y = -24rect_w=2.25rect_height=24square = mpatches.Rectangle((rect_x, rect_y), rect_w, rect_height,                               linewidth=0.7, edgecolor='black', facecolor='none')ax.add_patch(square)rect_x_zoom = 0.2rect_y_zoom = ylim-110rect_w_zoom=12rect_height_zoom=30square_zoom = mpatches.Rectangle((rect_x_zoom, rect_y_zoom), rect_w_zoom, rect_height_zoom,                               linewidth=1, edgecolor='none', facecolor='none')ax.add_patch(square_zoom)# --- Convert rectangle data coordinates to figure coordinates ---# Lower left and lower right corners of the rectanglell_data = (rect_x, rect_y)lr_data = (rect_x + rect_w, rect_y)ll_disp = ax.transData.transform(ll_data)lr_disp = ax.transData.transform(lr_data)ll_fig = fig.transFigure.inverted().transform(ll_disp)lr_fig = fig.transFigure.inverted().transform(lr_disp)# --- Get upper left and right of the inset in figure coordinates ---# These are in the inset's axes coordinates, so (0, 1) is upper-left, (1, 1) is upper-rightul_data_zoom = (rect_x_zoom, rect_y_zoom+rect_height_zoom)ur_data_zoom = (rect_x_zoom + rect_w_zoom, rect_y_zoom+rect_height_zoom)ul_disp_zoom = ax.transData.transform(ul_data_zoom)ur_disp_zoom = ax.transData.transform(ur_data_zoom)ul_fig_zoom = fig.transFigure.inverted().transform(ul_disp_zoom)ur_fig_zoom = fig.transFigure.inverted().transform(ur_disp_zoom)# --- Draw lines from rectangle to inset corners ---line_left = mlines.Line2D([ll_fig[0], ul_fig_zoom[0]], [ll_fig[1], ul_fig_zoom[1]],                          transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)line_right = mlines.Line2D([lr_fig[0], ur_fig_zoom[0]], [lr_fig[1], ur_fig_zoom[1]],                           transform=fig.transFigure, color='black', linestyle='--', zorder=10, lw=0.7)ax.grid(True, axis='y', which='major', linestyle=':', linewidth=0.5, color='gray')fig.lines.extend([line_left, line_right])plt.savefig(    f"{fig_path}/Boxplot_Comitted_Mass_Loss_marzeion_noIPSL.png")plt.show()           #%% Cell 5: Table# Specify Pathso_folder_data = (    "/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output Files/02. OGGM/02. Volume Area simulations A+1km2")o_file_data = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions.csv"unit="km3"factors = [10**-9]if unit=="Gt":    rho=0.85 #Gt/km³    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_gt.csv"else:    rho=1    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute.csv"# Load datasetds = pd.read_csv(o_file_data)ds_subregions_initial = ds[ds.time == 1985.0][[    "subregion",    "volume_irr",]].reset_index(drop=True)total_row = pd.DataFrame({    'subregion': ['total'],    'volume_irr': [ds_subregions_initial['volume_irr'].sum()]})ds_subregions_initial = pd.concat([ds_subregions_initial, total_row], ignore_index=True)ds_subregions = ds[ds.time == 2014.0][[    "subregion",    "volume_loss_percentage_irr",    "volume_loss_percentage_noirr",    "volume_irr",    "volume_noirr",]]# Process total volumesds_total = ds.groupby("time").sum()[[    "volume_irr",    "volume_noirr",]].round(2)# Calculate total rowdf_total = pd.DataFrame({    "subregion": ["total"],    "volume_loss_percentage_irr": ((ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_loss_percentage_noirr": ((ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_irr": (ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"])*rho,    "volume_noirr": (ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"])*rho})# Combine subregions with totalds_all_losses = pd.concat([ds_subregions, df_total], ignore_index=True)# Calculate deltasds_all_losses["delta_irr"] = ds_all_losses["volume_loss_percentage_irr"] - ds_all_losses["volume_loss_percentage_noirr"]ds_all_losses["delta_irr_abs"] = ds_all_losses["volume_irr"] - ds_all_losses["volume_noirr"]# Load RGI datadf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")df["subregion"] = df["rgi_subregion"].str.replace(    "-", ".").str.strip().str.lower()# Aggregate RGI dataareas = df.groupby("subregion")["rgi_area_km2"].sum().reset_index(name="area")volume = df.groupby("subregion")[    "rgi_volume_km3"].sum().reset_index(name="volume")nr_glaciers = df.groupby("subregion")[    "rgi_id"].count().reset_index(name="nr_glaciers")subregion_name = df.groupby("subregion")["full_name"].first().reset_index(name="name")subregion_map = {    "13.02": "Pamir",    "13.04": "East Tien Shan",    "13.06": "East Kun Lun"}subregion_name["name"] = subregion_name["subregion"].map(subregion_map).fillna(subregion_name["name"]) # Add total rowsareas = pd.concat([areas, pd.DataFrame(    {"subregion": ["total"], "area": [areas["area"].sum()]})], ignore_index=True)volume = pd.concat([volume, pd.DataFrame({"subregion": ["total"], "volume": [                   volume["volume"].sum()]})], ignore_index=True)nr_glaciers = pd.concat([nr_glaciers, pd.DataFrame({"subregion": [                        "total"], "nr_glaciers": [nr_glaciers["nr_glaciers"].sum()]})], ignore_index=True)names = pd.concat([subregion_name, pd.DataFrame({"subregion": [                        "total"], "name": ["High Mountain Asia"]})], ignore_index=True)areas['area']=areas['area'].round().astype(int)volume['volume']=volume['volume'].round().astype(int)# Merge RGI info into ds_all_lossesds_all_losses['subregion'] = areas["subregion"]ds_all_losses = ds_all_losses.merge(areas, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(volume, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(nr_glaciers, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(names, on="subregion", how="left")# ds_all_losses['error_margin_noi_rel'] = ds_all_losses['error_margin_noi_abs'] / \    # ds_all_losses['volume_noirr']*100# ds_all_losses['error_margin_cf_rel'] = ds_all_losses['error_margin_cf_abs'] / \    # ds_all_losses['volume_cf']*100# Final selection of columns and save to CSVds_all_losses = ds_all_losses[[    "subregion", "name", "nr_glaciers", "area", f"volume",    "volume_loss_percentage_irr", "volume_irr",    "volume_loss_percentage_noirr", "volume_noirr",  "delta_irr", "delta_irr_abs"#"error_margin_noi_rel", "error_margin_noi_abs",    # "volume_loss_percentage_cf", "volume_cf", "error_margin_cf_rel", "error_margin_cf_abs", "delta_cf",]].round(1)#Add comitted mass lossesdf_means_com = pd.read_csv(f"{wd_path}masters/mean_deltaV_Comitted.csv").reset_index(drop=True).drop('Unnamed: 0', axis=1)df_means_com['subregion'] = df_means_com['subregion'].replace('High Mountain Asia', 'total')df_means_com['delta_com'] =df_means_com['V_2264_irr_delta'] - df_means_com['V_2264_noirr_delta']df_all_losses = ds_all_losses.merge(df_means_com.round(1), on="subregion", how="left")df_means_future = xr.open_dataset(f"{wd_path}masters/master_volume_subregion_future_noi_bias_incl_total.nc")df_means_future = df_means_future.sel(time=2074).sel(sample_id='3-member-avg')for ssp in ['126','370']:    df_future=df_means_future.sel(ssp=ssp).subregion.values    formatted_codes = [code.replace('-', '.') for code in df_future]    df_future_vhist=df_means_future.sel(ssp=ssp).sel(exp="IRR").volume.values*factors    df_future_vhist_noirr=df_means_future.sel(ssp=ssp).sel(exp="NOI").volume.values*factors    df = pd.DataFrame({    'subregion': formatted_codes,    f'volume_irr_ssp{ssp}': df_future_vhist,    f'volume_noi_ssp{ssp}': df_future_vhist_noirr #- ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    })        df[f'volume_noi_ssp{ssp}'] = round(((df[f'volume_noi_ssp{ssp}'] - ds_subregions_initial.volume_irr )/ ds_subregions_initial.volume_irr) * 100,1)    df[f'volume_irr_ssp{ssp}'] = round(((df[f'volume_irr_ssp{ssp}'] - ds_subregions_initial.volume_irr )/ ds_subregions_initial.volume_irr) * 100,1)    df[f'delta_irr_ssp{ssp}'] = round(df[f'volume_irr_ssp{ssp}'] -df[f'volume_noi_ssp{ssp}'],1)    df_all_losses = df_all_losses.merge(df, on="subregion", how="left")    df_all_losses.to_csv(o_file_data_processed)   o_file_data_processed_2 = f"{fig_path}/Table_1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute.csv"df_all_losses.to_csv(o_file_data_processed_2)       #%% Cell 5b: Table SMALL# Specify Pathso_folder_data = (    "/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output Files/02. OGGM/02. Volume Area simulations A+1km2")o_file_data = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions.csv"unit="km3"factors = [10**-9]if unit=="Gt":    rho=0.85 #Gt/km³    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_gt_small.csv"else:    rho=1    o_file_data_processed = f"{o_folder_data}/1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute_small.csv"# Load datasetds = pd.read_csv(o_file_data)ds_subregions_initial = ds[ds.time == 1985.0][[    "subregion",    "volume_irr",]].reset_index(drop=True)total_row = pd.DataFrame({    'subregion': ['total'],    'volume_irr': [ds_subregions_initial['volume_irr'].sum()]})ds_subregions_initial = pd.concat([ds_subregions_initial, total_row], ignore_index=True)ds_subregions = ds[ds.time == 2014.0][[    "subregion",    "volume_loss_percentage_irr",    "volume_loss_percentage_noirr",    "volume_irr",    "volume_noirr",]]# Process total volumesds_total = ds.groupby("time").sum()[[    "volume_irr",    "volume_noirr",]].round(2)# Calculate total rowdf_total = pd.DataFrame({    "subregion": ["total"],    "volume_loss_percentage_irr": ((ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_loss_percentage_noirr": ((ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"]) / ds_total.loc[1985.0, "volume_irr"]) * 100,    "volume_irr": (ds_total.loc[2014.0, "volume_irr"] - ds_total.loc[1985.0, "volume_irr"])*rho,    "volume_noirr": (ds_total.loc[2014.0, "volume_noirr"] - ds_total.loc[1985.0, "volume_irr"])*rho})# Combine subregions with totalds_all_losses = pd.concat([ds_subregions, df_total], ignore_index=True)# Calculate deltasds_all_losses["delta_irr"] = ds_all_losses["volume_loss_percentage_irr"] - ds_all_losses["volume_loss_percentage_noirr"]ds_all_losses["delta_irr_abs"] = ds_all_losses["volume_irr"] - ds_all_losses["volume_noirr"]# Load RGI datadf = pd.read_csv(    f"{wd_path}masters/master_gdirs_r3_a1_rgi_date_A_V_RGIreg.csv")df["subregion"] = df["rgi_subregion"].str.replace(    "-", ".").str.strip().str.lower()# Aggregate RGI dataareas = df.groupby("subregion")["rgi_area_km2"].sum().reset_index(name="area")volume = df.groupby("subregion")[    "rgi_volume_km3"].sum().reset_index(name="volume")nr_glaciers = df.groupby("subregion")[    "rgi_id"].count().reset_index(name="nr_glaciers")subregion_name = df.groupby("subregion")["full_name"].first().reset_index(name="name")subregion_map = {    "13.02": "Pamir",    "13.04": "East Tien Shan",    "13.06": "East Kun Lun"}subregion_name["name"] = subregion_name["subregion"].map(subregion_map).fillna(subregion_name["name"]) # Add total rowsareas = pd.concat([areas, pd.DataFrame(    {"subregion": ["total"], "area": [areas["area"].sum()]})], ignore_index=True)volume = pd.concat([volume, pd.DataFrame({"subregion": ["total"], "volume": [                   volume["volume"].sum()]})], ignore_index=True)nr_glaciers = pd.concat([nr_glaciers, pd.DataFrame({"subregion": [                        "total"], "nr_glaciers": [nr_glaciers["nr_glaciers"].sum()]})], ignore_index=True)names = pd.concat([subregion_name, pd.DataFrame({"subregion": [                        "total"], "name": ["High Mountain Asia"]})], ignore_index=True)areas['area']=areas['area'].round().astype(int)volume['volume']=volume['volume'].round().astype(int)# Merge RGI info into ds_all_lossesds_all_losses['subregion'] = areas["subregion"]ds_all_losses = ds_all_losses.merge(areas, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(volume, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(nr_glaciers, on="subregion", how="left")ds_all_losses = ds_all_losses.merge(names, on="subregion", how="left")# ds_all_losses['error_margin_noi_rel'] = ds_all_losses['error_margin_noi_abs'] / \    # ds_all_losses['volume_noirr']*100# ds_all_losses['error_margin_cf_rel'] = ds_all_losses['error_margin_cf_abs'] / \    # ds_all_losses['volume_cf']*100# Final selection of columns and save to CSVds_all_losses = ds_all_losses[[    "subregion", "name", "nr_glaciers", "area", f"volume",    "volume_loss_percentage_irr", "volume_irr",    "volume_loss_percentage_noirr", "volume_noirr",  "delta_irr", "delta_irr_abs"#"error_margin_noi_rel", "error_margin_noi_abs",    # "volume_loss_percentage_cf", "volume_cf", "error_margin_cf_rel", "error_margin_cf_abs", "delta_cf",]].round(1)#Add comitted mass lossesds_all_losses.to_csv(o_file_data_processed)   o_file_data_processed_2 = f"{fig_path}/Table_1985_2014.{timeframe}.delta.Volume.subregions_paper_output_table_absolute_small.csv"ds_all_losses.to_csv(o_file_data_processed_2)               #%% Cell 6: Temperature plot - 2d legenddef create_precip_temp_colormap(    precip_range=(-20, 20), temp_range=(-1.5, 1.5), grid_size=256):    color_dry_warm = np.array([1.0, 0.4, 0.0])    color_dry_cold = np.array([0.6, 0.0, 0.6])    color_wet_warm = np.array([0.6, 1.0, 0.2])    color_wet_cold = np.array([0.0, 0.4, 1.0])    color_center  = np.array([1.0, 1.0, 1.0])    custom_colormap = np.zeros((grid_size, grid_size, 3))    for i in range(grid_size):        for j in range(grid_size):            x = -1 + 2 * j / (grid_size - 1)            y = -1 + 2 * i / (grid_size - 1)            wx = (x + 1) / 2            wy = (y + 1) / 2            top = (1 - wx) * color_dry_warm + wx * color_wet_warm            bottom = (1 - wx) * color_dry_cold + wx * color_wet_cold            color = (1 - wy) * bottom + wy * top            r = np.sqrt(x**2 + y**2)            fade = np.clip(1 - r, 0, 1)            color = (1 - fade) * color + fade * color_center            custom_colormap[i, j] = color    def colormap_callable(delta_temp, delta_precip):        rows = ((delta_temp - temp_range[0]) / (temp_range[1] - temp_range[0]) * (grid_size - 1)).astype(int)        cols = ((delta_precip - precip_range[0]) / (precip_range[1] - precip_range[0]) * (grid_size - 1)).astype(int)        rows = np.clip(rows, 0, grid_size - 1)        cols = np.clip(cols, 0, grid_size - 1)        return custom_colormap[rows, cols]    return custom_colormap, colormap_callabledef plot_colormap_legend(ax,colormap_grid, precip_range=(-20, 20), temp_range=(-1.5, 1.5)):    """    Plot the 2D color legend for the custom ΔPrecip–ΔTemp colormap.    Parameters:        colormap_grid: The [grid_size x grid_size x 3] RGB array from create_precip_temp_colormap()        precip_range: Tuple of (min, max) precipitation anomaly        temp_range: Tuple of (min, max) temperature anomaly    """    ax.imshow(colormap_grid, origin='lower',              extent=[precip_range[0], precip_range[1], temp_range[0], temp_range[1]],              aspect='auto')    ax.set_xlabel("ΔPrecipitation (%)", fontsize=12)    ax.set_ylabel("ΔTemperature (°C)", fontsize=12)    ax.tick_params(labelsize=12)    ax.set_title("Legend", fontsize=12)    ax.grid(False)    plt.tight_layout()    # plt.show()def plot_subplots(index, subplots, annotation, diff, timestamps, axes, shp, custom_cmap, timeframe, scale, title, vmin=None, vmax=None):    for time_idx, timestamp_name in enumerate(timestamps):        # Determine subplot location based on timeframe        if timeframe == 'monthly':            row = time_idx // 4  # Calculate row index            col = time_idx % 4            ax = axes[row, col]        elif timeframe == 'seasonal':            row = time_idx // 2            col = time_idx % 2            ax = axes[row, col]        elif timeframe == 'annual':            row, col = 0, 0            ax = axes        # Select time dimension based on scale and timeframe        if scale == "Local":            time_dim_name = list(diff.dims)[0]        elif scale == "Global" and timeframe != 'annual':            time_dim_name = list(diff.dims)[2]        # Select relevant data slice        if timeframe == 'annual':            diff_sel = diff        else:            diff_sel = diff.isel({time_dim_name: time_idx})        # Convert Dataset to DataArray if necessary        if isinstance(diff_sel, xr.Dataset):            diff_sel = diff_sel[list(diff_sel.data_vars.keys())[0]]                    # --- NEW: If colormap is callable, apply it manually ---        if callable(custom_cmap):            # Assuming `diff_sel` contains both temp and precip deltas            delta_temp = diff_sel['tas'] if 'tas' in diff_sel else diff_sel            delta_precip = diff_sel['pr'] if 'pr' in diff_sel else diff_sel            delta_temp_np = delta_temp.values            delta_precip_np = delta_precip.values            rgb_img = custom_cmap(delta_temp_np, delta_precip_np)            im = ax.imshow(rgb_img,                           extent=[diff_sel.lon.min(), diff_sel.lon.max(),                                   diff_sel.lat.min(), diff_sel.lat.max()],                           origin='lower',                           transform=ccrs.PlateCarree())        else:            im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',                                      transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False)        # # Plot data and the Karakoram outline        # im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',        #                           transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False)        if scale=="Local":            for attribute, subregion in subregions.groupby('o2region'):                           # Uncomment for coloring of specific subregions                linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"                # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,                subregion.plot(ax=ax, edgecolor='black', linewidth=1.2,                              facecolor='none') # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion            shp.plot(ax=ax, edgecolor='black', linewidth=1, facecolor='none')        ax.coastlines(resolution='10m')        # Set gridlines and labels        gl = ax.gridlines(draw_labels=True)        gl.top_labels = False        gl.right_labels = False            gl.ylabel_style = {'size': 12}        gl.ylocator = plt.MaxNLocator(nbins=3)            gl.xlabel_style = {'size': 12}        gl.xlocator = plt.MaxNLocator(nbins=3)                gl.xformatter = LONGITUDE_FORMATTER        gl.yformatter = LATITUDE_FORMATTER        """4 Include labels for the cbar and for the y and x axis"""        ax.set_title(title)    return im# def plot_P_T_perturbations_avg(scale, var, timeframe, mode, diftype, plotsave):""" Part 0 - Set plotting parameters"""# y0 = 1985  # if running from 1901 to 1985, than indicate extra id of counterfactual to access the data# ye = 2014# if running from 1901 to 1985, than indicate extra id of counterfactual to access the datay0s = [1985]#, 1901]yes = [2014]#, 1985]extra_ids = [""]#, "_counterfactual"]ptb_types = ["Irr"]#, "NoForcing"]scale = "Global"subplots = "off"all_vars_p = []all_vars_t = []all_vars_local_p = []all_vars_local_t = []# "Temperature"]:  # ,"Temperature"]:for var in ["Temperature", "Precipitation"]:    for timeframe in ["annual"]:  # :, "seasonal", "monthly"]:        for mode in ['dif']:  # , 'std']:            for y, y0 in enumerate(y0s):                ye = yes[y]                extra_id = extra_ids[y]                ptbtype = ptb_types[y]                if var == "Precipitation" and mode == 'dif':                    diftypes = ['rel']                else:                    diftypes = ['abs']                for dif in diftypes:                    print(var, timeframe, dif)                    diftype = dif                    timestamps = "YEAR"                    time_averaging = 'time.year'                    time_type = 'year'                    col_wrap = 1                    # Provide cbar ranges and colors for plots for different variables, modes (dif/std) and difference types (abs/rel)                    if var == "Precipitation":                        variable_name = "pr"                        var_suffix = "PR"                        if mode == 'dif' and diftype == 'rel':                            mode_suff = 'total'                            vmin = -20                            vmax = 20                            zero_scaled = (abs(vmin)/(abs(vmin)+abs(vmax)))                            colors = [(0, 'xkcd:mocha'), (zero_scaled,                                                          'xkcd:white'), (1, 'xkcd:aquamarine')]                            # custom_cmap = LinearSegmentedColormap.from_list(                            #     'custom_cmap', colors)                            unit = '%'                    elif var == "Temperature":                        variable_name = "tas"                        var_suffix = "TEMP"                                            mode_suff = 'total'                        vmin = -1.5                        vmax = 1.5                        zero_scaled = (abs(vmin)/(abs(vmin)+abs(vmax)))                        colors = [(0, 'cornflowerblue'), (zero_scaled,                                                          'xkcd:white'), (1, 'xkcd:tomato')]                        custom_cmap = LinearSegmentedColormap.from_list(                            'custom_cmap', colors)                        unit = '°C'                    members = [1, 3, 4, 6, 4]                    # members = [1, 1, 1, 1]                    all_diff = []  # create a dataset to add all member differences                    all_model_diffs = []                    models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]                    lonmin, lonmax, latmin, latmax = [60,109,22,52]                    for (m, model) in enumerate(models):                        model_diff = []                        for member in range(members[m]):                            # only open data for non model averages (except for IPSL-CM6 as only one member)                            if model == "IPSL-CM6" or member != 0:                                # Part 1: Delete                                diff_folder_in = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/03. Regridded Perturbations/{var}/{timeframe}/{model}/{member}"                                ifile_diff = f"{diff_folder_in}/REGRID.{model}.{var_suffix}.DIF.00{member}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.nc"                                diff = xr.open_dataset(ifile_diff)                                 if scale == "Local":  # scale the data to the local scale                                    diff = diff.where((diff.lon >= 60) & (diff.lon <= 109) & (                                        diff.lat >= 22) & (diff.lat <= 52), drop=True)                                                                                                        # loose all the filtered data (nan)                                diff_clean = diff.dropna(dim="lon", how="all")                                # include the values in the list for caluclating the avg difference by model                                model_diff.append(diff_clean)                                # include the values in the list for caluclating the avg difference over all models                                all_diff.append(diff_clean)                        all_model_diff = xr.concat(                            model_diff, dim="models").mean(dim="models")  # concatenate all models into a list averaged by model                        all_model_diffs.append(all_model_diff)                    all_model_diffs_avg = xr.concat(                        all_model_diffs, dim="models")  # concatenate all models                    all_diffs_avg = xr.concat(all_diff, dim="models").mean(                        dim="models")  # concatenate all members and calculate the mean over all the models                    all_diffs_avg_local = all_diffs_avg.where((all_diffs_avg.lon >= 60) & (all_diffs_avg.lon <= 109) & (                        all_diffs_avg.lat >= 22) & (all_diffs_avg.lat <= 52), drop=True)                    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"                    os.makedirs(o_folder, exist_ok=True)                    o_file=f"{o_folder}/{scale}_{var}_processed.nc"                    # all_diffs_avg.to_netcdf(o_file)                                        # Unpack axes                    if var == "Precipitation":                        all_vars_p = all_diffs_avg                        all_vars_local_p = all_diffs_avg_local                    elif var =="Temperature":                        all_vars_t = all_diffs_avg                        all_vars_local_t = all_diffs_avg_local                                    all_vars_2d = xr.merge([all_vars_p, all_vars_t])all_vars_local_2d = xr.merge([all_vars_local_p, all_vars_local_t])# 2D blended colormapcolormap_grid, custom_cmap = create_precip_temp_colormap(    precip_range=(-20, 20), temp_range=(-1.5, 1.5))""" Part 2 - Shapefile outline for Karakoram Area to be included"""# path to  shapefileshapefile_path = '/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/03. Shapefile/Karakoram/Pan-Tibetan Highlands/Pan-Tibetan Highlands (Liu et al._2022)/Shapefile/Pan-Tibetan Highlands (Liu et al._2022)_P.shp'shp = gpd.read_file(shapefile_path)target_crs = 'EPSG:4326'shp = shp.to_crs(target_crs)indices = ["A", "B"]#, "E", "F"]# Create the mosaic plotlayout = """AB"""figsize = (18.15,5) #based on aspect ratio of global and zoomedfig, axes = plt.subplot_mosaic(layout, subplot_kw={'projection': ccrs.PlateCarree()},                               figsize=figsize,                               gridspec_kw={'wspace': 0.05, 'width_ratios': [2.0, 1.63]})#,'height_ratios': [1, 1]})axes['A'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['B'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())# plot the irrmip differenceim = plot_subplots(indices[0], subplots, (sum(members)-len(models)+1),                   all_vars_2d, timestamps, axes[indices[0]], shp, custom_cmap, timeframe, "Global", "Global", vmin=vmin, vmax=vmax)im = plot_subplots(indices[1], subplots, (sum(members)-len(models)+1),                   all_vars_local_2d, timestamps, axes[indices[1]], shp, custom_cmap, timeframe, "Local", "High Mountain Asia", vmin=vmin, vmax=vmax)bbox = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['A'].transData,    xyB=(lonmin, latmax), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['A'].transData,    xyB=(lonmin, latmin), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1)# Add to the global axes (they own the lines)axes['A'].add_artist(line_top)axes['A'].add_artist(line_bottom)axes['A'].add_patch(bbox)sign_diff = xr.concat([np.sign(diff[variable_name])                      for diff in all_model_diffs], dim="models")agreement_on_sign = (    abs(sign_diff.mean(dim="models")) > 0.8)# Step 3: Combine the conditions to create the final maskwithin_threshold = agreement_on_sign.sel(lon=slice(lonmin, lonmax), lat=slice(latmax, latmin))# Step 4: Convert the mask to 2D by removing the singleton 'variable' dimension if neededwithin_threshold_2d = within_threshold.astype(    int).squeeze()# Step 6: Overlay the mask with dots in areas where agreement criteria are metaxes[indices[1]].contourf(    all_diffs_avg_local.lon, all_diffs_avg_local.lat, within_threshold_2d,    levels=[0.5, 1.5], colors='none', hatches=['////'], transform=ccrs.PlateCarree())""" 3C Add color bar for entire plot"""cbar_ax = fig.add_axes([0.94, 0.1, 0.1, 0.78])plot_colormap_legend(cbar_ax, colormap_grid, precip_range=(-20, 20), temp_range=(-1.5, 1.5))# add cbar in the figure, for overall figure, not subplots# Define the position of the colorbar# # cbar = fig.colorbar(im, cax=cbar_ax, extend='both')# # Increase distance between colorbar label and colorbar# cbar.ax.yaxis.labelpad = 20# if mode == 'dif':#     # cbar.set_label(f'$\Delta_{{ptb_type}}_{,{var}}$ [{unit}]', size='15')#     cbar.set_label(#         rf'$\Delta_{{{ptbtype}, {var}}}$ [{unit}]', size=15)#     if mode == 'std':#         cbar.set_label(#             f'{var} - model member std [{unit}]', size='15')# cbar.ax.tick_params(labelsize=12)# # adjust subplot spacing to be smaller# plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1,#                     top=0.9, wspace=0.05, hspace=0.05)hedging_patch = mpatches.Patch(    label='< 80% of model members agree on sign of change', hatch='////', edgecolor='black', facecolor='none')# Add the custom legend to the plotfig.legend(handles=[                        hedging_patch], loc='lower center', bbox_to_anchor=(0.5, -0.05), fontsize=12)# if plotsave == 'save':o_folder_diff = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/04. Figures/01. Climate data/02. Perturbations/"os.makedirs(f"{o_folder_diff}/", exist_ok=True)o_file_name = f"{o_folder_diff}/Mosaic.{var}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"# plt.savefig(o_file_name, bbox_inches='tight')plt.show()            #%% Cell6b: TP plot - 1d legendplt.rcParams.update({'font.size': 16})def plot_subplots(index, subplots, annotation, diff, timestamps, axes, shp, custom_cmap, timeframe, scale, title, subplot_index, vmin=None, vmax=None):    for time_idx, timestamp_name in enumerate(timestamps):        # Determine subplot location based on timeframe        if timeframe == 'monthly':            row = time_idx // 4  # Calculate row index            col = time_idx % 4            ax = axes[row, col]        elif timeframe == 'seasonal':            row = time_idx // 2            col = time_idx % 2            ax = axes[row, col]        elif timeframe == 'annual':            row, col = 0, 0            ax = axes        # Select time dimension based on scale and timeframe        if scale == "Local":            time_dim_name = list(diff.dims)[0]        elif scale == "Global" and timeframe != 'annual':            time_dim_name = list(diff.dims)[2]        # Select relevant data slice        if timeframe == 'annual':            diff_sel = diff        else:            diff_sel = diff.isel({time_dim_name: time_idx})                # Convert Dataset to DataArray if necessary        if isinstance(diff_sel, xr.Dataset):            diff_sel = diff_sel[list(diff_sel.data_vars.keys())[0]]                    diff_sel=diff_sel.reset_coords('height', drop=True).squeeze()        print(diff_sel)        # Plot data and the Karakoram outline        im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',                                  transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False, zorder=1)        if scale=="Local":            for attribute, subregion in shp.groupby('o2region'):                           # Uncomment for coloring of specific subregions                linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"                # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,                subregion.plot(ax=ax, edgecolor='black', linewidth=0.5,                              facecolor='none', zorder=3) # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion                        # shp.plot(ax=ax, edgecolor='black', linewidth=1, facecolor='none')        ax.coastlines(resolution='10m')        ax.add_feature(cfeature.OCEAN, facecolor='white', zorder=3) #mask the ocean, as some nan data is not filtered out        # Set gridlines and labels        gl = ax.gridlines(draw_labels=True)        gl.top_labels = False        gl.right_labels = False        if index in ["A","C"]:            ax.grid(visible=False)            gl.xlines = False            gl.ylines = False            gl.left_labels = False            gl.top_labels = False            gl.right_labels = False            gl.bottom_labels = False            # gl = ax.gridlines(draw_labels=False)        if index in ["A","C","B"]:            gl.bottom_labels = False            gl.ylabel_style = {'size': 14}        gl.ylocator = plt.MaxNLocator(nbins=3)            gl.xlabel_style = {'size': 14}        gl.xlocator = plt.MaxNLocator(nbins=3)                gl.xformatter = LONGITUDE_FORMATTER        gl.yformatter = LATITUDE_FORMATTER        """4 Include labels for the cbar and for the y and x axis"""        if subplot_index==False:            ax.set_title(title, fontsize=18)        else:            # ax.set_title(f'{subplot_index} {title}', fontsize=14, fontweight='bold', loc='right')            ax.text(0.95,0.92, f'{subplot_index}', transform=ax.transAxes,fontsize=14, fontweight='bold')                return im# def plot_P_T_perturbations_avg(scale, var, timeframe, mode, diftype, plotsave):""" Part 0 - Set plotting parameters"""# y0 = 1985  # if running from 1901 to 1985, than indicate extra id of counterfactual to access the data# ye = 2014# if running from 1901 to 1985, than indicate extra id of counterfactual to access the datay0s = [1985]#, 1901]yes = [2014]#, 1985]extra_ids = [""]#, "_counterfactual"]ptb_types = ["Irr"]#, "NoForcing"]scale = "Global"subplots = "off"all_vars_p = []all_vars_t = []all_vars_local_p = []all_vars_local_t = []all_model_diffs_avg_p = []all_model_diffs_avg_t = []all_member_diff_p = []all_member_diff_t = []# "Temperature"]:  # ,"Temperature"]:for var in ["Temperature", "Precipitation"]:    for timeframe in ["annual"]:  # :, "seasonal", "monthly"]:        for mode in ['dif']:  # , 'std']:            for y, y0 in enumerate(y0s):                ye = yes[y]                extra_id = extra_ids[y]                ptbtype = ptb_types[y]                if var == "Precipitation" and mode == 'dif':                    diftypes = ['rel']                else:                    diftypes = ['abs']                for dif in diftypes:                    print(var, timeframe, dif)                    diftype = dif                    timestamps = "YEAR"                    time_averaging = 'time.year'                    time_type = 'year'                    col_wrap = 1                    # Provide cbar ranges and colors for plots for different variables, modes (dif/std) and difference types (abs/rel)                    if var == "Precipitation":                        variable_name = "pr"                        var_suffix = "PR"                        if mode == 'dif' and diftype == 'rel':                            mode_suff = 'total'                            unit = '%'                    elif var == "Temperature":                        variable_name = "tas"                        var_suffix = "TEMP"                        mode_suff = 'total'                                                unit = '°C'                    vmin_t = -1#-1.5                    vmax_t = 1#1.5                    zero_scaled_t = (abs(vmin_t)/(abs(vmin_t)+abs(vmax_t)))                    # colors = [(0, 'xkcd:mocha'), (zero_scaled_t,                    #                                   'xkcd:white'), (1, 'xkcd:aquamarine')]                    # custom_cmap_p =  LinearSegmentedColormap.from_list(                    #     'custom_cmap', colors)                    brbg11 = plt.get_cmap('BrBG', 9)                    colors = brbg11([i for i in range(brbg11.N)[1:-1]])  # Get list of 11 RGBA colors                    custom_cmap_p = LinearSegmentedColormap.from_list('BrBG11', colors)                     # If you want a ListedColormap for use in pcolormesh, imshow, etc.                    # custom_cmap_p = ListedColormap(brbg11.colors)                                        vmin_p = -20                    vmax_p = 20                    zero_scaled_p = (abs(vmin_p)/(abs(vmin_p)+abs(vmax_p)))                                               colors = [(0, 'cornflowerblue'), (zero_scaled_p,                                                      'xkcd:white'), (1, 'xkcd:tomato')]                    custom_cmap_t = LinearSegmentedColormap.from_list(                         'custom_cmap', colors)                    members = [1, 3, 4, 6, 4]                    # members = [1, 1, 1, 1]                    all_diff = []  # create a dataset to add all member differences                    all_model_diffs = []                    models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]                    lonmin, lonmax, latmin, latmax = [60,109,22,52]                    all_member_diffs = []                    for (m, model) in enumerate(models):                        model_diff = []                        for member in range(members[m]):                            member_diff=[]                            # only open data for non model averages (except for IPSL-CM6 as only one member)                            if model == "IPSL-CM6" or member != 0:                                # Part 1: Delete                                diff_folder_in = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/03. Regridded Perturbations/{var}/{timeframe}/{model}/{member}"                                ifile_diff = f"{diff_folder_in}/REGRID.{model}.{var_suffix}.DIF.00{member}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.nc"                                diff =  xr.open_dataset(ifile_diff)                                if model=="IPSL-CM6":                                    print(diff[f'{variable_name}'].max())                                    print(diff[f'{variable_name}'].min())                                if scale == "Local":  # scale the data to the local scale                                    diff = diff.where((diff.lon >= 60) & (diff.lon <= 109) & (                                        diff.lat >= 22) & (diff.lat <= 52), drop=True)                                    print(diff.lon)                                                                                                        # loose all the filtered data (nan)                                diff_clean = diff.dropna(dim="lon", how="all")                                # include the values in the list for caluclating the avg difference by model                                model_diff.append(diff_clean)                                member_diff.append(diff_clean)                                all_diff.append(diff_clean)                                all_member_diff = xr.concat(                                    member_diff, dim="member")                                 all_member_diffs.append(all_member_diff)                                                                # include the values in the list for caluclating the avg difference over all models                                                       all_model_diff = xr.concat(                            model_diff, dim="models").mean(dim="models")  # concatenate all models into a list averaged by model                        all_model_diffs.append(all_model_diff)                     # concatenate all models into a list averaged by model                                        all_model_diffs_avg = xr.concat(                        all_model_diffs, dim="models")  # concatenate all models                    all_diffs_avg = xr.concat(all_diff, dim="models").mean(                        dim="models")  # concatenate all members and calculate the mean over all the models                    all_diffs_avg_local = all_diffs_avg.where((all_diffs_avg.lon >= 60) & (all_diffs_avg.lon <= 109) & (                        all_diffs_avg.lat >= 22) & (all_diffs_avg.lat <= 52), drop=True)                    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"                    os.makedirs(o_folder, exist_ok=True)                    o_file=f"{o_folder}/{scale}_{var}_processed.nc"                    # all_diffs_avg.to_netcdf(o_file)                                        # Unpack axes                    if var == "Precipitation":                        all_vars_p = all_diffs_avg                        all_vars_local_p = all_diffs_avg_local                        all_model_diffs_avg_p = all_model_diffs_avg                        all_member_diff_p = all_member_diffs                    elif var =="Temperature":                        all_vars_t = all_diffs_avg                        all_vars_local_t = all_diffs_avg_local                        all_model_diffs_avg_t = all_model_diffs_avg                        all_member_diff_t = all_member_diffs                                    """ Part 2 - Shapefile outline for Karakoram Area to be included"""# path to  shapefilesubregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"shp = gpd.read_file(subregions_path).to_crs('EPSG:4326')indices = ["A", "B","C","D"]#, "E", "F"]# Create the mosaic plotlayout = """ABCD"""figsize = (18.15,10) #based on aspect ratio of global and zoomedfig, axes = plt.subplot_mosaic(layout, subplot_kw={'projection': ccrs.PlateCarree()},                               figsize=figsize,                               gridspec_kw={'wspace': 0.05, 'width_ratios': [2.0, 1.63], 'hspace': 0.15})#,'height_ratios': [1, 1]}) #width_space before was 0.05axes['A'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['C'].set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())axes['B'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())axes['D'].set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())# plot the irrmip differenceim_t_g = plot_subplots(indices[0], subplots, (sum(members)-len(models)+1),                   all_vars_t, timestamps, axes[indices[0]], shp, custom_cmap_t, timeframe, "Global", "∆Temperature, Global", subplot_index="a", vmin=vmin_t, vmax=vmax_t)im_t_l = plot_subplots(indices[1], subplots, (sum(members)-len(models)+1),                   all_vars_local_t, timestamps, axes[indices[1]], shp, custom_cmap_t, timeframe, "Local", "∆Temperature, High Mountain Asia", subplot_index="b", vmin=vmin_t, vmax=vmax_t)im_p_g = plot_subplots(indices[2], subplots, (sum(members)-len(models)+1),                   all_vars_p, timestamps, axes[indices[2]], shp, custom_cmap_p, timeframe, "Global", "∆Precipitation, Global", subplot_index="c", vmin=vmin_p, vmax=vmax_p)im_p_l = plot_subplots(indices[3], subplots, (sum(members)-len(models)+1),                   all_vars_local_p, timestamps, axes[indices[3]], shp, custom_cmap_p, timeframe, "Local", "∆Precipitation, High Mountain Asia", subplot_index="d", vmin=vmin_p, vmax=vmax_p)bbox = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['A'].transData,    xyB=(lonmin, latmax), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1, zorder=10)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['A'].transData,    xyB=(lonmin, latmin), coordsB=axes['B'].transData,    axesA=axes['A'], axesB=axes['B'],    color='black', linestyle='-', linewidth=1, zorder=10)# Add to the global axes (they own the lines)axes['A'].add_artist(line_top)axes['A'].add_artist(line_bottom)axes['A'].add_patch(bbox)bbox2 = Rectangle((lonmin, latmin),              # lower-left corner    lonmax-lonmin,                    # width = 109 - 60    latmax-latmin,                    # height = 52 - 22    linewidth=2,    edgecolor='black',    facecolor='none',    linestyle='-',    transform=ccrs.PlateCarree(),    zorder=10)line_top = ConnectionPatch(    xyA=(lonmax, latmax), coordsA=axes['C'].transData,    xyB=(lonmin, latmax), coordsB=axes['D'].transData,    axesA=axes['C'], axesB=axes['D'],    color='black', linestyle='-', linewidth=1, zorder=10)    line_bottom = ConnectionPatch(    xyA=(lonmax, latmin), coordsA=axes['C'].transData,    xyB=(lonmin, latmin), coordsB=axes['D'].transData,    axesA=axes['C'], axesB=axes['D'],    color='black', linestyle='-', linewidth=1, zorder=10)# Add to the global axes (they own the lines)axes['C'].add_artist(line_top)axes['C'].add_artist(line_bottom)axes['C'].add_patch(bbox2)        # for i, item in enumerate(all_member_diff_p):#     print(f"Item {i}: type = {type(item)}")    # signed_p = [np.sign(diff_p['pr']) for diff_p in all_member_diff_p]# signed_t = [np.sign(diff_t['tas']) for diff_t in all_member_diff_t]# # Concatenate along 'members' (existing dim)# sign_diff_p = xr.concat(signed_p, dim='members')# sign_diff_t = xr.concat(signed_t, dim='members')""" Create hedged area for where 80% of the models agrees on the sign of change"""def process_var(ds, var_name):    if var_name in ds:        da = ds[var_name]        # Drop 'height' or 'time' if present        for dim in ['height', 'time']:            if dim in da.dims and da.sizes[dim] == 1:                print(var_name)                da = da.squeeze(dim)                print(da)        return xr.apply_ufunc(np.sign, da)    return None# # Process 'pr' and 'tas'# sign_diff_p = xr.concat(#     [process_var(ds, 'pr') for ds in all_member_diff_p if process_var(ds, 'pr') is not None],#     dim='members'# )# sign_diff_t = xr.concat(#     [process_var(ds, 'tas') for ds in all_member_diff_t if process_var(ds, 'tas') is not None],#     dim='members'# )sign_diff_p = xr.concat(    [ds['pr'] for ds in all_member_diff_p if isinstance(ds, xr.Dataset) and 'pr' in ds],    dim='members')sign_diff_t = xr.concat(    [ds['tas'] for ds in all_member_diff_t if isinstance(ds, xr.Dataset) and 'tas' in ds],    dim='members')# sign_diff_p = xr.concat(#     [np.sign(ds['pr'].reset_coords('height', drop=True)).squeeze() if 'height' in ds['pr'].coords else np.sign(ds['pr'])#         for ds in all_member_diff_p#         if isinstance(ds, xr.Dataset) and 'pr' in ds], dim='members'# )# # Process 'tas' data (with optional 'height' coordinate)# sign_diff_t = xr.concat(#     [np.sign(ds['tas'].reset_coords('height', drop=True)).squeeze() if 'height' in ds['tas'].coords else np.sign(ds['tas'])#         for ds in all_member_diff_t#         if isinstance(ds, xr.Dataset) and 'tas' in ds],dim='members'# )# 2. Subset the region of interest: South Asia (approx.)region_bounds = {    "lon_min": 60,    "lon_max": 109,    "lat_min": 22,    "lat_max": 52,}sign_diff_p = sign_diff_p.sel(    lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),    lat=slice(region_bounds["lat_max"], region_bounds["lat_min"]))sign_diff_t = sign_diff_t.sel(    lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),    lat=slice(region_bounds["lat_max"], region_bounds["lat_min"]))# 3. Compute agreement mask: where >80% of members agree on signwithin_threshold_p = (abs(sign_diff_p.mean(dim="members")) > 0.8)within_threshold_t = (abs(sign_diff_t.mean(dim="members")) > 0.8)# 4. Ensure masks are 2D, squeeze any singleton dimensionswithin_threshold_2d_p = within_threshold_p.astype(int)within_threshold_2d_t = within_threshold_t.astype(int)# 5. Prepare meshgrid for plotting# lon2d, lat2d = np.meshgrid(all_diffs_avg_local.lon.reset_coords('height', drop=True).squeeze(), all_diffs_avg_local.lat.reset_coords('height', drop=True).squeeze())lon2d, lat2d = np.meshgrid(process_var(all_diffs_avg_local,'lon'),process_var(all_diffs_avg_local,'lat'))for ds in all_member_diff_p:    if 'pr' in ds:        print(ds['pr'].shape, ds['pr'].dims)        for ds in all_member_diff_t:    if 'tas' in ds:        print(ds['tas'].shape, ds['tas'].dims)        # 6. Plotting with contourf and hatchingfor key, mask in zip(["B", "D"], [within_threshold_2d_t, within_threshold_2d_p]):    mask=mask.reset_coords('height', drop=True).squeeze()    axes[key].contourf(        lon2d, lat2d, mask,        levels=[0.5, 1.5],        colors='black',        hatches=['////'],        transform=ccrs.PlateCarree(), zorder=100    )        # hedging_patch = mpatches.Patch(#     label='> 80% of model members agree on the sign of change', hatch='////', edgecolor='black', facecolor='none')    """ Create area where models are within 10% chagne of another"""# stack model outputs into 3D arrays# diff_p_stack = xr.concat(#     [ds['pr'] for ds in all_member_diff_p if isinstance(ds, xr.Dataset) and 'pr' in ds],#     dim='members'# )# diff_t_stack = xr.concat(#     [ds['tas'] for ds in all_member_diff_t if isinstance(ds, xr.Dataset) and 'tas' in ds],#     dim='members'# )# region_bounds = {#     "lon_min": 60,#     "lon_max": 109,#     "lat_min": 22,#     "lat_max": 52,# }# # Subset region# diff_p_stack = diff_p_stack.sel(#     lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),#     lat=slice(region_bounds["lat_max"], region_bounds["lat_min"])# )# # Subset region# diff_t_stack = diff_t_stack.sel(#     lon=slice(region_bounds["lon_min"], region_bounds["lon_max"]),#     lat=slice(region_bounds["lat_max"], region_bounds["lat_min"])# )# median_p = diff_p_stack.median(dim='members')# median_t = diff_t_stack.median(dim='members')# tolerance_p = 0.2 * abs(median_p)# tolerance_t = 0.2 * abs(median_t)# within_range_p = abs(diff_p_stack - median_p) <= tolerance_p# agreement_fraction_p = within_range_p.sum(dim='members') / diff_p_stack.sizes['members']# within_threshold_magnitude_p = agreement_fraction_p < 0.8# within_range_t = abs(diff_t_stack - median_t) <= tolerance_t# agreement_fraction_t = within_range_t.sum(dim='members') / diff_t_stack.sizes['members']# within_threshold_magnitude_t = agreement_fraction_t < 0.8# within_threshold_magnitude_p=within_threshold_magnitude_p.astype(int).squeeze() #lose z dimension for plotting# within_threshold_magnitude_t=within_threshold_magnitude_t.astype(int).squeeze()# for key, mask in zip(["B", "D"], [within_threshold_magnitude_t, within_threshold_magnitude_p]):#     axes[key].contourf(#         lon2d, lat2d, mask, #if not 2D or plotting height as title, .reset_coords('height', drop=True)#         levels=[0.5, 1.5],#         colors='none',#         hatches=['////'],#         transform=ccrs.PlateCarree()#     )# hedging_patch = mpatches.Patch(#     label='> 80% of model members are within 10% of median', hatch='////', edgecolor='black', facecolor='none')""" 3C Add color bar for entire plot"""# add cbar in the figure, for overall figure, not subplots# Define the position of the colorbar# # cbar_ax = fig.add_axes([0.92, 0.1, 0.02, 0.78])cbar_ax_t = fig.add_axes([0.9, 0.53, 0.015, 0.35])  # [left, bottom, width, height]cbar_ax_p = fig.add_axes([0.9, 0.1, 0.015, 0.35])cbar_t = fig.colorbar(im_t_l, cax=cbar_ax_t, extend='both')cbar_p = fig.colorbar(im_p_l, cax=cbar_ax_p, extend='both')# Increase distance between colorbar label and colorbarcbar_t.ax.yaxis.labelpad = 20cbar_t.set_label(fr'$\Delta$ Temperature$_{{\text{{ Irr-NoIrr}}}}$ [{unit}]', size=16)cbar_t.ax.tick_params(labelsize=14)cbar_p.ax.yaxis.labelpad = 20cbar_p.set_label(fr'$\Delta$ Precipitation$_{{\text{{ Irr-NoIrr}}}}$ [{unit}]', size=16)cbar_p.ax.tick_params(labelsize=14)axes['A'].grid(False)axes['C'].grid(False)# im_t_g.grid(False)# im_p_g.grid(False)# Add the custom legend to the plot# fig.legend(handles=[hedging_patch], loc='lower center', bbox_to_anchor=(0.53, 0.02), fontsize=16)# if plotsave == 'save':# o_folder_diff = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/04. Figures/01. Climate data/02. Perturbations/"# os.makedirs(f"{o_folder_diff}/", exist_ok=True)# o_file_name = f"{o_folder_diff}/Mosaic.{var}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"o_file_name = f"{fig_folder}/Mosaic.∆PT.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png"# o_file_name = f"{fig_folder}/01. EGU25/Mosaic.∆PT.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.png" #without hedgingplt.savefig(o_file_name, bbox_inches='tight')plt.show()#%% Cell 6c: TP plot - read out subregional averagesplt.rcParams.update({'font.size': 16})def _latlon_names(da):    # detect coord names    for la in ["lat","latitude","y"]:        if la in da.coords: lat = la; break    else: raise ValueError("No latitude coord found")    for lo in ["lon","longitude","x"]:        if lo in da.coords: lon = lo; break    else: raise ValueError("No longitude coord found")    return lat, londef regional_mean_by(shp_gdf, da, attr="o2region", area_weight=True):    """    Compute mean of DataArray `da` inside each polygon of `shp_gdf`, grouped by column `attr`.    Returns an xarray.DataArray indexed by region names (and any remaining dims like time).    """    # make sure we have a DataArray    if isinstance(da, xr.Dataset):        da = da[list(da.data_vars)[0]]    lat, lon = _latlon_names(da)    # optional area weights (approx): cos(lat)    if area_weight:        w = np.cos(np.deg2rad(da[lat]))        # broadcast to da dims        w = xr.ones_like(da) * w    # build regions object from the polygons    regions = regionmask.Regions(        outlines=list(shp_gdf.geometry),        names=list(shp_gdf[attr]),        abbrevs=list(shp_gdf[attr]),        name=attr,    )    # mask_3D has a boolean layer for each region (region, lat, lon)    mask3D = regions.mask_3D(da)    # ensure NaNs in data don’t contaminate denominator    valid = xr.where(~xr.ufuncs.isnan(da), 1.0, 0.0)    if area_weight:        num = (da * w * mask3D).sum(dim=[lat, lon])        den = (w * valid * mask3D).sum(dim=[lat, lon])    else:        num = (da * mask3D).sum(dim=[lat, lon])        den = (valid * mask3D).sum(dim=[lat, lon])    reg_mean = num / den  # dims: region (+ any remaining dims like time/season)    reg_mean = reg_mean.rename({"region": attr})    reg_mean = reg_mean.assign_coords({attr: regions.names})    return reg_meandef calculate_regional_means(index, subplots, annotation, diff, timestamps, axes, shp, custom_cmap, timeframe, scale, title, subplot_index, vmin=None, vmax=None):    for time_idx, timestamp_name in enumerate(timestamps):        # Determine subplot location based on timeframe        if timeframe == 'annual':            row, col = 0, 0            ax = axes        # Select time dimension based on scale and timeframe        if scale == "Local":            time_dim_name = list(diff.dims)[0]        elif scale == "Global" and timeframe != 'annual':            time_dim_name = list(diff.dims)[2]        # Select relevant data slice        if timeframe == 'annual':            diff_sel = diff        else:            diff_sel = diff.isel({time_dim_name: time_idx})                # Convert Dataset to DataArray if necessary        if isinstance(diff_sel, xr.Dataset):            diff_sel = diff_sel[list(diff_sel.data_vars.keys())[0]]                    diff_sel=diff_sel.reset_coords('height', drop=True).squeeze()        # print(diff_sel)        # Plot data and the Karakoram outline        # im = diff_sel.plot.imshow(ax=ax, vmin=vmin, vmax=vmax, extend='both',        #                           transform=ccrs.PlateCarree(), cmap=custom_cmap, add_colorbar=False, zorder=1)        regional_means = []        if scale=="Local":            for attribute, subregion in shp.groupby('o2region'):                           # Uncomment for coloring of specific subregions                # linecolor = "black"  # if subregion.o2region.values in highlighted_subregions else "black"                # subregion.plot(ax=ax, edgecolor='yellow', linewidth=4,                # subregion.plot(ax=ax, edgecolor='black', linewidth=0.5,                #               facecolor='none', zorder=3) # facecolor=region_colors[i],, alpha=0.4 # Plot the subregion                                reg_means = regional_mean_by(subregion, diff_sel, attr="o2region", area_weight=True)                print(reg_means['o2region'].values)   # array of region names/IDs                print(reg_means.values)               # array of average values                regional_means.append(list(zip(reg_means['o2region'].values, reg_means.values)))                            return regional_means# def plot_P_T_perturbations_avg(scale, var, timeframe, mode, diftype, plotsave):""" Part 0 - Set plotting parameters"""# y0 = 1985  # if running from 1901 to 1985, than indicate extra id of counterfactual to access the data# ye = 2014# if running from 1901 to 1985, than indicate extra id of counterfactual to access the datay0s = [1985]#, 1901]yes = [2014]#, 1985]extra_ids = [""]#, "_counterfactual"]ptb_types = ["Irr"]#, "NoForcing"]scale = "Global"subplots = "off"all_vars_p = []all_vars_t = []all_vars_local_p = []all_vars_local_t = []all_model_diffs_avg_p = []all_model_diffs_avg_t = []all_member_diff_p = []all_member_diff_t = []# "Temperature"]:  # ,"Temperature"]:for var in ["Temperature", "Precipitation"]:    for timeframe in ["annual"]:  # :, "seasonal", "monthly"]:        for mode in ['dif']:  # , 'std']:            for y, y0 in enumerate(y0s):                ye = yes[y]                extra_id = extra_ids[y]                ptbtype = ptb_types[y]                if var == "Precipitation" and mode == 'dif':                    diftypes = ['rel']                else:                    diftypes = ['abs']                for dif in diftypes:                    print(var, timeframe, dif)                    diftype = dif                    timestamps = "YEAR"                    time_averaging = 'time.year'                    time_type = 'year'                    col_wrap = 1                    # Provide cbar ranges and colors for plots for different variables, modes (dif/std) and difference types (abs/rel)                    if var == "Precipitation":                        variable_name = "pr"                        var_suffix = "PR"                        if mode == 'dif' and diftype == 'rel':                            mode_suff = 'total'                            unit = '%'                    elif var == "Temperature":                        variable_name = "tas"                        var_suffix = "TEMP"                        mode_suff = 'total'                                                unit = '°C'                    vmin_t = -1#-1.5                    vmax_t = 1#1.5                    zero_scaled_t = (abs(vmin_t)/(abs(vmin_t)+abs(vmax_t)))                    # colors = [(0, 'xkcd:mocha'), (zero_scaled_t,                    #                                   'xkcd:white'), (1, 'xkcd:aquamarine')]                    # custom_cmap_p =  LinearSegmentedColormap.from_list(                    #     'custom_cmap', colors)                    brbg11 = plt.get_cmap('BrBG', 9)                    colors = brbg11([i for i in range(brbg11.N)[1:-1]])  # Get list of 11 RGBA colors                    custom_cmap_p = LinearSegmentedColormap.from_list('BrBG11', colors)                     # If you want a ListedColormap for use in pcolormesh, imshow, etc.                    # custom_cmap_p = ListedColormap(brbg11.colors)                                        vmin_p = -20                    vmax_p = 20                    zero_scaled_p = (abs(vmin_p)/(abs(vmin_p)+abs(vmax_p)))                                               colors = [(0, 'cornflowerblue'), (zero_scaled_p,                                                      'xkcd:white'), (1, 'xkcd:tomato')]                    custom_cmap_t = LinearSegmentedColormap.from_list(                         'custom_cmap', colors)                    members = [1, 3, 4, 6, 4]                    # members = [1, 1, 1, 1]                    all_diff = []  # create a dataset to add all member differences                    all_model_diffs = []                    models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]                    lonmin, lonmax, latmin, latmax = [60,109,22,52]                    all_member_diffs = []                    for (m, model) in enumerate(models):                        model_diff = []                        for member in range(members[m]):                            member_diff=[]                            # only open data for non model averages (except for IPSL-CM6 as only one member)                            if model == "IPSL-CM6" or member != 0:                                # Part 1: Delete                                diff_folder_in = f"/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/03. Regridded Perturbations/{var}/{timeframe}/{model}/{member}"                                ifile_diff = f"{diff_folder_in}/REGRID.{model}.{var_suffix}.DIF.00{member}.{y0}_{ye}_{timeframe}_{diftype}{extra_id}.nc"                                diff =  xr.open_dataset(ifile_diff)                                if model=="IPSL-CM6":                                    print(diff[f'{variable_name}'].max())                                    print(diff[f'{variable_name}'].min())                                if scale == "Local":  # scale the data to the local scale                                    diff = diff.where((diff.lon >= 60) & (diff.lon <= 109) & (                                        diff.lat >= 22) & (diff.lat <= 52), drop=True)                                    print(diff.lon)                                                                                                        # loose all the filtered data (nan)                                diff_clean = diff.dropna(dim="lon", how="all")                                # include the values in the list for caluclating the avg difference by model                                model_diff.append(diff_clean)                                member_diff.append(diff_clean)                                all_diff.append(diff_clean)                                all_member_diff = xr.concat(                                    member_diff, dim="member")                                 all_member_diffs.append(all_member_diff)                                                                # include the values in the list for caluclating the avg difference over all models                                                       all_model_diff = xr.concat(                            model_diff, dim="models").mean(dim="models")  # concatenate all models into a list averaged by model                        all_model_diffs.append(all_model_diff)                     # concatenate all models into a list averaged by model                                        all_model_diffs_avg = xr.concat(                        all_model_diffs, dim="models")  # concatenate all models                    all_diffs_avg = xr.concat(all_diff, dim="models").mean(                        dim="models")  # concatenate all members and calculate the mean over all the models                    all_diffs_avg_local = all_diffs_avg.where((all_diffs_avg.lon >= 60) & (all_diffs_avg.lon <= 109) & (                        all_diffs_avg.lat >= 22) & (all_diffs_avg.lat <= 52), drop=True)                    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"                    os.makedirs(o_folder, exist_ok=True)                    o_file=f"{o_folder}/{scale}_{var}_processed.nc"                    # all_diffs_avg.to_netcdf(o_file)                                        # Unpack axes                    if var == "Precipitation":                        all_vars_p = all_diffs_avg                        all_vars_local_p = all_diffs_avg_local                        all_model_diffs_avg_p = all_model_diffs_avg                        all_member_diff_p = all_member_diffs                    elif var =="Temperature":                        all_vars_t = all_diffs_avg                        all_vars_local_t = all_diffs_avg_local                        all_model_diffs_avg_t = all_model_diffs_avg                        all_member_diff_t = all_member_diffs                                    """ Part 2 - Shapefile outline for Karakoram Area to be included"""# path to  shapefilesubregions_path = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/02. QGIS/RGI outlines/GTN-G_O2regions_selected_clipped.shp"shp = gpd.read_file(subregions_path).to_crs('EPSG:4326')indices = ["A", "B","C","D"]#, "E", "F"]# Create the mosaic plotlayout = """ABCD"""figsize = (18.15,10) #based on aspect ratio of global and zoomedfig, axes = plt.subplot_mosaic(layout, subplot_kw={'projection': ccrs.PlateCarree()},                               figsize=figsize,                               gridspec_kw={'wspace': 0.05, 'width_ratios': [2.0, 1.63], 'hspace': 0.15})#,'height_ratios': [1, 1]}) #width_space before was 0.05#                    all_vars_t, timestamps, axes[indices[0]], shp, custom_cmap_t, timeframe, "Global", "∆Temperature, Global", subplot_index="a", vmin=vmin_t, vmax=vmax_t)im_t_l = calculate_regional_means(indices[1], subplots, (sum(members)-len(models)+1),                   all_vars_local_t, timestamps, axes[indices[1]], shp, custom_cmap_t, timeframe, "Local", "∆Temperature, High Mountain Asia", subplot_index="b", vmin=vmin_t, vmax=vmax_t)im_p_l = calculate_regional_means(indices[3], subplots, (sum(members)-len(models)+1),                   all_vars_local_p, timestamps, axes[indices[3]], shp, custom_cmap_p, timeframe, "Local", "∆Precipitation, High Mountain Asia", subplot_index="d", vmin=vmin_p, vmax=vmax_p)flat_t = [item for sublist in im_t_l for item in sublist]flat_p = [item for sublist in im_p_l for item in sublist]df_t = pd.DataFrame(flat_t, columns=["o2region", "mean_value"])df_p = pd.DataFrame(flat_p, columns=["o2region", "mean_value"])# Save to CSVdf_t.to_csv(f"{wd_path}/summary/∆T_regional_means.csv", index=False)df_p.to_csv(f"{wd_path}/summary/∆P_regional_means.csv", index=False)#%% Cell 6d: Read out values for irrigationfig,ax = plt.subplots(figsize=(9,10))scale ="Local"variables=["Precipitation","Temperature"]short_variables=['pr','tas']types=['min','max']for v,var in enumerate(variables):    o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/01. Climate data/08. Processed Perturbations Plots"    # os.makedirs(o_folder, exist_ok=True)    o_file=f"{o_folder}/{scale}_{var}_processed.nc"    svar = short_variables[v]        with xr.open_dataset(o_file) as ds:        print(svar)        for m in range(2):            if m==0:                val_max = ds[svar].max().values            else:                val_max = ds[svar].min().values            max_loc = ds[svar].where(ds[svar] == val_max, drop=True)            # Extract coordinates of the maximum value            lon_max = max_loc.lon.values[0]            lat_max = max_loc.lat.values[0]            print(types[m],svar,':', val_max, 'lon:', lon_max,'lat:',  lat_max)   #%% Cell 7: Presentation plot - Increased Irrigation Areafolder_data=("/Users/magaliponds/OneDrive - Vrije Universiteit Brussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/Irrigation_Data_Table.csv"                       )inputdata=pd.read_csv(folder_data, header=0, index_col="Year")# Define blue color shades and hatch patterns# colors = ['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087']cmap = cm.get_cmap('Blues', 6)  # Get 5 different shades of bluecolors = [cmap(i) for i in np.linspace(0.2, 1, 6)]  # Adjust brightnesshatch_patterns = ['//', '\\', '//', '\\', '//']plt.figure(figsize=(8,5))stacked_areas = plt.stackplot(inputdata.index,              inputdata["USA (Mha)"],              inputdata["Pakistan (Mha)"],              inputdata["China (Mha)"],              inputdata["India (Mha)"],              inputdata["Other countries (Mha)"],              labels=["USA", "Pakistan", "China", "India", "Other countries"],              alpha=0.7, colors=colors)  # Adjust transparency# Annotate country names on the plotcountry_labels = ["USA", "Pakistan", "China", "India", "Other \n countries","total"]mid_year = inputdata.index[len(inputdata) // 2]  # Select a middle year for annotationcumulative_values = np.zeros(len(inputdata))  # Initialize cumulative sum for stackingstacked_data = np.cumsum(inputdata.values, axis=1)for i, country in enumerate(inputdata.columns[:-1]):    mid_year = inputdata.index[len(inputdata) // 2]  # Middle year for annotation    mid_index = 10#len(inputdata) // 2  # Middle index of data    country_label=country_labels[i]        # Compute the middle value of the stacked region    if i == 0:        mid_value = inputdata.iloc[mid_index, i] / 2  # First country is at the bottom    else:        mid_value = (stacked_data[mid_index, i] + stacked_data[mid_index, i - 1]) / 2  # Middle of stacked area    # Annotate the country in the correct position    plt.text(1998, mid_value, country_label, fontsize=12,  ha='right', va='center')             # bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))# Labels and titleplt.xlabel("Year")# Set xticks to match the "Year" column (index-based ticks)plt.ylabel("Area Equipped for Irrigation (Mha)")plt.title("Global Irrigation Expansion Over Time")# plt.legend(loc="upper left")plt.savefig(f"{fig_folder}/00. Appendix/Timeline_Irrigation_Increase.png")# Show the plotplt.show()#%% Cell 8 Plot figure comparison reference datafig,axes=plt.subplots(2,1, figsize=(15,7.5), sharex=True) axes = axes.flatten()years_hugo=np.arange(2000,2020)axes[0].plot(years_hugo, np.ones(len(years_hugo))*weighted_B_hugo, color='crimson', ls=':',lw=3, label="Hugonnet et al., reg 13-15, A>5km$^{2}$, avg., ref period")draw_std_boxplot( axes[0],years_hugo, np.ones(len(years_hugo))*weighted_B_hugo, np.ones(len(years_hugo))*weighted_errB_hugo, box_width=1,  color='crimson', alpha=0.5, ls=':',label="Hugonnet et al., reg 13-15, A>5km$^{2}$, std, ref period")# ds_zemp_1314['Year'] = ds_zemp_1314.Year.astype(int) # draw_std_boxplot(axes, ds_zemp_1314.Year, ds_zemp_1314[' annual'], ds_zemp_1314[' sig_annual'], box_width=1, color='blue', ls='-',alpha=0.5, label="Zemp et al, reg 13-14")# draw_std_boxplot(axes, ds_zemp_1315.Year, ds_zemp_1315[' annual'], ds_zemp_1315[' sig_annual'], box_width=1, color='slateblue', ls='-',alpha=0.5, label="Zemp et al, reg 13-15")# axes.boxplot([ds_zemp_1314.Year.values, ds_zemp_1314[' annual'].values])# color=region_colors[14], marker= "^",ls=":", label="Zemp et al, reg 13-14")#,marker="o")# draw_std_boxplot(axes, ds_zemp_tot.Year, ds_zemp_tot[' annual'], ds_zemp_tot[' sig_annual'], box_width=1, color='blue', ls='-',alpha=0.5, label="Zemp et al, reg 13-14")# draw_std_boxplot(axes[0], ds_zemp_tot.Year, ds_zemp_tot[' annual_mya'], ds_zemp_tot[' sig_annual_mya'], box_width=1, color='blue', ls='-',alpha=0.3, label="Zemp et al. std, reg 13-15")# axes[0].plot( ds_zemp_tot[ds_zemp_tot['Year'].between(2000, 2019)].Year, ds_zemp_tot[ds_zemp_tot['Year'].between(2000, 2019)][' annual_mya_ref'], color='blue', ls=':',label="Zemp et al., reg 13-15 avg, ref period")# axes[1].plot(ds_zemp_tot.Year, ds_zemp_tot[' annual'], color='blue', ls='-',label="Zemp et al., reg 13-15, avg", lw=2) #ds_zemp_tot[' sig_annual'], box_width=1, draw_std_boxplot(axes[0], ds_zemp_tot_reg.Year, ds_zemp_tot[' annual_mya'], ds_zemp_tot_reg[' sig_annual_mya'], box_width=1, color='blue', ls='-',alpha=0.3, label="Zemp et al. std, reg 13-15")axes[0].plot( ds_zemp_tot_reg[ds_zemp_tot_reg['Year'].between(2000, 2019)].Year, ds_zemp_tot_reg[ds_zemp_tot['Year'].between(2000, 2019)][' annual_mya_ref'], color='blue', ls=':',lw=3, label="Zemp et al., reg 13-15 avg, ref period", zorder=100)axes[1].plot(ds_zemp_tot_reg.Year, ds_zemp_tot_reg['annual_mya'], color='blue', ls='-',label="Zemp et al., reg 13-15, avg", lw=2) #ds_zemp_tot[' sig_annual'], box_width=1, # axes.plot(ds_zemp_1315.Year, ds_zemp_1315[' annual'], color=region_colors[15], marker= "^",ls=":", label="Zemp et al, reg 13-15")#,marker="o")# draw_std_boxplot(axes, glambie_ds_13.start_dates, glambie_ds_13['combined_mwe'], glambie_ds_13['combined_mwe_errors'], box_width=1, color='pink', alpha=0.5, label="GlaMBIE, reg 13")# draw_std_boxplot(axes, glambie_ds_14.start_dates, glambie_ds_14['combined_mwe'], glambie_ds_14['combined_mwe_errors'], box_width=1, color='violet', alpha=0.5, label="GlaMBIE, reg 14")# draw_std_boxplot(axes, glambie_ds_15.start_dates, glambie_ds_15['combined_mwe'], glambie_ds_15['combined_mwe_errors'], box_width=1, color='orchid', alpha=0.5, label="GlaMBIE, reg 15")# draw_std_boxplot(axes, glambie_weighted.start_dates, glambie_weighted['combined_mwe'], glambie_weighted['combined_mwe_errors'], box_width=1, color='orange', alpha=0.5, label="GlaMBIE, reg. 13-15")draw_std_boxplot(axes[0], glambie_weighted.start_dates, glambie_weighted['combined_mwe_mya'], glambie_weighted['combined_mwe_errors_mya'], box_width=1, color='orange', ls='-',alpha=0.3, label="GlaMBIE, reg. 13-15")axes[0].plot(glambie_weighted[glambie_weighted['start_dates'].between(2000, 2019)].start_dates, glambie_weighted[glambie_weighted['start_dates'].between(2000, 2019)]['combined_mwe_mya_ref'], color='orange', ls = '--', lw=3, label="GlaMBIE, reg. 13-15, ref period")axes[1].plot(glambie_weighted.start_dates, glambie_weighted['combined_mwe'], color='orange', ls = '-', lw=2, label="GlaMBIE, reg. 13-15, ref period")# axes.plot(glambie_ds_13.start_dates,glambie_ds_13.combined_mwe, color=region_colors[13], ls=":",marker= ".",label="GlaMBIE, reg 13")# axes.plot(glambie_ds_14.start_dates,glambie_ds_14.combined_mwe, color=region_colors[14], ls=":", marker= ".",label="GlaMBIE, reg 14")# axes.plot(glambie_ds_15.start_dates,glambie_ds_15.combined_mwe, color=region_colors[15], ls=":", marker= ".",label="GlaMBIE, reg 15")#Hugo: Glacier-specific geodetic mass balance (m w.e. a-1) 2000_2020/ according to Hugonnet et al. (2021) # axes.plot( [1985,2013], np.ones(2)*weighted_B_irr, color=colors['irr'1][0], ls='--', label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# draw_std_boxplot(axes, weighted_B_irr_ts.Year, weighted_B_irr_ts.values, np.zeros(len(weighted_B_irr)), box_width=1, color='grey', ls='-',alpha=0.4, label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# axes.plot(weighted_B_irr_ts.Year, weighted_B_irr_ts.values, color=colors['irr'][0], ls='-', label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")axes[0].plot(weighted_B_irr_ts.Year, np.ones(len(weighted_B_irr_ts))*weighted_B_irr_ts.mean().values, color=colors['irr'][0], ls='-', lw=3, label="Modelled Historic (W5E5), reg 13-15, A>5km$^{2}$")# axes.plot(weighted_B_irr_ref_ts.Year, weighted_B_irr_ref_ts.values, color=colors['irr'][0], ls='--', label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$")axes[0].plot(weighted_B_irr_ref_ts.Year, np.ones(len(weighted_B_irr_ref_ts))*weighted_B_irr_ref_ts.mean().values, color=colors['irr'][0], ls=':', lw=3, label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$, ref period")axes[1].plot(weighted_B_irr_ts.Year, weighted_B_irr_ts.values, color=colors['irr'][0], ls='-', lw=2, label="Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$ avg.,  ref period")# axes.plot( [1985,2014], np.ones(2)*weighted_B_noirr, color=colors['noirr'][0], ls='--', label="Modelled Historic NoIrr (W5E5), reg 13-15, A>5km$^{2}$")axes[0].set_title('Multi year average')axes[1].set_title('Annual timeseries')dataset_legend = [    mpatches.Patch(facecolor='crimson', label='Hugonnet et al.'),    mpatches.Patch(facecolor='orange', label='GlaMBIE'),    mpatches.Patch(facecolor='blue', label='Zemp et al.'),    mpatches.Patch(facecolor=colors['irr'][0], label='"Modelled Historic ref (W5E5), reg 13-15, A>5km$^{2}$'),]# Legend for line styles (gray lines)style_legend = [    Line2D([0], [0], color='grey', lw=5, alpha=0.5, label='Average ± std'),  # thick grey    Line2D([0], [0], color='grey', lw=2, ls=':', label='Average (2000–2019)'),   # dotted    Line2D([0], [0], color='grey', lw=2, ls='-', label='Average (full period)'), # solid]# Combine both legendsall_legend_items = dataset_legend + style_legend# Add to the figure (adjust location as needed)legend1 = fig.legend(    handles=dataset_legend,    loc='lower center',    bbox_to_anchor=(0.5,-0.15 ),    ncol=4,    frameon=False,    fontsize=12)# Second row legend: line styleslegend2 = fig.legend(    handles=style_legend,    loc='lower center',    bbox_to_anchor=(0.5, -0.20),    ncol=3,    frameon=False,    fontsize=12)# Optional: make space for both legend rowsfig.subplots_adjust(bottom=0)area_glambie = round(glambie_weighted[glambie_weighted['start_dates'] == float(area_rgi_year)]['total_area'].values[0])area_hugo = round(df_hugo.Area.sum())basis=-0.5amp=2.5area_oggm = round(df[df['sample_id']=='CESM2.000'].rgi_area_km2.sum())axes[1].text(1957, basis-0*amp, 'Total Area (km$^2$):',fontsize=14, fontweight='bold')axes[1].text(1957, basis-0.05*amp, f'Zemp et al.: {round(ds_zemp_tot_area)} ({area_rgi_year})',fontsize=14)axes[1].text(1957, basis-0.1*amp, f'GlaMBIE.: {area_glambie} ({area_rgi_year})',fontsize=14)axes[1].text(1957, basis-0.15*amp, f'Hugonne et al.: {area_hugo} (rgi date)',fontsize=14)axes[1].text(1957, basis-0.2*amp, f'Modelled area: {area_oggm} (rgi date)',fontsize=14)axes[0].set_ylabel("∆B m w.e. yr$^{-1}$")axes[1].set_ylabel("∆B m w.e. yr$^{-1}$")axes[1].set_xlabel("Year")# fig.legend(loc='lower center', bbox_to_anchor=(0.5,-0.2), ncols =2)fig.subplots_adjust(wspace=0.05)plt.xlim(1956,2024)plt.show()plt.savefig(f'{fig_path}/Ref_B_Comparison_Figure.png')                                                