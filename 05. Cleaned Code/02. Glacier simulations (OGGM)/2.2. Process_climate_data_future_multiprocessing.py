#!/usr/bin/env python3"""Created on Fri May 16 14:38:41 2025@author: magalipondsThis directory works with multiprocessing. Therefore, only uncomment the sections that you want to run.Initialization fo glacier directories occurs in script 1.1. This takes up quite some time to run. Ideally you only run them once and save the output.Futhermore all steps should be run consecutively:    Cell 0: Load the packagesCell 0b: Load custom OGGM functions (with specified function directory in line with custom function directory)Cell 0c: Initialize OGGM with the preferred parameter set upCell 0d: Set the base parametersCell 1: Open list of RGIs, initialized before (made in cell 1 of script 2.1)Cell 2:  Process the future climate data (bias corrected toward perturbed noi-w5e5 climate in function custom_process_cmip data)Cell 3: Run the GEM with perturbed climate data (main output: Volume & area evolution) (future)Cell 4a: Compile the climate data output for the future and historic in the irr scenario (for sense checking)Cell 4b: Compile the climate data output for the future and historic in the noi scenario (for sense checking)Cell 5: Run with hydro output for the future (daily and monthly output)      """# %% Cell 0: Load data packages#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Wed Jul  3 15:36:52 2024@author: magalipondsThis code runs perturbs the climate data in the future and adds this to the baseline climate in OGGM and runs the climate & MB model with this data"""# -*- coding: utf-8 -*-import oggmimport multiprocessingimport tracebackimport loggingfrom multiprocessing import Pool, set_start_method, get_contextfrom multiprocessing import Processimport concurrent.futuresfrom matplotlib.lines import Line2Dimport oggmfrom oggm import utils, cfg, workflow, tasks, DEFAULT_BASE_URL, graphics, global_tasksfrom oggm.core import massbalance, flowlinefrom oggm.utils import floatyear_to_date, hydrodate_to_calendardatefrom oggm.sandbox import distribute_2dfrom oggm.sandbox.edu import run_constant_climate_with_biasfrom oggm.tasks import process_cmip_dataimport geopandas as gpdimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport matplotlib.colors as clrsimport xarray as xrimport osimport seaborn as snsimport salemfrom matplotlib.ticker import FuncFormatterimport pandas as pdimport numpy as npfrom matplotlib import animationfrom IPython.display import HTML, displayimport cartopy.crs as ccrsimport cartopy.feature as cfeaturefrom scipy.optimize import curve_fitfrom tqdm import tqdmimport pickleimport textwrapimport matplotlib.patches as mpatchesimport matplotlib.lines as mlinesfrom matplotlib.colors import LinearSegmentedColormap, TwoSlopeNormimport sysfrom xarray.coding.times import CFDatetimeCoderimport osimport sysfrom concurrent.futures import ProcessPoolExecutor# %% Cell 0b: Load custom OGGM functions (with specified function directory in line with custom function directory)function_directory = "/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/src/05. Cleaned Code/ 02. Glacier simulations (OGGM)"sys.path.append(function_directory)from OGGM_data_processing import process_perturbation_data,custom_process_cmip_data,custom_process_gcm_data#%% Cell 0c: Initialize OGGM with the preferred model parameter set upfolder_path = '/Users/magaliponds/Documents/00. Programming'wd_path = f'{folder_path}/04. Modelled perturbation-glacier interactions - R13-15 A+1km2/'wd_path_fut = f'{folder_path}/04. Modelled perturbation-glacier interactions - R13-15 A+1km2 future/'os.makedirs(wd_path, exist_ok=True)cfg.initialize(logging_level='WARNING')cfg.PATHS['working_dir'] = utils.mkdir(wd_path, reset=False)# make a sum dirsum_dir = os.path.join(wd_path, 'summary')os.makedirs(sum_dir, exist_ok=True)# make a logging directorylog_dir = os.path.join(wd_path, "log")os.makedirs(log_dir, exist_ok=True)# Make a pkl directorypkls = os.path.join(wd_path, "pkls")os.makedirs(pkls, exist_ok=True)pkls_subset = os.path.join(wd_path, "pkls_subset_success")cfg.PARAMS['baseline_climate'] = "GSWP3_W5E5"cfg.PARAMS['store_model_geometry'] = Truecfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # üîß set number of cores# %% Cell 0d: Set base parameterscolors_models = {    "W5E5": ["#000000"],  # "#000000"],  # Black    # Darker to lighter shades of purple    "E3SM": ["#785EF0", "#8F7BF1", "#A6A8F2"],    # Darker to lighter shades of pink    "CESM2": ["#DC267F", "#E58A9E", "#F0A2B6", "#F7BCC4"],    # Darker to lighter shades of orange    "CNRM": ["#FE6100", "#FE7D33", "#FE9A66", "#FEB799", "#FECDB5", "#FEF1E1"],    "IPSL-CM6": ["#FFB000"]  # Dark purple to lighter shades}members = [1, 3, 4, 6, 4, 1]members_averages = [1, 2, 3, 5, 3]models = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM", "W5E5"]models_shortlist = ["IPSL-CM6", "E3SM", "CESM2", "CNRM", "NorESM"]timeframe = "monthly"y0_clim = 1985ye_clim = 2014y0_cf = 1901ye_cf = 1985#%% Cell 1a: Open filtered subsetdef load_single_pkl(filepath):    with open(filepath, 'rb') as f:        return pickle.load(f)def main():    # Collect all .pkl file paths    pkl_files = [os.path.join(pkls_subset, f) for f in os.listdir(pkls_subset) if f.endswith('.pkl')]        # Use all available cores (or limit it manually)    with ProcessPoolExecutor(max_workers=9) as executor:        gdirs = list(executor.map(load_single_pkl, pkl_files))    print(f"Loaded {len(gdirs)} glacier directories.")    # Optional: create dictionary    gdirs_dict = {gdir.rgi_id: gdir for gdir in gdirs}       return gdirs, gdirs_dictif __name__ == '__main__':    gdirs, gdirs_dict = main()    print(f"‚úÖ Loaded all gdirs")#%% Cell 2: Process the future climate data, including bias correction to noi past climate data # For the future we work with noi and irr timeseries from CESM2, no loonger with perturbations  - no W5E5 baseline required from calibration, no better estimatemembers = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["NOI", "IRR"]#, "NOI"]y0=2015ye=2074# if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessingcfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # üîß set number of coresdef load_gdirs(pkl_dir):    return [        pickle.load(open(os.path.join(pkl_dir, f), 'rb'))        for f in os.listdir(pkl_dir) if f.endswith('.pkl')    ]def main():    # opath_climate = os.path.join(sum_dir, 'climate_historical.nc')    # utils.compile_climate_input(    #     gdirs, path=opath_climate, filename='climate_historical')        remake="False"    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >= 1: #skip 0                        sample_id = f"{model}.00{member}"                        print(sample_id, ex, "SSP",ssp)                        if remake=="True":                                                    # Provide the path to the perturbation dataset                            # if error with lon.min or ds['time.year'] check if lon>0 in creating input dataframe                            i_folder_ptb = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/01. Climate data/{model}/{y0}/"                            ds_path = f"{i_folder_ptb}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total.nc"                            # ds_path = f"{i_folder_ptb}/{model}.00{member}.{y0_cf}_{ye_cf}.{timeframe}.perturbation.input.%.degC.counterfactual.nc"                            ds = xr.open_dataset(ds_path)                                                        #add the original timeline for bias correction (reference period 1961-1990)                            i_folder_ptb_og = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/01. Input files/01. Climate data/{model}/1985/"                            ds_path_og = f"{i_folder_ptb_og}/{model}.{ex}.00{member}.1985_2014_selparam_monthly_total.nc"                            ds_og = xr.open_dataset(ds_path_og)                            ds = ds.assign_coords(lat=ds_og.lat) #assign ds with the coords of ds_og (3e-6 difference)                            ds_combined = xr.concat([ds_og,ds], dim="time")                            ds_combined[['pr','sn','tas']] = ds_combined[['pr','sn','tas']].fillna(-1e9) #fill na values with very small nr                            # ds_combined = ds_combined.rename({'time_bnds': 'bnds'})                            # ds_combined = ds_combined.set_coords('bnds')                                                        time64_combined = pd.to_datetime([t.isoformat() for t in ds_combined['time'].values])                            ds_combined = ds_combined.assign_coords(time=('time', time64_combined))                                                        all_prcp = (ds_combined.pr + ds_combined.sn) #convert mm/s to mm/day in function, already summed for days in month                            all_prcp = all_prcp.to_dataset(name='pr')                            all_prcp.pr.attrs['units'] = "kg m-2 s-1"                            all_prcp["time"] = xr.decode_cf(all_prcp).time                            # all_prcp['time.month'] = ('time', all_prcp['time'].dt.month.data)                                                        tas = ds_combined.tas.to_dataset(name='tas')                            tas.tas.attrs['units'] = "degc"                            tas["time"] = xr.decode_cf(tas).time                            tas = tas.sortby('time')                            # tas['time.month'] = ('time', tas['time'].dt.month.data)                                                        o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/03. Future climate data/"                            ds_p_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc"                            ds_t_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc"                            os.makedirs(o_folder, exist_ok=True)                            all_prcp.to_netcdf(ds_p_path)                            tas.to_netcdf(ds_t_path)                        else:                            o_folder = f"/Users/magaliponds/Library/CloudStorage/OneDrive-VrijeUniversiteitBrussel/1. VUB/02. Research/01. IRRMIP/03. Data/03. Output files/03. Future climate data/"                            ds_p_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc"                            ds_t_path = f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc"                                                        all_prcp = xr.open_dataset(f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_P_processed.nc")                            tas = xr.open_dataset(f"{o_folder}/{model}.SSP{ssp}.{ex}.00{member}.{y0}_{ye}_selparam_monthly_total_T_processed.nc")                                                if ex=="NOI":                            output_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        else:                            output_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        # output_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        sample_id =f"{model}.00{member}"                        print(sample_id)                        # print(tas["time.month"])                                                time_vals = tas['lat'].values                        duplicates = pd.Series(time_vals)[pd.Series(time_vals).duplicated()]                        print("Made it to CMIP Processing")                                                # Provide the sample ID to provide the processed pertrubations with the correct output suffix                        workflow.execute_entity_task(custom_process_cmip_data, gdirs,                                                                         fpath_temp=ds_t_path, fpath_precip=ds_p_path,                                                                          y0=1986, y1=2073, filesuffix='', #files must start in januari                                                                         output_filesuffix=output_filesuffix, exp=ex, sample_id=sample_id)                        # print(all_prcp)                                               opath_perturbations = os.path.join(                            sum_dir, f'gcm_data_SSP{ssp}_{ex}.00{member}_noi_bias')                        # # opath_perturbations = os.path.join(                                    utils.compile_climate_input(gdirs, path=opath_perturbations, filename='gcm_data',                                                    input_filesuffix=output_filesuffix,                                                    use_compression=True)if __name__ == '__main__':        gdirs = load_gdirs(pkls_subset)    subset_gdirs = gdirs         multiprocessing.set_start_method('spawn', force=True)    main()                #%% Cell 3: Run climate model   members = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["IRR","NOI"]#,"IRR"]#, "NOI"]y0=2014ye=2074# opath_climate = os.path.join(sum_dir, 'climate_historical.nc')# utils.compile_climate_input(#     gdirs_3r_a5, path=opath_climate, filename='climate_historical')remake="False"logging.basicConfig(filename='error_log.txt', level=logging.ERROR)# # if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessingcfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # üîß set number of coresdef main():    errors = []    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >=1: #skip 0                        sample_id = f"{model}.00{member}"                        print(sample_id, ex, "SSP",ssp)                        if ex=="NOI":                            init_model_suffix = f'_perturbed_CESM2.00{member}'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        if ex == "IRR":                            init_model_suffix = f'_baseline_W5E5.000'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                                                out_id = f'_CESM2{input_filesuffix}'                                                    for gdir in gdirs:                            try:                                workflow.execute_entity_task(                                    tasks.run_from_climate_data, [gdir],                                    ys=y0, ye=ye,                                    max_ys=None, fixed_geometry_spinup_yr=None,                                    store_monthly_step=False,                                    store_model_geometry=True,                                    store_fl_diagnostics=True,                                    climate_filename='gcm_data',                                    climate_input_filesuffix=input_filesuffix,                                    output_filesuffix=out_id,                                    zero_initial_glacier=False, bias=0,                                    temperature_bias=None, precipitation_factor=None,                                    init_model_filesuffix=init_model_suffix,                                    init_model_yr=2014 #2014                                )                            except Exception as err:                                err_msg = (f"‚ùå ERROR for RGI_ID={gdir.rgi_id}, "                                           f"sample_id={sample_id}, ssp={ssp}, exp={ex}, member={member}\n"                                           f"{traceback.format_exc()}")                                logging.error(err_msg)                                errors.append(gdir.rgi_id)                                print(f"[ERROR] Logged: {gdir.rgi_id}")if __name__ == '__main__':        gdirs = load_gdirs(pkls_subset)        multiprocessing.set_start_method('spawn', force=True)    main()            #%% Only compile run outputmembers = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["370","126"]exp = ["IRR", "NOI"] y0=2014 #2015ye=2074def main():    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member !=0 and member<=2: #skip 0                        print(member)                        sample_id = f"{model}.00{member}"                        # if ex=="NOI":                         input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        # else:                        #     input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                                                    out_id = f'_CESM2{input_filesuffix}'                        opath = os.path.join(                            sum_dir, f'climate_run_output{input_filesuffix}.nc'                        )                        ds_ptb = utils.compile_run_output(                            gdirs, path=opath, input_filesuffix=out_id                        )                    if __name__ == '__main__':        def load_gdirs(pkl_dir):        return [            pickle.load(open(os.path.join(pkl_dir, f), 'rb'))            for f in os.listdir(pkl_dir) if f.endswith('.pkl')        ]        gdirs = load_gdirs(pkls_subset)    subset_gdirs = gdirs         multiprocessing.set_start_method('spawn', force=True)    main()       #%% Cell 4a: Compile climate historic and future - IRRmembers = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["IRR"]#"IRR", "NOI"]y0=2015ye=2074def main():    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >=1: #skip 0                        sample_id = f"{model}.00{member}"                        input_filesuffix = f"_SSP{ssp}_{ex}.00{member}"                        out_id = f'{input_filesuffix}'                        opath = os.path.join(                            sum_dir, f'climate_input_data{input_filesuffix}.nc'                        )                        ds_ptb = utils.compile_climate_input(                            gdirs, filename="gcm_data", input_filesuffix=out_id, path=opath                        )if __name__ == '__main__':    gdirs = load_gdirs(pkls_subset)    subset_gdirs = gdirs     multiprocessing.set_start_method('spawn', force=True)    main()       ds_base = utils.compile_climate_input(        gdirs, filename="climate_historical", input_filesuffix="", path=os.path.join(            sum_dir, f'climate_input_data_historical.nc')    )#%%Cell 4b: Compile climate historic and future - NOImembers = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["NOI"]#"IRR", "NOI"]y0=2015ye=2074gdirs = load_gdirs(pkls_subset)subset_gdirs = gdirs def main():    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >=1: #skip 0                        sample_id = f"{model}.00{member}"                        input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        out_id = f'{input_filesuffix}'                        opath = os.path.join(                            sum_dir, f'climate_input_data{input_filesuffix}.nc'                        )                        ds_ptb = utils.compile_climate_input(                            gdirs, filename="gcm_data", input_filesuffix=out_id, path=opath                        )                        if member ==1: #skip 0                            print(member)                            ds_base = utils.compile_climate_input(gdirs, filename="gcm_data", input_filesuffix=f"_perturbed_CESM2.00{member}",                                                                   path=os.path.join(sum_dir, f'climate_input_data_historical_noi_CESM2.00{member}')                            )if __name__ == '__main__':    multiprocessing.set_start_method('spawn', force=True)    main()                                   # %% Cell 5: with hydro future    members = [4]models = ["CESM2"]timeframe = "monthly"ssps = ["126","370"]#"126",exp = ["IRR","NOI"]#,"IRR"]#, "NOI"]y0=2014ye=2074# opath_climate = os.path.join(sum_dir, 'climate_historical.nc')# utils.compile_climate_input(#     gdirs_3r_a5, path=opath_climate, filename='climate_historical')remake="False"cfg.PARAMS['store_monthly_hydro'] = Truelogging.basicConfig(filename='error_log.txt', level=logging.ERROR)# # if you get a long error log saying that "columns" can not be renamed it is often related to multiprocessingcfg.PARAMS['use_multiprocessing'] = Truecfg.PARAMS['core'] = 9 # üîß set number of coresdef main():    errors = []    for m, model in enumerate(models):        for member in range(members[m]):            for s, ssp in enumerate(ssps):                for e, ex in enumerate(exp):                    if member >=1: #skip 0                        sample_id = f"{model}.00{member}"                        print(sample_id, ex, "SSP",ssp)                        if ex=="NOI":                            init_model_suffix = f'_perturbed_CESM2.00{member}'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                        if ex == "IRR":                            init_model_suffix = f'_baseline_W5E5.000'                            input_filesuffix = f"_SSP{ssp}_{ex}.00{member}_noi_bias"                                                out_id = f'CESM2{input_filesuffix}_hydro'                                                    for gdir in gdirs:                            try:                                workflow.execute_entity_task(                                    tasks.run_with_hydro, [gdir],                                    run_task=tasks.run_from_climate_data,                                    ys=y0, ye=ye,                                    max_ys=None, fixed_geometry_spinup_yr=None,                                    store_monthly_step=False,                                    store_monthly_hydro=True,                                    store_model_geometry=True,                                    store_fl_diagnostics=True,                                    climate_filename='gcm_data',                                    climate_input_filesuffix=input_filesuffix,                                    output_filesuffix=out_id,                                    zero_initial_glacier=False, bias=0,                                    temperature_bias=None, precipitation_factor=None,                                    init_model_filesuffix=init_model_suffix,                                    init_model_yr=2014                                )                            except Exception as err:                                err_msg = (f"‚ùå ERROR for RGI_ID={gdir.rgi_id}, "                                           f"sample_id={sample_id}, ssp={ssp}, exp={ex}, member={member}\n"                                           f"{traceback.format_exc()}")                                logging.error(err_msg)                                errors.append(gdir.rgi_id)                                print(f"[ERROR] Logged: {gdir.rgi_id}")                        # Try compiling the output only if you want to after all gdirs are run                        try:                            print("Compiling member:", member)                            #                         )                            if ex=="NOI":                                opath = os.path.join(                                    sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}_noi_bias_hydro.nc')                            else:                                opath = os.path.join(                                    sum_dir, f'climate_run_output_perturbed_SSP{ssp}_{sample_id}_hydro.nc')                            ds_ptb = utils.compile_run_output(                                gdirs, input_filesuffix=out_id, path=opath                            )                        except Exception as err:                            logging.error(f"‚ö†Ô∏è Failed to compile output for {sample_id}\n{traceback.format_exc()}")    # Save all errors at the end    if errors:        with open(os.path.join(sum_dir, "gdir_errors_hdyro.txt"), "w") as f:            for rgi_id in errors:                f.write(rgi_id + "\n")        print(f"\nüö® Finished with {len(errors)} gdir errors. See gdir_errors.txt.")    else:        print("\n‚úÖ All glaciers processed successfully.")if __name__ == '__main__':    def load_gdirs(pkl_dir):        return [            pickle.load(open(os.path.join(pkl_dir, f), 'rb'))            for f in os.listdir(pkl_dir) if f.endswith('.pkl')        ]        gdirs = load_gdirs(pkls_subset)    subset_gdirs = gdirs         multiprocessing.set_start_method('spawn', force=True)    main()         